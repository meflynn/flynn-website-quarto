[
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Student Resources",
    "section": "",
    "text": "This page contains general information for students who are currently enrolled in, or who are interested in taking, my classes.\n\nGeneral Advice\nStudents often ask for advice on how to succeed in class. The following is a list of general tips and guidelines that may be useful to anyone taking any of my undergraduate or graduate courses.\n\nAttend class regularly. The most basic thing you can do to improve your performance is to attend class regularly, take good notes, and pay attention. Please note that I post all lecture slides online, so you do not need to copy each individual slide. Instead, try to focus on the overarching concepts or additional details that supplement the core points appearing on the slides. Also, please note that missing class for family events or other social activities does not exempt you from completing assigned material on time. While I realize that everyone values different aspects of their college experience in different ways, please understand that it is not my responsibility to accommodate the social schedules of every student. My job is to provide you with an opportunity to learn—whether or not you take advantage of that opportunity is up to you.\nDo the readings. As with attendance, another simple step you can take to improve performance is to keep up with the readings. Spend even a short amount of time each day with the material covered in class and in the assigned texts—spending 15 minutes per day reviewing material will be considerably more useful to you can 4 hours of cramming just before a test.\nAsk Questions. Don’t be afraid to ask questions. The whole reason you’re here is because you lack knowledge, not because you possess it. Whether it’s in class, after class, or during office hours, asking questions to clarify difficult material is a critical step.\nDocument special needs, disabilities, or illness early. I’m always happy to accommodate students with special needs, but please do not wait until the last week in the semester to inform me of something that has been systematically affecting your grade all semester. Please speak with the Student Access Center for more information.\n\n\n\nStudent Resources\nStudents deal with a lot. Aside from demanding class schedules, many students are also dealing with complicated personal lives, family issues, or working multiple jobs to make ends meet. This can take a toll on students, and many may be dealing with depression, anxiety, food insecurity, or other issues that can make it difficult to keep up in the classroom. The sheet below provides a variety of on-campus and off-campus resources for students.\n\nStudent Resource Sheet\n\n\n\nWriting Resources\n\nGoogle Scholar Instructions"
  },
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "Consulting",
    "section": "",
    "text": "I also offer freelance statistical and data analysis consulting services. I can work with you and your organization on a range of topics, including data collection, research study designs, performance metrics, statistical analyses, and more. For more information please visit my consulting page:\nPrior Analytics, LLC"
  },
  {
    "objectID": "posts/remote-computing/remote-computing.html",
    "href": "posts/remote-computing/remote-computing.html",
    "title": "Bayesian modeling, speed boosts, and how to run R remotely",
    "section": "",
    "text": "Tip\n\n\n\nWho is this for? More narrowly, people who are running Bayesian models and are looking for a speed boost. More broadly, anyone who needs more resources than their local machine can offer. Either way, remote options can be a great way to access more power and computing resources."
  },
  {
    "objectID": "posts/remote-computing/remote-computing.html#use-a-cluster",
    "href": "posts/remote-computing/remote-computing.html#use-a-cluster",
    "title": "Bayesian modeling, speed boosts, and how to run R remotely",
    "section": "Use a Cluster",
    "text": "Use a Cluster\nIf you work at a larger university (or maybe even a smaller one) the chances are pretty good that the computer science/engineering department folks have a high performance computing cluster. The nice thing about these is that they are often available for use by anyone on campus.\nThe basic idea here is that you submit specific jobs that you want the cluster to process. These might be tasks that require significant processing speed or maybe more memory than your local machine can provide.\nAs an example, Kansas State University’s “Beocat” computing cluster is open to K-State faculty. You are first required to create an account. Once that’s done you have a personal profile that you can access and from which you can submit jobs for the cluster. Usually there’s some sort of manager that sorts jobs on the basis of estimated computing needs, duration, etc., so you might have to wait a little while for your models to start, finish, etc. You can also use a desktop client like Cyberduck that provides a GUI through which you can manage files and move them back and forth between your local machine and the cluster.\nI’ll note that I had mixed success with this. R already runs on Beocat, but getting Stan and {brms} properly configured was a chore, and it never quite got here. I reached out for help a couple of times and the Beocat people (while very responsive) were never able to resolve things, so I had to find an alternative."
  },
  {
    "objectID": "posts/remote-computing/remote-computing.html#amazon-web-services",
    "href": "posts/remote-computing/remote-computing.html#amazon-web-services",
    "title": "Bayesian modeling, speed boosts, and how to run R remotely",
    "section": "Amazon Web Services",
    "text": "Amazon Web Services\nAmazon Web Services is another option. Someone recommended this to me, and I started to look into it but never actually used it. My impression is that is functions much like accessing a university-based cluster. Amazon has a bunch of powerful computers that you can submit jobs to. Unlike your university’s cluster, though, there may be some small fee attached to it. I’m always a bit wary of this sort of arrangement as I don’t trust my up-front estimates of time and resource needs to be accurate, but maybe others with more experience will have more to say."
  },
  {
    "objectID": "posts/remote-computing/remote-computing.html#r-studio-server",
    "href": "posts/remote-computing/remote-computing.html#r-studio-server",
    "title": "Bayesian modeling, speed boosts, and how to run R remotely",
    "section": "R Studio Server",
    "text": "R Studio Server\nThe first two options rely on accessing fairly substantial computing resources set up by third (second?) parties. But what if your jobs don’t require that much power or time? Can you find more local solutions? Yes.\nLet’s imagine that we have a slower laptop computer and a faster desktop computer located in our office. In this situation we might want to connect the laptop that we’re operating at home or at a coffee shop to the more powerful desktop so we can make the desktop do the more labor-intensive task of running our models.\nThis is the exact situation I now find myself in. I was fortunate enough to apply for a small internal grant for a new desktop and purchased a Mac Studio with the funds. It currently sits in my office on campus. I also often use my Macbook Pro, which as I mentioned previously doesn’t have a ton of memory. It’s fine for writing the models and code, but I don’t want to start a model that is going to suck up all of my memory and processing power, leaving me unable to complete other basic tasks.\nThere are a couple of ways that you can go about this. This first option was very graciously provided by Matti Vuorre in response to a question I posed on Mastodon. It requires that you download and install Tailscale, Docker, and possibly [RStudio Server](https://support–rstudio-com.netlify.app/products/rstudio/download-server/.\nI’ll let people who are interested go ahead and check out Matti’s blog post on the subject because it’s more detailed than I can get into here. Suffice it to say his directions are very clear and I was able to get this approach up and running fairy easily.\nOnly a couple of major notes that I came across. First, make sure your remote (e.g. desktop) computer can’t go to sleep as this will break the connection and you won’t be able to establish/re-establish the connection from your remote location.\nSecond, you’ll also need something like Git/GitHub set up to help you move projects and files to the remote session."
  },
  {
    "objectID": "posts/remote-computing/remote-computing.html#mac-screen-share",
    "href": "posts/remote-computing/remote-computing.html#mac-screen-share",
    "title": "Bayesian modeling, speed boosts, and how to run R remotely",
    "section": "Mac Screen Share",
    "text": "Mac Screen Share\nAfter playing with Matti’s recommendation for a while I came across another even more direct solution—Mac OS’ native Screen Sharing app. The only major limitation here that I’m currently aware of is that it requires both the local and remote computers to be Macs.\nFirst, download Tailscale (linked above) on the local and remote devices. This will establish a VPN that you can use to connect the two computers securely. The nice thing about Tailscale is that it’s super easy to use. Specifically, it can create a menu bar icon where you can easily access the IP addresses of the relevant machines connected to the network. There’s also a web-based dashboard that you can use to monitor the devices on the network.\n\n\n\nFigure 1: Screenshot showing the location of the sharing menu in System Settings.\n\n\nSecond, this also requires you to edit some of the security and connection settings on your remote machine. To do this you first open up the System Settings menu, either through the icon on your task bar at the bottom of your screen or by clicking on the Apple icon in the upper left. Then you click on the General menu, and then on sharing over on the right hand side. Figure 1 shows the locations of these buttons in Mac OS Ventura.\n\n\n\nFigure 2: Screenshot showing the security and permissions options that you can change\n\n\nOnce you’ve navigated to this menu you then need to alter the security settings and permissions. Figure 2 shows the menu options here that you can change. A couple of these are linked by default, and I can’t remember which exactly, but you basically want to enable File Sharing, Remote Login, and Remote Management. You can also limit which profiles can log in to the computer, and you should definitely make sure that the device is still password protected with a solid password. Also note that you do not have to make these adjustments on your local machine. In my case I’ve only made them on the more powerful machine I want to use remotely.\n\n\n\nFigure 3: Figure showing the screen sharing IP address menu\n\n\nFinally, if you type “Screen Sharing” into the finder on your local Mac (e.g. your weaker laptop) and hit Enter it will open a small window where you can enter your remote device’s IP address from Tailscale. You can see this in Figure 3. After you hit “Connect” it will prompt you to log in to the remote device using your user credentials.\nThe great thing about this approach is it opens a window that you can use to directly control your remote device from your local screen. This means you can open RStudio and manipulate objects, files, apps, etc., as you normally would. I’m a lazy man, and being able to actually see the content and “use” the more powerful computer directly and without having to makes my life easier.\nSo far this option has worked great, and provided nobody points out some very obvious and critical security issues (I mean it’s definitely possible), I think this will become a staple of my workflow moving forward. That said, my understanding is that this should be pretty secure through a couple of mechanisms. First, more recent versions of the Mac OS secure connections when you’re using your login information, and keystrokes and mouse movements are all encrypted. Second, Tailscale uses end-to-end encryption and nothing is passed through a cloud-based server of any kind. There are other options that I’ve come across, like using an ssh tunnel, but this is not a procedure I’m familiar with. Ultimately I’d defer to more computer-savvy people for more suggestions on how to use this method in the most secure way possible."
  },
  {
    "objectID": "posts/grad-program-director/grad-program-director.html",
    "href": "posts/grad-program-director/grad-program-director.html",
    "title": "What I learned in my first year as a graduate program director",
    "section": "",
    "text": "tl;dr: This post provides a brief overview of some of my biggest takeaways after my first year directing the Security Studies program at Kansas State University. Forging positive working relationships with other rank and file administrators is key, as is institutionalizing lots of the procedures that may have formerly been relegated to post-it notes on a computer monitor."
  },
  {
    "objectID": "posts/grad-program-director/grad-program-director.html#rank-and-file-administration",
    "href": "posts/grad-program-director/grad-program-director.html#rank-and-file-administration",
    "title": "What I learned in my first year as a graduate program director",
    "section": "Rank and file administration",
    "text": "Rank and file administration\nRelationships with other rank and file administrators are key. As much as academics complain about the administration side of universities, it’s important to recognize that there’s a real and meaningful distinction between the kind of upper-level administrative bloat that we often think of when we use “administration” as a pejorative term, and the rank and file administrators who help make the university run.\nProgram directors come and go, but these folks have often been in their jobs for a long time and know the relevant processes and options better than I ever will. In my experience they are also typically very willing to help accommodate student needs and can help you as a new program director to navigate the quirks of any given case.\nIn our case, we also have a representative agent from the Graduate School who works out at Ft. Leavenworth. She’s fantastic, and is often our first point of contact with students who might be interested in our programs. But for her to do her job well she needs the active involvement of program staff. For example, it’s not always immediately obvious what we cover in our courses. A representative trying to represent multiple graduate programs will probably need to sit down and discuss the substance of those programs with faculty to better understand what’s on offer. That way she’s better positioned to sell those programs and provide better guidance to interested students to steer them towards programs that would be a better fit.\nThis specific position is probably unique to our program, but the basic idea is that this kind of communication often falls on program directors."
  },
  {
    "objectID": "posts/grad-program-director/grad-program-director.html#upper-level-administration",
    "href": "posts/grad-program-director/grad-program-director.html#upper-level-administration",
    "title": "What I learned in my first year as a graduate program director",
    "section": "Upper-level administration",
    "text": "Upper-level administration\nThis part is far more fragmented and case-specific. There are lots of senior-level positions that are going to affect you in your time as a program director. These different offices often seem to operated more or less independently of one another and this can frequently cause some headaches.\nI came into this position just as we got a new dean of the graduate school/vice provost for graduate education. She’s been great and seems to be genuinely interested in helping departments and programs to maintain, and in many cases rebuild, relationships with Ft. Leavenworth. In our case this kind of senior level support has been essential.\nIt’s also been extremely important to maintain communication to make sure various administrators are getting reliable information on our program. It became clear over the course of this year that the statistics produced by university institutional research was often poorly suited for questions administrators were asking about our program. So making sure your department is keeping reliable enrollment and graduation data can be a good idea.\nBut even as one senior administrator might work to support your program, the policies of other offices might work against you. This can be intensely frustrating and create a tremendous amount of uncertainty. In our case, the Security Studies program no longer has its own independent operating funds (it used to). Our former dean didn’t seem to have a strategy guiding our college except to collect cuts through attrition. If there was a deeper plan, it was never communicated with us. But this meant that the departments of history and political science lost something like 14 faculty over the last 6-7 years, and many of these people taught courses that were relevant to the program. We’ve also lost the majority of our funding lines to support graduate teaching/research assistants. These cuts come through our home departments and directly impact the program’s ability to recruit and retain students.\nAs this was happening, some individuals in the dean’s office and in the university’s research office have remained strong supporters of the program. They’ve tried to help our home departments and faculty find sources of funding to help sustain operations.\nUltimately the point there is to never assume that the left hand knows what the right is doing. Different administrators have very different priorities, and your ability to sustain your program is not something that registers for everyone equally. Certain decisions or policies might be bad for your program, for your department, but good for someone else, and there’s no guarantee that those decisions or policies will ever be explained to you.\nLots of this might be driven by structural factors beyond any one person’s control. Kansas has been coping with cuts to higher ed long before Covid hit. Maybe some strategies or policies would be better/worse at blunting the impacts of these big events, but it might not be any one person’s “fault” if resources are scarce. This all depends on where you are. As frustrated as I get I also definitely would not want to be a dean in this environment."
  },
  {
    "objectID": "posts/hurdle-lognormal-densities/hurdle-lognormal-densities.html",
    "href": "posts/hurdle-lognormal-densities/hurdle-lognormal-densities.html",
    "title": "Hurdle lognormal distribution densities?",
    "section": "",
    "text": "As a part of a larger project I’ve been working with BRMS and some models and family functions that are new to me. In particular, my work on troop deployments has made me think more about hurdle models. I’ve had some experience with zero-inflated models in the past, but haven’t spent a lot of time with hurdle models, specifically. Anyway, without going down the rabbit hole, I’ve been thinking about how to create probability density functions for some of these models. The {countreg} package contains some functions for hurdle negative binomial models, which got me thinking about building something similar for hurdle lognormal distributions. I’m really operating one the frontiers of my own experience/abilities, so I may be way off here, but let’s give this a shot and see if it works.\n\nBackground\nI work a lot with military deployment data. Typically these are country-year observations of the number of US military personnel stationed in various overseas locations (think Germany, Japan, etc.). So far most of our work has treated deployments as a predictor variable, but more recently we’ve started thinking more about modeling deployment levels themselves. In general, there tends to be a ton of skew in these data. For example, from 1990 forward the troop deployment data we have are more or less distributed like this:\n\n\n# Simulation . Values reflect what we see in our data.\nsims <- 1e4\nmuval = 2.8\nsdval = 2.54\npival = 0.2\n\n\nsimvals <- rep(NA, sims)\nsimvals[c(1:2000)] <- rep(0, sims*0.2)\nsimvals[c(2001:10000)] <- rlnorm(sims*0.8, meanlog = muval, sdlog = sdval)\n\n\nhist(log1p(simvals), breaks = 200, main = \"Distribution of Simulated Troop Data\")\n\n\n\n\n\n\n\n\nWe’ve got about 20% zero values, and the non-zero values have a median of 16 and a mean of about 1,700. Conceivably every country could receive deployments, but some are highly unlikely to (e.g. North Korea). But even countries that do host US personnel tend to host very small deployments, as you can see by the relatively small median value. The mean is dragged upwards by large, long-standing legacy deployments in places like Germany, Japan, and South Korea.\n\n\nProblem\nI’m glossing over a lot of the details here, but working on this has prompted me to think more about using hurdle models (like I already said). Yada yada yada, this has led me to think about what a probability density function for a hurdle model looks like. The {stats} package in R comes with a set of functions for handling lognormal distributions, but doesn’t appear to have anything to handle hurdle variants. Riffing off of the aforementioned {countreg} package, this is my attempt to create something comparable for hurdle lognormal distributions (I couldn’t find one with Elaine).\n\nSeinfeld Jason Alexander GIF from Seinfeld GIFs\n\n\n\n\nMaybe a working probability density function?\nThis is my shot at creating a probability density function…umm…function. I started with the builtin dlnorm() function, but the problem is that it only accepts positive values. If we have a lot of data that are failing to cross that hurdle (i.e. 0 values) then this doesn’t really work. I think the solution is to insert an argument that details the propotion/probability of 0 values in the data (i.e. the pval argument). For non-zero values we have to first get the probability using dlnorm(). Once we generate the probability value for the x value using dlnorm() we then have to weight that probability value by the proportion of non-zero values in the data. In this case we enter a pval argument of 0.2 since 20% of the data are 0s, but when we calculate the probability for some values > 0 we want to make sure that’s weighted by 0.8 since 80% of observations are > 0. So really this is \\(1 - pval\\). If x equals zero then the probability should (I think?) just default to the pval argument.\n\n\ndhlnorm <- function(x, meanlog, sdlog, pval) {\n  \n    if (x > 0) {\n    \n    value <- dlnorm(x, meanlog = meanlog, sdlog = sdlog, log = FALSE) * (1-pval)\n    \n    return(value)\n    \n  } else {\n    \n    value <- pval\n    \n    return(value)\n  }\n  }\n\n\nprob <- dhlnorm(3, meanlog = muval, sdlog = sdval, pval = 0.2)\n\nprint(prob)\n## [1] 0.03346686\n\nAgain, this is very much on the frontiers of my experience, so if anyone has any feedback I’d appreciate it. I have some potential applications in mind, so this is more than just running down a rabbit hole. I promise."
  },
  {
    "objectID": "posts/software-choice/software-choice.html",
    "href": "posts/software-choice/software-choice.html",
    "title": "On software choice",
    "section": "",
    "text": "tldr; If Stata or SPSS works well for what you do, that’s great—You should keep using the tool that makes you better at your job! However, as university faculty we’re not just making choices for ourselves, but for the students we teach. And those choices have a lot of knock-on effects. In general I think we should tend to err on the side of lowering barriers to entry for students and providing them with the most flexible tools for the job they want to have, but also the jobs they might have. While R has a lot of problems and quirks, I think it’s the best relative option considering these various factors.\nThis post is all about the glories of R. Not really. But maybe kind of? This recent tweet gained some traction and generated a bit of discussion on the subject of proprietary software vs open source alternatives like R:\n\n\nToo many young students are having their time and money wasted by being forced to use out of date tools like SPSS because their professors are scared of the new stuff. There. I said it. #rstats #python #datascience.\n\n— Keith McNulty (@dr_keithmcnulty) April 16, 2021\n\n\nI’m not going to delve much into the motivations for various faculty avoiding R (or even other alternatives like Python). I don’t necessarily think it’s fear driving people, but more likely they don’t see the utility or think the return on investment is low. But there are two bits that caught my attention that speak to these issues. First, the idea of wasting money. Second, the idea that tools are out of date.\nThis post is really primarily about my thoughts on switching from someone who uses R to an R user. I think this is a distinction with a meaningful difference, and it definitely pertains to some of these issues. I went from someone who was trained on and primarily used Stata for all of my work-related modeling (as well as other stuff, like tracking my cat’s weight when he was sick) to someone who uses R for just about everything, from modeling, to writing letters, to building my website. Suffice it to say, I’m currently a big fan.\nWhat this post is not intended to be is a screed about why R is objectively the best programming language/platform for social scientists. That said, I do think it’s relatively the best. I learned R about a decade ago while I was still using Stata for most of my work, but remained a very casual user and it took me another several years to make the switch. Part of this was because it felt like the learning curve was pretty long and arduous. Even now, there are a lot of things about R that are a massive pain in the ass. For example, the tedious process of updating a particular package when you have five projects up and running, or a package with a given function name masking the identically-named function you really want to call. I’m looking at you {plyr}. Even still, I lose a lot of time dealing with some of these issues, so I completely understand and even share a lot of the frustrations that people have with R. Hence the disclaimer about it not being objectively best. But, relatively I think it has a lot of qualities that are desirable, and even better than, many of the other major software packages used by social scientists. The rest of this post is intended to highlight some of these benefits.\n\nCost\nFirst, the most basic issue. Stata was always fairly expensive, but in the last year or two they announced that they were switching to an annual subscription model as opposed to the one-time charge for a perpetual license for whatever version you happened to purchase. Even when I started buying Stata as a graduate student in 2007, it was pretty expensive considering that I was making all of 17,000 per year before taxes. For students who are currently looking to purchase Stata Stata SE is running 179 per year for student pricing. The multicore variant is upwards of 375 per year. That’s a lot.\nThe upside of R here is clear—it’s free. There can be costs associated with using R and some of the packages you can run across. Querying Google Maps, for example, can incur costs if you run over a certain threshold. But if your goal is primarily to read in data, clean it, and run a linear regression you’re on solid ground.\nResources for students in many PhD programs have been getting slim for a while now. Particularly at state schools. And while faculty often have limited control over the baseline rate of pay for graduate students at these institutions, one thing we can control is the cost associated with things like books, software, etc. Is $180 going to break the bank for a graduate student? Maybe not, but it’s also lumped in with a lot of other costs that they incurring while in graduate school.\nIt’s also not unreasonable to expect that graduate students will need more powerful tools to do the kind of work they’re pursuing. Maybe work that they, or their advisers, are conducting requires more powerful software capabilities. With Stata MP running from about 300-400 per year, that’s a pretty huge cost. While faculty may have funds to pay for this kind of thing, graduate students might not. This creates a kind of capabilities gap between graduate students and faculty in a way that’s nowhere near as evident with alternatives like R. Maybe some departments have licenses to run better versions on multiple lab computers, but this is also a massive expense that many departments may not be in a position to keep up either.\n\n\nOut of date software?\nThis one’s a little trickier. I think it’s kind of true. Running a linear regression is going to be the same, regardless of what you use. If that’s all you’re doing then it doesn’t really matter, and it’s hard to be out of date on this point. But I remember a major motivation for learning R was that Stata’s graphical capabilities were extremely poor compared with the things I was seeing coming out of R. They’ve gotten better for sure. I remember being super excited in 2014 or 2015 when Stata announced that they were finally implementing transparency capabilities. I’ve always enjoyed the data visualization part of the job, so Stata’s lag in implementing capabilities that were long-since standard in R was frustrating. But that’s the thing, while running a regression is the same whether you do it in R, Stata, or SPSS, the ability to effectively communicate that information is not divorced from running the models themselves. The ability to effectively communicate information is a huge part of our jobs, so the inferior quality of data visualization capabilities in some software packages is a big problem.\nThe problem with capability lags isn’t limited to data visualization. We see it across the board in modeling capabilities as well. There’s a pretty strong assumption on the part of faculty who get by just fine with a given proprietary software platform that their students will also get by just fine with those capabilities. But this just doesn’t seem realistic. Stata finally implemented Bayesian modeling capabilities with version 15, but these capabilities had been present on other software platforms for a long time. And while Stata did finally enable users to implement Bayesian modeling, these capabilities were seriously limited compared to other packages available through R (e.g. not running multiple chains).\nThe point here isn’t that everyone needs to get into Bayesian modeling—that’s just an example—but that if students want to learn about state of the art methods, proprietary software packages often aren’t going to be the place for it. And state of the art here shouldn’t be taken as synonymous with “highly advanced” or “futuristic”. But if and when they’re interests take them into methodological territory that doesn’t overlap perfectly with yours as an adviser the standard of what’s expected may diverge considerably from what’s capable with the tools you assume are fine. When I learned social network analysis I was taught on Pajek. But looking around at what was being published in Political Science and Sociology, it seemed like everyone was using R, not Pajek. While I learned a lot of substantive knowledge, the implementational/computational skills I learned were useless and I had to go back and relearn a lot of material. Not just because of tastes, but because the software just wasn’t capable of estimating the sorts of models that were relevant to my interests. More to the point, the field in practice seemed to have advanced beyond that that particular package was even capable of.\nHaving to learn a new software package to implement what I wanted wasn’t the end of the world, but it did create a lot of costs in terms of time and effort to re-learn basic implementation issues that could have been avoided had the person who taught the course been more in line with the state of the art, so to speak. But I get it—Pajek probably worked fine for them for what they were doing. The problem is that wasn’t effective or useful for what was expected of the students they were teaching. The advantage of R in this context is that if you only need to use linear regression for your work, great. But in teaching that material to your students you’re also teaching them the basic language and ecosystem they’ll need to simply install another package that can handle material that is more relevant to their interests. That is, when you’re teaching someone in R they’ll use that environment and language for their basic stats and regression skills, but if their interests take them in directions that diverge from their instructor’s they don’t have to reinvent the wheel by learning a new platform and language. Even if you only ever use frequentist models, it’s a short jump for a student whose research interests take into into multilevel Bayesian modeling with packages like {brms}. I’d even venture to guess that it will be easier for an adviser to keep up with what the student is doing if they’re both working from a shared language as the synatx is largely the same.\n\n\nExtended benefits\nAnother advantage of R seems to be that learning it seems to have much greater utility for students who are seeking non-academic jobs. Don’t get me wrong, there are some private firms that definitely use other proprietary software packages, like Stata or SPSS. In our work we’ve dealt with survey firms who send us our data in SPSS file formats. I think that’s bad practice, but the point is the probability of being able to use some of these software packages outside of academia is not 0. That said, use of R and other languages like Python seem to be much more widespread in industry. Again, proceeding from the premise of “I can afford this software and it’s fine for my work” seems faulty when, as instructors and advisers, our role extends beyond what works for us. Not all students are going to end up in academia. Even many of those on the PhD track who aspire to get tenure track jobs will be forced out of the academic job market and into private sector jobs. Not all of them will choose to go into analytics or data science jobs (substantive skills are enormously beneficial, too), but a lot of them will. Combining substantive expertise with methodological skills that are more readily transferable across the academic/industry divide seems like another way to better prepare students for whatever is to come.\nI’d also venture to guess that R makes interdisciplinary collaboration easier. Obviously this isn’t ironclad, but my experience has generally been that different disciplines seem to cluster around particular software packages. While not insurmountable, this certainly raises the cost of conducting collaborative work with people outside of your discipline. Not just learning a new language, but figuring out how to unify workflows based around totally separate software languages and the processes those incentivize can be tricky.\n\n\nCulture\nThis last one is admittedly more idiosyncratic, but I just find the culture of the broader R community to be far more welcoming and helpful than Stata. To some extent this isn’t surprising. The people contributing to open source software development genuinely like working on what they’re doing. They have to, otherwise they wouldn’t be doing it. It’s not hard to find enthusiastic R users, but I’m not sure the same can be said for alternative platforms. This makes an enormous difference insofar as there is a massive reserve of people who are willing and happy to help when you run into problems.\nBut you don’t have to be a superfan of the software you’re using—it’s a tool, a means to an end. But there are other cultural bits that I do think make a difference. I guess I’d label this broadly as “workflow” culture. Again, this sort of thing is present with other software packages. Scott Long’s Workflow of Data Analysis Using Stata was a great resource for managing your workflow and projects. That said, I think the broader culture that has grown up around and along with R is one that is much more cognizant of tying the initially disparate elements of your projects together into a single, unified, workflow. You can see this with the marriage of statistical modeling and writing in RMarkdown, or in the development of different packages used to communicate information through web-based platforms, like {shiny}, {bookdown}, or {pkgdown}, or interfacing easily with GitHub.\nObviously a lot of this is still up to the individual user. Rather, using R by itself doesn’t automatically make you a better social scientist. It doesn’t automatically make your work more replicable. People can still be bad at their job regardless of what platform they use. What it does do is front-loads a lot of the capabilities and ancillary packages that make these things more visible and accessible. It’s more likely to make you aware that these tools even exist, whereas they tend to be more obscured on platforms like Stata.\n\n\nTakeaway?\nTo reiterate, if Stata or SPSS works well enough for what you do, that’s great—You should keep using the tool that makes you better at your job! However, as university faculty we’re not just making choices for ourselves, but for the students we teach. And those choices have a lot of knock-on effects. In general I think we should tend to err on the side of lowering barriers to entry for students and providing them with the most flexible tools for the job they want to have, but also the jobs they might have. While R has a lot of problems and quirks, I think it’s the best relative option considering these various factors."
  },
  {
    "objectID": "posts/tables-modelsummary-brms/tables-modelsummary-brms.html",
    "href": "posts/tables-modelsummary-brms/tables-modelsummary-brms.html",
    "title": "Making tables for multinomial models with {modelsummary} and {brms}",
    "section": "",
    "text": "NOTE Since moving the site to Quarto I’ve been having some trouble getting Quarto to render this particular post. Something to do witht he particular model objects that I read in in the second code chunk. I think the post is still useful, but I’m going to omit the output from the code chunks since it breaks the post. I’ll try to follow up with someone smarter than I to see how I can go about fixing things.\ntl;dr: Learn how to make some cool and customizable tables for multinomial logit models using {brms} and {modelsummary}.\nUPDATE: Vincent informed me that the most recent version of {modelsummary} relies entirely on the {parameters} package. Apparently the {broom} package will no longer be actively developing. Keep this in mind if you’re trying this approach and get stuck.\n\nBackground\nI’m coming off a couple of long projects where we were using a lot of multinomial logit models, and making publication quality tables was a major challenge. Actually, started using {brms} several years ago partly because I had data where we had 1) lots of individuals making choices, 2) those individuals were all grouped in some pretty clear ways, and 3) we were also interested in modeling group-level characteristics that might relate to individuals’ choices. This more or less marked my full transition from Stata to R and from frequentist stats to Bayesian stats.\nEarly on the biggest problem I ran into was finding a way to generate tables for multinomial models run using {brms}. Initially most packages didn’t support {brms} and/or developing tables for multilevel/hierarchical models required a lot of extra legwork. Building tables to accommodate choice models was another issue. Taken together, these problems meant that I had to write a lot of extensive code by hand to generate clean Latex tables for the models I was using. The process has, thankfully, become much simpler over the last couple of years.\nFirst, for those who aren’t familiar {brms} is an amazing package created by Paul Bürkner. It stands for Bayesian Regression Modeling using Stan, and, as the name suggests, provides users with a convenient front-end for building regression models using Stan as a back end. This is particularly useful for building multilevel models, but also lots of other stuff.\nSecond, {modelsummary} is another amazing and ever-expanding package created by Vincent Arel-Bundock. This packages does a few different things, but most importantly and prominently it helps users to create excellent tables for summarizing data and building tables for regression output. The package is incredibly flexible, supports dozens of different model types, and Vincent is constantly adding new features.\nBoth packages are fantastic. If you’re interested in Bayesian modeling, or just looking for a new and easy way to build tables for whatever modeling package you already use, you should check out one or both of these packages.\n\n\nMultilevel Multinomial Logit Models\nLots of regression models are going to be fairly simple to present in a table format, and there are some fairly easy ways to go about generating those tables. Typically you’ll have a single column per model, and each row of your table will be for a single variable. You might also have some summary statistics for the model at the bottom (think $ N $, $ R^2 $, etc.).\nMultinomial models get a little more complicated because you’ll typically have multiple outcomes. Specifically, you’ll have $k-1 $ columns to present in the table, where $k $ is the number of choices respondents have. In our case we had lots of models where survey respondents offered their assessments of various actors and we condensed those assessments down into four general categories: Positive, Neutral, Negative, and Don’t Know. This means we ultimately ended up with three columns per model, with the “Neutral” response serving as the baseline category against which the others were compared.\nMultilevel models complicate things slightly because you may also have summary statistics for the groups in your data in addition to the general summary statistics for the model.\nLast, Bayesian models and {brms} specifically provide users with a ton of additional information they might want to present beyond the traditional stuff you’d find in frequentist models. Some of this information can take a long time to compile. There’s often going to be an efficiency and transparency tradeoff here, and so you may want to customize what you present in your table. Even if you end up presenting lots of information in the end, having the ability to control what’s in your table at the outset can be really useful as you run the code to make sure the basic output looks right.\nAnyway, the goal here is rather niche, but it’s to talk through the process of building nice and readable tables when we’re using these models and have lots of information to present. {modelsummary} lets us do this, but also requires a bit of additional effort to fully customize our output.\n\n\nGetting Started\nOK, first we’re going to load our libraries. The relevance of some of these is immediately obvious given what I’ve already said, but I’ll talk more about some of the additional packages we need below.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(here)\nlibrary(modelsummary)\nlibrary(brms)\nlibrary(parameters)\nlibrary(kableExtra)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(data.table)\n\nIn addition to loading the libraries we want to load our model objects. In this case I have three models, each with three outcome categories. Each of these ends with a “p1” or a “t1” to denote the reference group for the model’s outcome variable (e.g. if the outcome variable is asking about troops, people, or government).\nThen I’m creating a list object to store the three model objects. Note here that I’m leaving the label for the individual models blank by including the empty quotation marks in the list function. Depending on your situation you can go ahead and name these if you want. In my case it makes more sense to keep them empty because I plan to add a grouping header/title later in the final table and based on how {modelsummary} works including the titles here would create some redundancies in the final output and take up extra horizontal space. If you’re working with HTML output and you have scrollable tables maybe this doesn’t matter, but it can matter a lot for print versions..\n\nm.c.t1 <- readRDS(here::here(\"files/data-files/m.c.t1.rds\"))\nm.c.p1 <- readRDS(here::here(\"files/data-files/m.c.p1.rds\"))\nm.c.g1 <- readRDS(here::here(\"files/data-files/m.c.g1.rds\"))\n\n# Create a list object to store the three separate model objects.\nmod.list <- list(\" \" = m.c.t1,\n                 \" \" = m.c.p1,\n                 \" \" = m.c.g1)\n\nBefore we move on, let’s take a quick look at the models and what they look like.\n\nsummary(mod.list[[1]])\n\nThere’s a lot going on here, but you can see from the summary output that organizing this could be a bit of a bear. We have three outcome categories, lots of categorical variables (some conceptually related), model summary statistics, etc.\n\n\nCustomization\nThe first two chunks are pretty boiler plate, and for lots of types of models you can probably just go on to use {modelsummary} directly and get some nice tables. But this section is going to dive into some of the extra steps required to make the tables look nice and clean, but also to save us a ton of time.\nThe big issue that we have to address is that {modelsummary} is going to automatically try to generate lots of different types of goodness-of-fit or model summary statistics for the models you’re including. If this were OLS it would be super quick. But since we’re using {brms} models the defaults can run for a very long time and, depending on your workflow needs, cause some major slowdowns.\n{modelsummary} is using the packages like {tidy}, {parameters}, and {broom} to extract information from the models in your list and to generate a basic data frame with that output that serves as the basis for the final table. Things like WAIC and LOOIC can take a long time to calculate, and depending on your needs you might not want them right way.\nBeyond that, you might just want to customize how the footer of your table looks, what information is included in which places, etc. This is a good way to do that, but it takes a little extra work.\nFirst, we need to assign a “custom” class to each of the three model objects stored in the mod.list object. Assigning the custom class is going allow us to write some custom {tidy} and {glance} functions that will let us select the specific summary stats and other model info that we want to include.\n\nfor (i in seq_along(mod.list)){\n  class(mod.list[[i]]) <- c(\"custom\", class(mod.list[[i]]))\n}\n\nNext we can write out custom tidy function, appropriately labeled here tidy.custom. Actually, I think you have to name it this so {modelsummary} recognizes that you want to use a custom function.\nThere’s a lot going on here, and some of it is going to be specific to the models I’m using as an example.\nFirst, you create the function, where x is the object placeholder, and conf.level=.95 is specifying the default confidence/credible interval threshold. The ... just leaves it open for other arguments.\nNext we create an object named out using the {parameters} package. First we start off creating a data frame using this function, then we standardize all of the variable/column names (i.e. make them lower case). So far pretty standard.\nThe mutate chunk is where things get more complicated, and where you’ll need to substitute your own model-specific factors. {modelsummary} also has a formula-based argument for helping you to arrange your models in the resulting table, but we need to do a little extra work here to properly identify the outcome variable levels. This is partly a function of the fact that {brms} does some weird things like attaching outcome choices (i.e. the binary outcome variable for the individual equations) as prefixes to each variable name, meaning there are no outcome levels for {parameters} to automatically detect. So we need to create them.\nAll I’m going here is using case_when(...) to identify the outcome variable levels/choices and creating a categorical y.level variable containing the full name for each of the choices for the outcome variable. As I mentioned above, those are “Don’t know”, “Negative”, and “Positive” (with “Neutral” as the omitted reference category).\nNext, I take the term column that {parameters} generates, which contains all of the predictor variables, and I remove the outcome level prefixes that {brms} attaches. This way I have two columns with variable names (term) and the outcome variable choice/model equation (i.e. y.level).\n\n# tidy method extracts level names into new column\ntidy.custom <- function(x, conf.level=.95, ...) {                                   # Create function\n  out = parameters::parameters(x, ci=conf.level, verbose=FALSE) |>                 # Call {parameters} to pull model parameter info with specified credible interval\n        parameters::standardize_names(style=\"broom\") |>                            # make names lower case\n        mutate(y.level = case_when(grepl(\"mudk\", term) ~ \"Don't Know\",              # Change outcome level values to plain meaning for output table\n                                 grepl(\"muneg\", term) ~ \"Negative\",\n                                 grepl(\"mupos\", term) ~ \"Positive\"),\n               term = gsub(\"dk|neg|pos\", \"\", term))                                 # remove outcome prefix {brms} attaches to variable names\n  return(out)\n}\n\nNext we can move on to writing a custom function to pull the relevant model and summary statistic information. First, we need to tell glance() to quiet down since it’s going to do lots of stuff we don’t necessarily want it to do right now. I can’t remember if the gof.check lines are necessary at this point (I seem to recall it had no effect one way or the other when I was initially working on this), but I’ll turn those off just to be safe, too.\n\n# Write custom glance function to extract summary information.\nglance.custom <- function(x, ...) {\n  ret <- tibble::tibble(N = summary(x)$nobs)\n  ret\n}\n\n# Turn off GOF stuff\ngof.check <- modelsummary::gof_map\ngof.check$omit <- TRUE\n\nNext we can move on to using the {parameters} package to full information on the “random” part of our varying intercepts model. We’ll also use this step to include some basic summary information on the models as mentioned above.\nUltimately the goal here is to generate a clean data frame containing summary information that we want. I’ve included more specific comments in the code chunk below for readers who want to scrutinize each step, but much of this mirrors what we just did with the tidy.custom() function above, it’s just doing it to the “random” or “varying” part of the model rather than the population-level coefficients.\n\n# Write function to loop over list of models\nrows <- lapply(mod.list, function(x){\n\n  temp.sd <- parameters::parameters(x, effect = \"random\")  |>                   # start with parameters and pull \"random\" component of model\n    filter(grepl(\".*sd.*\", Parameter)) |>                                       # filter out parameters containing standard deviation info\n    filter(grepl(\".*persYes.*|.*nonpersYes.*|.*Intercept.*\", Parameter)) |>     # further filter parameters containing SD info for relevant variables\n    dplyr::select(Parameter, Median) |>                                         # Keep only the Parameter (name) column and the Median column\n    dplyr::mutate(Median = as.character(round(Median, 2)),\n      y.level = case_when(                                                       # Like before, create a y.level column for outcome variable level/equation\n      grepl(\".*mudk.*\", Parameter) ~ \"dk\",\n      grepl(\".*mupos.*\", Parameter) ~ \"pos\",\n      grepl(\".*muneg.*\", Parameter) ~ \"neg\",\n    ),\n    Parameter = gsub(\"_mudk_|_mupos_|_muneg_\", \"_\", Parameter)) |>              # Remove the {brms} prefix from the Parameter column\n    pivot_wider(id_cols = Parameter,                                             # Rearrange  columns from long to wide\n                values_from = Median,\n                names_from = y.level) |> \n    mutate(Parameter = case_when(                                                # Rename relevant parameters to appear how you want in text\n      grepl(\".*Intercept.*\", Parameter) ~ \"sd(Intercept)\"\n    ))\n  \n  temp.obs <- tibble::tribble(~Parameter, ~dk, ~neg, ~pos,                       # Create another data frame containing observation count and grouping info\n                              \"N\", as.character(nobs(x)), \"\", \"\",\n                              \"Group\", \"Country\", \"\", \"\",\n                              \"\\\\# Groups\", as.character(length(unique(x$data$country))), \"\", \"\")\n  \n\n  temp.com <- bind_rows(temp.obs, temp.sd)                                       # Bind two data frames together for consolidated footer data frame\n\n  return(temp.com)\n  \n  }\n)\n\n# Group everything from the three models together\n# Also select relevant columns containing information\nrows.com <- bind_cols(rows[[1]], rows[[2]], rows[[3]]) |> \n  dplyr::select(1, 2, 3, 4, 6, 7, 8, 10, 11, 12) \n\n# Rename those columns so they'll match eventual output data frame names\nnames(rows.com) <- c(\"term\", \"col1\", \"col2\", \"col3\", \"col4\", \"col5\", \"col6\", \"col7\", \"col8\", \"col9\")\n\nGreat! Next we can start cleaning up the variable names for presenting them in the table, and we can even go a step further to add grouping labels to help readers move between broad categories of predictor variables (e.g. income, age, etc.).\nHere’s we’re going to generate a tribble with the “raw” variable names and the “clean” label for presentation. Note that we have to arrange them in the order in which we want them to appear.\n\ncoef.list <- tibble::tribble(~raw, ~clean,\n                            \"b_mu_contact_persYes\", \"Personal Contact: Yes\",\n                            \"b_mu_contact_persDontknowDdeclinetoanswer\", \"Personal Contact: DK/Decline\",\n                            \"b_mu_contact_nonpersYes\", \"Network Contact: Yes\",\n                            \"b_mu_contact_nonpersDontknowDdeclinetoanswer\", \"Network Contact: DK/Decline\",\n                            \"b_mu_benefit_persYes\", \"Personal Benefit: Yes\",\n                            \"b_mu_benefit_persDontknowDdeclinetoanswer\", \"Personal Benefit: DK/Decline\",\n                            \"b_mu_benefit_nonpersYes\", \"Network Benefit: Yes\",\n                            \"b_mu_benefit_nonpersDontknowDdeclinetoanswer\", \"Network Benefit: DK/Decline\",\n                            \"b_mu_age25to34years\", \"25-34\",\n                            \"b_mu_age35to44years\", \"35-44\",\n                            \"b_mu_age45to54years\", \"45-54\",\n                            \"b_mu_age55to64years\", \"55-65\",\n                            \"b_mu_ageAge65orolder\", \">65\",\n                            \"b_mu_income.5.cat21M40%\", \"21-40\",\n                            \"b_mu_income.5.cat41M60%\", \"41-60\",\n                            \"b_mu_income.5.cat61M80%\", \"61-80\",\n                            \"b_mu_income.5.cat81M100%\", \"81-100\",\n                            \"b_mu_genderFemale\", \"Female\",\n                            \"b_mu_genderNonMbinary\", \"Non-binary\",\n                            \"b_mu_genderNoneoftheabove\", \"None of the above\",\n                            \"b_mu_minorityYes\", \"Minority: Yes\",\n                            \"b_mu_minorityDeclinetoanswer\", \"Minoriy: Decline to answer\",\n                            \"b_mu_religCatholicism\", \"Catholic\",\n                            \"b_mu_religChristianityprotestant\", \"Protestant\",\n                            \"b_mu_religBuddhism\", \"Buddhism\",\n                            \"b_mu_religHinduism\", \"Hindu\",\n                            \"b_mu_religIslam\", \"Islam\",\n                            \"b_mu_religJudaism\", \"Judaism\",\n                            \"b_mu_religShinto\", \"Shinto\",\n                            \"b_mu_religMormonism\", \"Mormonism\",\n                            \"b_mu_religLocal\", \"Local Religion\",\n                            \"b_mu_religOther\", \"Other\",\n                            \"b_mu_religDeclinetoanswer\", \"Religion: Decline to answer\",\n                            \"b_mu_ed_z\", \"Education\",\n                            \"b_mu_ideology_z\", \"Ideology\",\n                            \"b_mu_troops_crime_persYes\", \"Personal Crime Experience: Yes\",\n                            \"b_mu_american_inf_1DontknowDdeclinetoanswer\", \"Influence 1: DK/Decline\",\n                            \"b_mu_american_inf_1Alittle\", \"Influence 1: A little\",\n                            \"b_mu_american_inf_1Some\", \"Influence 1: Some\",\n                            \"b_mu_american_inf_1Alot\", \"Influence 1: A lot\",\n                            \"b_mu_american_inf_2DontknowDdeclinetoanswer\", \"Influence 2: DK/Decline\",\n                            \"b_mu_american_inf_2Veryative\", \"Influence 2: Very negative\",\n                            \"b_mu_american_inf_2Negative\", \"Influence 2: Negative\",\n                            \"b_mu_american_inf_2Positive\", \"Influence 2: Positive\",\n                            \"b_mu_american_inf_2Veryitive\", \"Influence 2: Very positive\",\n                            \"b_mu_basecount_z\", \"Base count\",\n                            \"b_mu_gdp_z\", \"GDP\",\n                            \"b_mu_pop_z\", \"Population\",\n                            \"b_mu_troops_z\", \"Troop deployment size\",\n                            \"b_mu_Intercept\", \"Intercept\")\n\nNext we’re going to add a new column that contains the grouping list. Since we’re using lots of categorical predictor variables we want to make sure they’re grouped in a sensible way.\n\ncoef.list <- coef.list |> \n  mutate(group = case_when(\n           grepl(\".*ontact.*\", raw) ~ \"Contact Status\",\n           grepl(\".*enefit.*\", raw) ~ \"Economic Benefits\",\n           grepl(\".*age.*\", raw) ~ \"Age\",\n           grepl(\".*ncome.*\", raw) ~ \"Income Quintile\",\n           grepl(\".*gender.*\", raw) ~ \"Gender Identification\",\n           grepl(\".*minority.*\", raw) ~ \"Minority Self-Identification\",\n           grepl(\".*relig.*\", raw) ~ \"Religious Identification\",\n           grepl(\".*ed_z.*\", raw) ~ \"Education\",\n           grepl(\".*ideology_z.*\", raw) ~ \"Ideology\",\n           grepl(\".*crime.*\", raw) ~ \"Crime Experience\",\n           grepl(\".*inf_1.*\", raw) ~ \"American Influence (Amount)\",\n           grepl(\".*inf_2.*\", raw) ~ \"American Influence (Quality)\",\n           TRUE ~ \"Group-Level Variables\"\n         )) \n\nNext, because we’re dealing with a really wide table we’ll have coefficients with standard errors underneath. This means that each variable name is actually going to take up two lines. So we need to do a little extra here to properly format this bit. I have to admit I think Vincent actually came up with this particular solution as I was puttering with it for a while. So more props to him!\n\n# Find how long the coefficient list is for the final table hline\nlast.line <- length(coef.list[[1]]) * 2\n\ncoef_map <- setNames(coef.list$clean, coef.list$raw)\nidx <- rle(coef.list$group)\nidx <- setNames(idx$lengths  * 2, idx$values)\n\n\n\nPutting it all together\nFinally, we’ve done all of the prep work. Now we can generate the actual table itself! I’m going to change just a couple of things here. Since the output is going to appear on a webpage, I’ll change the “output” argument to HTML. I’ll also delete the save_kable() bit that tells it to save to a latex file, but you can add that in at the end if you want output.\nYou can see that we use the last.line values from the previous chunk in that last row_spec() line to tell it where to put an horizontal line.\n\nmodelsummary::modelsummary(mod.list,\n                  estimate = \"{estimate}\",\n                  statistic = \"conf.int\",\n                  fmt = 2,\n                  group = term ~ model + y.level,\n                  gof_map = gof.check,\n                  coef_map = coef_map,\n                  add_rows = rows.com,\n                  stars = FALSE,\n                  output = \"kableExtra\",\n                  caption = \"Bayesian multilevel multinomial logistic regressions. Population level effects. \\\\label{tab:contactfull}\") |> \n  kable_styling(bootstrap_options = c(\"striped\"), font_size = 10, position = \"center\", full_width = FALSE) |>\n  add_header_above(c(\" \", \"US Presence\" = 3, \"US People\" = 3, \"US Government\" = 3)) |> \n  group_rows(index = idx, bold = TRUE, background = \"gray\", color = \"white\", hline_after = TRUE)  |>  \n  row_spec(last.line, hline_after = TRUE) |> \n  column_spec(1, width = \"5cm\") |>\n  column_spec(2:10, width = \"3cm\") |> \n  kableExtra::scroll_box(width = \"100%\", height = \"600px\") \n\n\n\nWhat’s left?\nThere are a few things I’d like to tweak. For example, {modelsummary} adds little slashes before the outcome variable name and inserts the name from the model list before that. I leave the list entries blank to avoid this since horizontal space is a premium.\nBlogdown/hugo is also being a bit frustrating with the final table. I’ve tried making the font larger, but for whatever reason the horizontal scrollbar isn’t working with the kableExtra table in this environment. Not great, but it looks ok for now. Sizing and scale were a littl tough to tweak in Tex files, but I got it more or less where it needed to be by the end. I’m sure someone with a little more skill than me can figure out some of the hiccups.\nThere’s way more you can do to cusomtize the output of these tables, particularly in the footer. You might want to add particular model fit or summary statistics, and the glance.custom() function is a great place to specify what you want. Much is possible, but I’m going to stop here for now since it took me something like a month to get this posted."
  },
  {
    "objectID": "posts/whats-the-difference/whats-the-difference.html",
    "href": "posts/whats-the-difference/whats-the-difference.html",
    "title": "What’s the Difference?",
    "section": "",
    "text": "In the social sciences we talk a lot about groups being different from one another. We might also talk about things like spending on certain programs going up or down over time. While this can seem direct, comparing quantities of interest can often be made complicated by a variety of factors. Below I’ll provide an overview of some of the issues I want you to be aware of as we begin to read journal articles and other readings that will invariably discuss inter-group and inter-temporal differences."
  },
  {
    "objectID": "posts/whats-the-difference/whats-the-difference.html#comparing-two-groups",
    "href": "posts/whats-the-difference/whats-the-difference.html#comparing-two-groups",
    "title": "What’s the Difference?",
    "section": "Comparing two groups",
    "text": "Comparing two groups\nComparing group behaviors can be complicated because in addition to differences between the groups, there is also variation in how the individual members of those groups tend to behave. Let’s imagine that we want to compare bipartisanship in US foreign policy. There are different metrics we could use, but we’ll settle for Congressional voting patterns. We might want to know if Democrats are more or less bipartisan than Republicans. We could imagine that we’re going to sample votes taken by members of Congress where each member of Congress votes on a series of votes during each year—let’s say 100 votes—and we could code those votes as bipartisan (1) or not (0). Rather than collecting data on every vote we decide to just sample from one year. At the end we might end up with a distribution of bipartisan votes that looks like this:\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe figure shows the distribution of the percentage of the 100 votes each legislator makes that are bipartisan—A value of 60 means that 60 of the 100 votes, or 60% of the 100 votes, a given legislator makes are bipartisan and 40% are not. We’ll focus on the raw count of votes rather than percent for now, but the 100 vote total simplifies this to make these voting totals more intuitive for present purposes. Dealing with proportional measures can get more complicated, but we’ll set some of that aside for now."
  },
  {
    "objectID": "posts/whats-the-difference/whats-the-difference.html#statistical-methods-for-determining-differences-between-groups",
    "href": "posts/whats-the-difference/whats-the-difference.html#statistical-methods-for-determining-differences-between-groups",
    "title": "What’s the Difference?",
    "section": "Statistical methods for determining differences between groups",
    "text": "Statistical methods for determining differences between groups\nSo are Republicans and Democrats different? Are they meaningfully different? How can we tell? The idea here is that when we’re dealing with data of the kind shown in the above example, we want to understand if the differences between two groups are 1) systematic or are they the result of chance, and 2) how big or substantively meaningful are the differences we observe? Making these sorts of judgements by looking at this figure alone is difficult.\nFor example, a simple comparison of the mean values of the two parties suggests that Democrats have a higher rate of bipartisanship than do Republicans. But the data shown above are also very noisy, they represent just a sample from a broader set of votes, and there’s quite a bit of overlap between members of each group, so we can’t be sure from casual observation that the differences we think we observe are accurate representations of broader differences between the two groups. This is where we turn to some basic statistical methods that can help us to estimate if and how two or more groups might be different from one another with respect to some outcome of interest.\n\nDifference of Means Tests\nThe sorts of statistical methods used by researchers vary quite a bit depending on the question at hand, but for this basic example we can start with something really simple called a difference of means test, or a t test. This test allows us to look at the values of two continuous variables and compare their mean values. Another way to think of this is to compare the values of a given variable across two groups. In our example we’re interested in comparing the mean number of bipartisan votes for Republican and Democratic legislators. So political party represents our grouping variable and the outcome of interest is the count of the number of bipartisan votes.\n\n\n\n\nDifference of means test of bipartisan voting\n \n  \n    Parameter1 \n    Parameter2 \n    Mean_1 \n    Mean_2 \n    Diff \n    CI \n    CI_low \n    CI_high \n    t \n    df_error \n    p \n    Method \n    Alternative \n  \n \n\n  \n    Republican \n    Democrat \n    44.88 \n    50.51 \n    -5.63 \n    0.95 \n    -6.6 \n    -4.67 \n    -11.47 \n    406.57 \n    0 \n    Welch Two Sample t-test \n    two.sided \n  \n\n\n\n\n\nThe output in @ref(tab:t-test-example) resembles the output for a regression model that we’ll discuss below, but is much simpler. It tells us the names of the two groups, the mean score for each of the two groups, the difference between those mean scores, and the confidence interval, confidence interval limit values, and the test statistic or t value. There is also more information contained in the table about the type of t test and the direction of the test. Sometimes you might want to look at whether or not a group has a score that is higher or lower than another group, meaning that you have some specific direction in mind. In our case we’re content to just look at a non-directional test to see if the means of the two groups are different without hypothesizing the direction of that difference in advance. For now our basic hypothesis is that the groups are different, and our null hypothesis is that there is no difference between the two groups.\nSo is there a difference in the bipartisan voting behavior of the two parties? The t test shows that the Republican score is indeed lower than the Democratic score, and that this difference is statistically significant at the 0.95 level. The Democratic score is 5.63 higher than the Republican score as can be seen in the “Diff” column, and the test yields a test statistic of -11.17. This translates to a very low p value, which indicates the probability of seeing a test statistic greater than or equal to the observed test statistic of the null hypothesis were true. This is a lot, I know, but the important point is that researchers generally want low p values.\nSo this result is statistically significant, but is it substantively significant? That’s up to the researcher and requires some knowledge of the specific topic and/or broader domain expertise. In this example we see that, on average, Republican legislators case a bipartisan vote about 45% of the time and Democratic legislators case a bipartisan vote about 50% of the time. This this case we’re looking at a difference of approximately 5 percentage points. Again, whether or not this is substantively meaningful requires a lot of additional context, and my purpose here is less to tell you the answer to that specific question for this hypothetical and more to help you to learn to be thoughtful and cautious when consuming statistical analyses.\n\n\nRegression Models\nThe t test represents a very simple approach to analyzing the differences between two groups. While this can be useful, sometimes our data and the relationships between variables of interest are too complicated for a simple t test. In some cases we might want to know how groups compare when taking into account a number of other factors. Regression models are a broad class of statistical tools that allow us to model more complex relationships among various predictor and outcome variables. A fuller treatment of regression models is more than I want to get into for the purposes of this class, but there are a couple of points I want to highlight.\n\n\n\n\nLinear Regression Predicting Bipartisan Votes\n \n  \n      \n    Model 1 \n  \n \n\n  \n    Republicans \n    −5.634*** \n  \n  \n     \n    (0.487) \n  \n  \n    Intercept \n    50.509*** \n  \n  \n     \n    (0.331) \n  \n  \n    Num.Obs. \n    434 \n  \n  \n    R2 \n    0.236 \n  \n  \n    R2 Adj. \n    0.234 \n  \n  \n    AIC \n    2643.2 \n  \n  \n    BIC \n    2655.4 \n  \n  \n    Log.Lik. \n    −1318.600 \n  \n  \n    F \n    133.598 \n  \n  \n    RMSE \n    5.05 \n  \n\n\n + p \n\n\n\n\nFirst, Table @ref(tab:linear-regression-example) shows the results of a regression model where we model bipartisan votes as a function of party identification. There’s a lot here but we’re only going to focus on a couple of points.\nFirst, the higlighted row with the red text is really what we’re interested in when comparing the two parties as we discuss above. The left column shows us the name of the variable—in this case it’s the group we’re interested in, which is the political party of the observed legislator. The right column shows us the correlation coefficient. Basically this tells us how big the difference is between the Democratic legislators (the reference group here) and the Republican legislators. Note that the number listed here, -5.634, is the same as the number listed in the t test above!\nThe asterisks next to the coefficient represent the level of statistical significance associated with that coefficient. This is often used as a marker of importance, but for reasons we’ve discussed briefly above, this isn’t always a great basis on which to judge the substantive significance of a result.\nWhere do these stars come from? Well, the number in the parentheses below the coefficient is the standard error of the coefficient. Basically, you divide the coefficient value by the standard error (i.e. \\(-5.64/0.48\\)) and (according to some arbitrarily defined conventions) a coefficient earns another asterisk every time that resulting ratio crosses a particular threshold. Don’t worry about that too much right now.\nThe Intercept represents the expected value of the outcome when the other predictor variables are set to 0. In this case the intercept represented the mean value of the bipartisan voting indicator (our outcome) when the Republican variable is set to 0. In other words, this is the mean score for the Democrats! See above in the t test table to check it out (there’s some minor rounding)!\nThere are other details here that can be useful, like the “N” or the number of observations. This tells you how many times we’re observing the phenomenon of interest. We often want more information, but a larger N doesn’t solve all our problems if there are other serious flaws with our measurement strategy or research design.\nAt the bottom of the table you can often find a listing of the significance thresholds. Each symbol corresponds to a particular p value. Remember that we derive these p values from the coefficient and the standard error. You can have very very small p values for coefficients with very very small substantive effects, so this is not equivalent to the importance of a coefficient. I can’t say that enough!"
  },
  {
    "objectID": "posts/military-phds/military-phds.html",
    "href": "posts/military-phds/military-phds.html",
    "title": "Military Officers Pursuing PhDs: Are Three Years Enough?",
    "section": "",
    "text": "tl;dr: My goal here is to catalog what ended up being a fairly wide ranging (and I think informative) series of tweets from lots of different people on timelines for military students pursuing their PhD in political science or related fields. A lot of folks brought a lot of different perspectives to this question, but Twitter isn’t always the best format for organizing material in a clear and coherent way."
  },
  {
    "objectID": "posts/military-phds/military-phds.html#background",
    "href": "posts/military-phds/military-phds.html#background",
    "title": "Military Officers Pursuing PhDs: Are Three Years Enough?",
    "section": "Background",
    "text": "Background\n\n\nI don’t know who in the military is responsible for creating these programs that are sending officers out with the expectation that they’re going to readily get a PhD in three years, but stop it.\n\n— Michael Flynn (@flynnpolsci) December 10, 2021\n\n\nAre three years enough time for a military student to complete a PhD program in political science or related fields? The original post that sparked this exchange is below, but I’ll include other key followup posts, too.\nMany active duty military officers who are interested in pursuing a PhD do so through some different avenues. I’m not going to claim expert knowledge as to all the pathways, but one that often pops up is the Advanced Strategic Planning and Policy Program (ASP3). The US Army’s School of Advanced Military Studies describes the ASP3 program as follows:\n\nThe Advanced Strategic Planning and Policy Program is a multi-year program that prepares field-grade officers for service as strategic planners through a combination of practical experience, professional military education, and a doctorate from a civilian university. Once selected for the program, officers apply to doctoral programs at respected American universities in a liberal arts field of study related to strategy. They spend up to two years in graduate school satisfying all course and exam requirements leading to acceptance as a doctoral candidate. During these years, officers will also attend professional military education at the School of Advanced Military Studies (SAMS) at Fort Leavenworth, Kansas studying history, strategic theory, and the practice of strategic planning. Officers will then serve a developmental assignment in a strategic planning position. Those officers selected for battalion or brigade command will be afforded the opportunity. After the developmental assignment, the officer will spend one year working full time on the dissertation at SAMS or another suitable location and then be available for utilization as a strategic planner.\n\nAs outlined in the description, the general idea is to allocate three years to the completion of a PhD—two years of coursework and one year of dissertation writing. Contrast this with civilian programs in the US that often follow a path like this:\n\nCoursework. Usually 2-3 years of coursework, with a combination of general field seminars, methods seminars, and more focused subfield seminars. Specific requirements will vary by department. For example, we were required to take a set of core seminars in all subfields, a sequence of five quantitative methods courses (3 in statistics/econometrics, and two in game theory). We didn’t offer a qualitative methods course, I now wish we did, but you can imagine that extends, or makes more variable, what a given methods sequence might look like.\nExams. We had Qualifying Exams in year 1 or 2, and everyone took comprehensive exams after five semesters of coursework,\nThe prospectus. Basically your plan for your dissertation that you write and defend. Some are shorter (15-25 pages) and others can be longer (mine was ~60 pages).\nThe dissertation. Again, there are different models to follow here. Some write the “three paper” model, others follow more of a book model. A lot of this depends on advisory, department, program, topic, etc.\n\nThere are a number of difficulties in assessing timelines here as we’re not exactly comparing apples to apples. Here are a few points to consider that might alter baseline expectations:\n\nCivilian students are often working as teaching or research assistants (TA/RA) while taking courses and working on the dissertation.\nMilitary students often have full funding, meaning they’re not required to work as an TA/RA.\nUltimate career trajectories aren’t the same. Military students are often going to go back to the force while civilian students are likely training for a career in academia, NGOs, the private sector, government, etc.\nThe academic job market can introduce distortions into the timeline as there are a lot of external factors that affect the date of completion, and at a certain point you’re often mostly done and waiting to get a job before you officially “complete” the program.\n\nI’m going to try to block off the main arguments for and against this time frame below. Let me also declare a couple of caveats before diving into more detail. First, because I’m the one writing this post and assembling the material there’s inevitably going to be a bit more content pertaining to my own viewpoint here. I came out of a political science program with more of a quantitative/statistical focus in terms of research methodology. I’m also currently the Director of Security Studies at Kansas State University—a program that’s run by both political science and history departments. We deal overwhelmingly with Army officers. Last, I can’t include all of the responses, but if you’re interested you should comb through the thread to check out what others think (especially some of the respondents who are or were active duty and dealing with this very issue)."
  },
  {
    "objectID": "posts/military-phds/military-phds.html#whats-a-phd-for-anyway",
    "href": "posts/military-phds/military-phds.html#whats-a-phd-for-anyway",
    "title": "Military Officers Pursuing PhDs: Are Three Years Enough?",
    "section": "What’s a PhD for, Anyway?",
    "text": "What’s a PhD for, Anyway?\nFirst, let me explain what I think the value of the PhD is, and why (presumably) people would want to get one. There are a couple of dimensions to this question. First, a PhD is a research degree—it’s a degree focused on evaluating existing knowledge and producing new knowledge.\nDo you need one to do research? No, but getting one offers you an opportunity to set aside a prolonged period of time to train under established experts and to learn as a part of a broader cohort/community. In the social sciences the emphasis is often on learning to synthesize theoretical and empirical insights from various sources, and using these insights to develop your own research program. For military officers, specifically, it offers them an opportunity to acquire skills that are perhaps not readily available through their regular training and educational processes.\nSecond, for officers who are pursuing a PhD at a civilian university it offers them a chance to get out of the bubble of the military. Taking time to embed oneself in a civilian institution, to learn from civilian experts, and to work alongside civilians in their broader cohort. More succinctly, there’s a socialization aspect here that may not be as readily available in venues that are predominately military students.\nOverall, it’s about having the time to really immerse yourself in material and a community that is focused primarily on developing research skills and building knowledge. For people pursuing an academic path these skills and experiences will be beneficial for obvious reasons. And military students who may soon retire and/or transition into the private sector or government will find these skills useful for lots of the same reasons.\nBut what about military students who will likely remain in the military? Lots of the skills students develop while pursuing graduate education at civilian institutions will continue to be of use for students once they return to their day jobs in the military. Often the questions that motivate these students during their PhD studies arise from their time in uniform. Ideally they will return to their jobs better equipped to tackle some of these problems.\nI once had a student who had been deployed to Afghanistan and was tasked with assessing the efficacy of certain measures on insurgent activities. This experience worked out great for a measurement paper assignment in one of his seminars. Another student coming from a logistics background did some great work on the relationship between transportation infrastructure and the efficacy of UN peacekeeping operations.\nUltimately these students seem to have been able to effectively use their time to improve develop skills to allow them to better perform the tasks they were already being asked to do in the military.\nThe question then is what amount of time is “enough” to distinguish the PhD from other opportunities, like an MA degree? Students will cover a lot of the same ground in a one or two-year MA program as compared to a three-year PhD program, so what is the value added that makes an accelerated PhD program “worth it”?"
  },
  {
    "objectID": "posts/military-phds/military-phds.html#is-three-years-enough-time-to-earn-a-phd",
    "href": "posts/military-phds/military-phds.html#is-three-years-enough-time-to-earn-a-phd",
    "title": "Military Officers Pursuing PhDs: Are Three Years Enough?",
    "section": "Is Three Years Enough Time to Earn a PhD?",
    "text": "Is Three Years Enough Time to Earn a PhD?\nSo can it be done in three years? I’ve given this some thought in the intervening months and the short answer is “Yes, it can be done, but it’s not ideal”.\nTo be clear, I don’t mean “ideal” from an academic-training-other-academics perspective. I mean ideal from the perspective of maximizing student success, ensuring sufficient value added from the PhD program, and return on investment from a taxpayer dollar perspective. I’ll return to address these issues below.\nUltimately I think this path can work really well for some people, but I worry that we’re leaving a lot on the table, and even a fourth year would go a long ways towards optimizing outcomes.\nI’ll come back to these concerns below. For now, I’ll divide this section into two sections to capture the general arguments in support of, or in opposition to, this time frame. Let’s tackle some of the supportive arguments first.\n\nYes, it’s Enough\nFirst, let’s go over some of the arguments in favor. Several respondents to the thread think three years as enough, or nearly enough. One particularly important feature that some folks noted is that military officers who choose to pursue these programs are often highly motivated and further along in their careers.\nPhilip Hultquist, a professor at the School of Advanced Military Studies at Ft. Leavenworth, discusses some of his experiences with military students pursuing PhDs in this sub-thread.\n\n\nMichael, I shared your skepticism when I first learned of it too, but since working @us_sams I’ve seen it done. I’m on a diss committee and have given advice to several students. Though I teach in a different program I can provide some context. A quick 🧵\n\n— Philip Hultquist (@HultquistPhilip) December 10, 2021\n\n\nPhilip notes that for military officers pursuing a PhD, the PhD is their full time job. They don’t need to worry about funding, grants, RA/TA responsibilities, etc. The military’s financial backing and ability to focus strictly on the degree program is a huge benefit with respect to finishing an accelerated timeline.\nImportantly, in addition to selecting for high achievers, these students typically have an MA degree in hand already when they enter a PhD program. Our own Security Studies program allows students to transfer up to 30 hours of coursework from a previous MA degree towards PhD coursework. So these students are 1) already familiar with graduate-level work, 2) probably have some prior exposure to the relevant literature, and 3) possibly already have some sort of methods training.\nThis prior experience can be a determining factor. Sheena Greitens, a professor at the LBJ School at UT-Austin, makes a good point about the fact that faculty at civilian universities should not automatically discount the prior educational and research experience military officers may have.\nThis impulse seems to reflect a difference between US and European programs, where the former typically require PhD students with an MA to repeat coursework and the latter will accept MA experience towards the doctorate. Elsewhere in this conversation she also makes the point that dealing with these timelines may require programs and/or advisers to make adjustments to their approach and expectations.\n\n\nLBJ requires a masters first. Similarly, when I did the Oxford MPhil, DPhil was a 3-yr option after that. Doesn’t work for every project (didn’t for mine, so I came back to US!). But the idea that everyone needs 5+ yrs regardless of prep beforehand strikes me as too rigid.\n\n— Sheena Chestnut Greitens (@SheenaGreitens) December 11, 2021\n\n\nFurthermore, people in the military may have pressing considerations beyond the degree itself. Nick Frazier offers some thoughts on career considerations that make longer program duration less feasible/desirable for the Army and military officers. In particular, he highlights the lengthy time period that officers “owe” to the military subsequent to finishing their PhD, and also difficulties assessing an officer’s time in graduate school when it comes to promotions.\n\n\nI don’t know who in the military is responsible for creating these programs that are sending officers out with the expectation that they’re going to readily get a PhD in three years, but stop it.\n\n— Michael Flynn (@flynnpolsci) December 10, 2021\n\n\nI don’t take this to mean that PhD timelines should be shortened to provide special accommodations to the US military. Rather, I think this is akin to the mantra “a good dissertation is a finished dissertation.” If military officers can finish in three years, there are very good reasons to do so.\nIn short, military students who are pursuing PhDs are often selected because they have the relevant prior experience, are high achievers, have financial backing that alleviates pressures others face, and have strong personal and professional incentives to succeed. Furthermore, the expectation that longer timelines are necessary is partly a function of more rigid programs and faculty advisers.\n\n\nNo, It’s Not Enough\nThe section title is a slight misrepresentation of my views on this since I think it’s possible and fine for some folks. More accurately, I think a four year PhD track would be desirable for lots of reasons. As I said above, I think there are lots of benefits to military officers pursuing PhDs, and I think we need to make sure that there is sufficient value added to ensure the degree is meaningfully different from another MA degree.\nAt the most basic level three years is, well, only three years. That’s just not a lot of time for things to happen, no matter how driven a student may be.\nFirst, two years of coursework is probably fine. That’s a little bit less than what a lot of PhD programs require, it’s probably OK if we’re generally selecting students who have already received an MA degree in the chosen field prior to enrolling in a PhD. In such cases we’re totaling around 4 years of coursework, which is more than enough.\nMy concerns really kick in with the prospectus and dissertation-writing process (but a little before, too). One year is not a lot of time to write and defend a prospectus, and then write and defend a dissertation. Can it be done? Sure, but it assumes a lot of things go right. And I think this is where the pressure really falls on admissions committees and faculty to try to screen for those candidates who are most likely to succeed under the given framework.\nBut here’s the thing I worry about—this doesn’t allow a lot of room for failure. And I don’t mean completely failing the program and being forced to leave. Research and the production of knowledge are inherently messy processes, and not everything goes right on the first go around. Lots of smart people fail at getting a PhD because it’s not just about reading and retaining knowledge, but about synthesizing insights from existing work and using those to create something new. Coding rules require revising, inclusion criteria may need a tweak, whatever. Anyone who has collected original data as a part of their dissertation (or any project) can probably attest to this fact. And this is coming from a guy who had to write two prospectuses because my first one was hot garbage.\nAgain, can these hurdles be overcome? Yes, but I worry that for those who don’t fit the neat and clean model, or those who have some unexpected hurdles arise, will be pushed over their time limits or never selected to begin with. Maybe they’ll still finish, but I’m willing to bet there’s much more uncertainty once they’re back in their regular jobs.\nSome of this is echoed in tweets by Will Winecoff, an associate professor at the University of Indiana, and a followup tweet by Dan Drezner, a professor at Tufts University. Both note that while the students they’ve worked with are strong, many (most?) don’t finish within three years.\n\n\nIn my experience it doesn’t get done in 3 years, but it does get done. These folks are highly-motivated and have long track records of diligence. I have a lot of respect for the ones who enter these programs.\n\n— W. K. Winecoff (@whinecough) December 10, 2021\n\n\nI also worry that this limits the variability in what, and how, people research during the PhD process. Relying on existing sources, whether it’s quantitative or qualitative data, or more historical work or case studies, is suitable for lots of dissertations. But there are also lots of civilian dissertations that require field work, interviews, and other research tools that require more time and likely draw out the pre-writing process. Even lots of more quantitative dissertations may require more substantial methods training.\nClose advising, using seminar papers as chapters, and a three-paper dissertation model may be great for getting people across the finish line in the allotted time, but it seems to me that they necessarily preclude certain educational/research pathways and impose a substantial degree of homogeneity on the type of research military officers are able to conduct. I worry we’re leaving a lot of potentially good work on the table as a result.\nThis timeline also imposes a barrier for military students who seek to deepen methodological expertise. For civilian students who are going into academia they also have plenty of time after getting the PhD to spend more time learning/developing methods, improving language skills, spending time in their chosen country or region, etc. Military officers may not have this flexibility, and so maximizing what they can learn while still pursuing the PhD is perhaps more important for them.\nI think some folks will call back to the previous section and note that the time spent working on the MA also also contributes to these goals. Maybe. Lots of students at the MA level don’t necessarily know that they’ll be going on to pursue a PhD. Professional and research interests also change over time. As I note above, this presumes lots of things go right. So it’s not clear to me that the cumulative time is equivalent to allowing more time to pursue the PhD.\nThe point here is not to simply be training military officers as academic researchers. The point is that many of their projects are motivated by the substantive concerns of their jobs, and we should want to train them as effectively as possible so the PhD brings real value added when they return to those jobs. Learning about research design, measurement, interview techniques, etc., are all skills that map directly onto what a lot of our students have done on previous deployments.\nLast, beyond having additional time to develop their own skills, the time where military students are interfacing with civilian faculty and students is, itself, highly valuable. You learn a lot about intellectual collaboration outside of seminars while working alongside your colleagues. Also, I think there’s tremendous normative value in these programs because they break down the barriers between the civilian and military worlds."
  },
  {
    "objectID": "posts/military-phds/military-phds.html#takeaways",
    "href": "posts/military-phds/military-phds.html#takeaways",
    "title": "Military Officers Pursuing PhDs: Are Three Years Enough?",
    "section": "Takeaways",
    "text": "Takeaways\nSo who’s right? ¯\\_(ツ)_/¯\nI think there are lots of reasons why a three year track can make sense for some people, and they can indeed be very successful. But I think even an additional year would bring tremendous benefits to everyone involved for lots of reasons.\nBut saying a four year track is better should not be taken as a call to scrap these relationships. Ultimately I think having military students in our programs and giving them an opportunity work alongside civilians is much better than not having them around at all. We get lots of bright students coming through our program, and lots of other folks have shared similar assessments/experiences. The challenge under the current system, then, is to figure out how faculty and programs can work with military students to maximize their chances of success, and what they take away, while we have them."
  },
  {
    "objectID": "posts/hurdle-lognormal-densities-ii/lognormal-densities-ii.html",
    "href": "posts/hurdle-lognormal-densities-ii/lognormal-densities-ii.html",
    "title": "Hurdle lognormal densities, take II",
    "section": "",
    "text": "I previously wrote about a project in which I was attempting to figure out how to build a probability density function for a hurdle log-normal model. Ultimately I kind of left this topic hanging because I wasn’t really sure that the solution I had settled on was correct. The good news is, it was. The bad news is, I spent quite a bit of time trying to solve a problem that already had a solution. Still, it was probably good practice to just work through the problem on my own, even if it was somewhat moot. All that said, it’s probably also useful to run through the problem again and the answer.\nTo quickly rehash, we’re dealing with U.S. troop data reported on a country-year basis from 1950-2020. The data have a long right tail and are truncated at 0. There are also a lot of 0 values. The following figure shows some simulated data to give you a rough idea of what the distribution of the data look like.\n\n# Simulation . Values reflect what we see in our data.\nsims <- 1e4\nmuval = 2.8\nsdval = 2.54\npival = 0.2\n\n\nsimvals <- rep(NA, sims)\nsimvals[c(1:2000)] <- rep(0, sims*0.2)\nsimvals[c(2001:10000)] <- rlnorm(sims*0.8, meanlog = muval, sdlog = sdval)\n\nggplot(as.data.frame(simvals), aes(x = log1p(simvals))) +\n  geom_histogram(bins = 100, fill = \"dodgerblue1\") +\n  theme_minimal() +\n  labs(x = \"log(Troops)\",\n       y = \"Count\",\n       title = \"Simulated troop deployment values\")\n\n\n\n\n\n\n\n\nUltimately I want to be able to calculate probability values for specific values of the troops variable. The {stats} package has a built-in probability density function for lognormal distributions. The trick was to produce one that would work with a hurdle lognormal distribution like the one shown above. The function I settled on is below:\n\n\ndhlnorm <- function(x, meanlog, sdlog, pval) {\n  \n    if (x > 0) {\n    \n    value <- dlnorm(x, meanlog = meanlog, sdlog = sdlog, log = TRUE) + log(1-pval)\n    \n    return(exp(value))\n    \n  } else {\n    \n    value <- pval\n    \n    return(value)\n  }\n  }\n\n\nprob <- dhlnorm(3, meanlog = muval, sdlog = sdval, pval = 0.2)\n\nprint(prob)\n## [1] 0.03346686\n\nThis just uses the dlnorm() function to calculate probability values for cases where troops \\(>0\\). Note that I have it set to return the log of the probability value. This allows us to add the log of \\(1-pval\\), which is just one minus the hurdle probability value. This ultimately gives us the log of the probability of the chosen value, adjusting for the 0 values in the distribution. We just exponentiate that value to get the return the probability value we’re interested in. In this case that’s \\(\\approx\\) 0.0335. This is a little different than what I did in the previous post, but gets us the same result with a little more flexibility.\nSo is this right?\nTurns out the {brms} package already has a suite of functions that calcualte these quantities of interest. As is often the case, even though I’ve been running some hurdle lognormal models, I missed the probability density functions corresponding to these distributions. Using the simulated data we can see if that function matches the homemade one above.\n\n\nprob.brms <- brms::dhurdle_lognormal(3, mu = muval, sigma = sdval, hu = 0.2, log = FALSE)\n\nprint(prob)\n## [1] 0.03346686\nprint(prob.brms)\n## [1] 0.03346686\n\nNote that the arguments in the {brms} function have different names, but otherwise things look good! The probability values are identical. As another experiment, let’s look at the probability values for various values of the troops variable that we actually observe. Deployments usually run from about 0 up through a max of about 500,000 during the Vietnam War. This is relatively brief, though. Most of the higher values cluster around 150,000-200,000 in Germany during the Cold War. The following loop just compares the home-rolled function (the black line) with the {brms} function (the red line) as a further check to make sure I didn’t screw anything up.\n\n\nxval <- seq(0, 14, length = 100)\nprob.test <- rep(NA, 100)\nprob.brms <- rep(NA, 100)\n\ntest.df <- data.frame(xval = xval, \n                   prob.test = prob.test,\n                   prob.brms = prob.brms)\n\nfor(i in 1:100) {\n  xval <- test.df[i, 1]\n  test.df[i, 2] <- dhlnorm(x = xval, meanlog = muval, sdlog = sdval, pval = 0.2)\n  test.df[i, 3] <- brms::dhurdle_lognormal(x = xval, mu = muval, sigma = sdval, hu = 0.2, log = FALSE)\n}\n\nggplot(test.df, aes(x = xval)) +\n  geom_line(aes(y = prob.test), size = 2.5) +\n  geom_line(aes(y = prob.brms), color = \"red\", size = 1) +\n  theme_minimal() +\n  labs(x = \"log(Troops)\",\n       y = \"Probability\")\n\n\n\n\n\n\n\n\nLooks good! Overall this was a nice learning experience, and even though I could have arrived at the same conclusion with a pre-assembled function, it felt good to dig into the process on my own since this isn’t something I’m typically thinking about.\nThe next step will be integrating this into some marginal structural models so we cal calculate some inverse probability of treatment weights. This is something I’ve been working on for a while now, but have stuck with basic Gaussian distributions for the treatment weighting models. Given the distribution of the data this produces some ill-fitting models, and I think this is a way to generate better weights. I haven’t come across this method in the literature. There are papers on continuous treatment models, but most examples tend to utilize normally distributed treatments. As always, if this is something anyone has come across before send me a link! I’d love to check it out."
  },
  {
    "objectID": "book.html",
    "href": "book.html",
    "title": "Book",
    "section": "",
    "text": "Click the image to purchase!\nFrom the publisher:\nIn a time where US deployments are uncertain, this book shows how US service members can either build the necessary support to sustain their presence or create added animosity towards the military presence.\nThe United States stands at a crossroads in international security. The backbone of its international position for the last 70 years has been the massive network of overseas military deployments. However, the US now faces pressures to limit its overseas presence and spending. In Beyond the Wire, Michael Allen, Michael Flynn, Carla Martinez Machain, and Andrew Stravers argue that the US has entered into a “Domain of Competitive Consent” where the longevity of overseas deployments relies upon the buy-in from host-state populations and what other major powers offer in security guarantees. Drawing from three years of surveys and interviews across fourteen countries, they demonstrate that a key component of building support for the US mission is the service members themselves as they interact with local community members. Highlighting both the positive contact and economic benefits that flow from military deployments and the negative interactions like crime and anti-base protests, this book shows in the most rigorous and concrete way possible how US policy on the ground shapes its ability to advance its foreign policy goals.\n\n“This contemporary research rigorously details the many positive, and potentially negative, impacts of US military overseas deployments. It is rich in analysis and insight and an absolute must read for our US national security policy makers, so they more deeply understand how to best shape future military deployments. There is no doubt that Beyond the Wire will have a profound and lasting impact on our national security and foreign policy.”\n– Richard B. Myers, General, USAF, Ret., 15th Chairman of the Joint Chiefs of Staff, and President Emeritus, Kansas State University\n\n“Beyond the Wire is the most ambitious study to date examining the politics of overseas U.S. military deployments. Drawing on an impressive fourteen country survey and in-depth interviews across three continents, the authors unpack when and how host nations give consent or resist U.S. military presence. Their findings carry deep implications for global hierarchy and the liberal international order.”\n– Andrew Yeo, SK-Korea Foundation Chair and Senior Fellow, Brookings Institution, and Professor of Politics, The Catholic University of America\n\n“Beyond the Wire offers an illuminating and innovative take on the topic of societal-military relations –in the authors’ case, on those between individual soldiers based overseas and the communities that host them. The book shows how the character of interactions between foreign military personnel and local citizens can have far reaching implications for international politics. In so doing, it moves civil-military relations research in new and exciting directions.”\n– Risa Brooks, Allis Chalmers Associate Professor of Political Science at Marquette University"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Michael E. Flynn",
    "section": "",
    "text": "Michael Flynn\n\n\nAssociate Professor of Political Science\n\n\nKansas State University\n\n\n\n\n\n\nCV\n\n\n\n\nAbout Me\nMy name is Michael Flynn. I’m currently an associate professor of political science and director of the security studies program at Kansas State University. My research focuses primarily on the political economy of states’ foreign policy behavior. My peer-reviewed research has appeared in the American Political Science Review, the British Journal of Political Science, International Studies Quarterly, and various other journals. I have also published several blog posts or op-eds in outlets like the Monkey Cage, The Conversation, and Political Violence @ a Glance. My research has also been featured in media outlets like The Economist, Al Jazeera, and Newsweek.\nI have recently finished a book project that focuses on the politics of US overseas military deployments. This is forthcoming with Oxford University Press and will be released in October of 2022 and is available for pre-order at Amazon. Click the book image below!\n  \n\n\n\n\nResearch Interests\nBroadly speaking, my research interests include the following subjects and topic areas:\n\nUS Foreign Policy\nPolitical Economy of Security\nMilitary Deployments\nStatistical Research Methods\nBayesian Modeling\nMultilevel Modeling"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "This page contains information on R software packages that I’ve developed.\n\n\n\n\n\ntroopdata\n\n\nTools for analyzing US military deployment and basing data\n\n\nCRANGitHub\n\n\n\n\n\n\ngreenbook\n\n\nTools for analyzing US foreign policy spending data\n\n\nGitHub"
  },
  {
    "objectID": "minerva.html",
    "href": "minerva.html",
    "title": "Minerva Research Initiative Project",
    "section": "",
    "text": "This page is dedicated to our Minerva Research Initiative project: The Social, Political, and Economic Consequences of the US Military’s Overseas Deployments (Grant # W911NF-18-1-0087)."
  },
  {
    "objectID": "minerva.html#research-team",
    "href": "minerva.html#research-team",
    "title": "Minerva Research Initiative Project",
    "section": "Research Team",
    "text": "Research Team\n\nMichael E. Flynn\nMichael A. Allen\nCarla Martinez Machain\nAndrew Stravers"
  },
  {
    "objectID": "minerva.html#application-documents",
    "href": "minerva.html#application-documents",
    "title": "Minerva Research Initiative Project",
    "section": "Application Documents",
    "text": "Application Documents\nYou can find a copy of our white paper and fully length proposals here.\n\nWhite Paper\nFull Proposal"
  },
  {
    "objectID": "minerva.html#data-documentation",
    "href": "minerva.html#data-documentation",
    "title": "Minerva Research Initiative Project",
    "section": "Data Documentation",
    "text": "Data Documentation\n\nCodebook for Survey\nCodebook for Protest Data\nCodebook for Military Expenditures"
  },
  {
    "objectID": "minerva.html#datasets",
    "href": "minerva.html#datasets",
    "title": "Minerva Research Initiative Project",
    "section": "Datasets",
    "text": "Datasets\n\nSurvey Data\n\nYears 1-3 Data Combined (Current)\nYear 2 Data (Outdated)\nYear 1 Data (Outdated)\n\n\n\nMilitary Spending\n\nDeployment Cost Data\nMilitary Construction Data\n\nAlso see the {troopdata} package for most up-to-date military construction spending data."
  },
  {
    "objectID": "minerva.html#research-publications",
    "href": "minerva.html#research-publications",
    "title": "Minerva Research Initiative Project",
    "section": "Research Publications",
    "text": "Research Publications\n\nAllen, Michael A., Michael E. Flynn, Carla Martinez Machain, and Andrew Stravers. 2020. Outside the wire: US military deployments and public opinion in host states. American Political Science Review. 114(2): 326-341."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Michael Flynn’s Blog",
    "section": "",
    "text": "Bayesian modeling, speed boosts, and how to run R remotely\n\n\n\n\n\n\n\n\n\nDec 18, 2022\n\n\nMichael Flynn\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking tables for multinomial models with {modelsummary} and {brms}\n\n\n\n\n\n\n\n\n\nJun 14, 2022\n\n\nMichael E. Flynn\n\n\n27 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat I learned in my first year as a graduate program director\n\n\n\n\n\n\n\n\n\nJun 6, 2022\n\n\nMichael E. Flynn\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMilitary Officers Pursuing PhDs: Are Three Years Enough?\n\n\n\n\n\n\n\n\n\nMay 24, 2022\n\n\nMichael E. Flynn\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s the Difference?\n\n\n\n\n\n\n\n\n\nOct 12, 2021\n\n\nMichael E. Flynn\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHurdle lognormal densities, take II\n\n\n\n\n\n\n\n\n\nJun 11, 2021\n\n\nMichael Flynn\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn software choice\n\n\n\n\n\n\n\n\n\nApr 22, 2021\n\n\nMichael Flynn\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHurdle lognormal distribution densities?\n\n\n\n\n\n\n\n\n\nMar 3, 2021\n\n\nMichael Flynn\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  }
]