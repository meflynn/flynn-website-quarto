[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Michael Flynn‚Äôs Blog",
    "section": "",
    "text": "Better Organizing Plots with ggplot and patchwork\n\n\n\n\n\n\n\n\nAug 7, 2024\n\n\nMichael Flynn\n\n\n17 min\n\n\n3,224 words\n\n\n\n\n\n\n\n\n\n\n\n\nUsing regular expressions (regex) to turn raw text into a data table\n\n\n\n\n\n\n\n\nJul 25, 2024\n\n\nMichael Flynn\n\n\n21 min\n\n\n4,022 words\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian modeling, speed boosts, and how to run R remotely\n\n\n\n\n\n\n\n\nDec 18, 2022\n\n\nMichael Flynn\n\n\n13 min\n\n\n2,466 words\n\n\n\n\n\n\n\n\n\n\n\n\nMaking tables for multinomial models with {modelsummary} and {brms}\n\n\n\n\n\n\n\n\nJun 14, 2022\n\n\nMichael E. Flynn\n\n\n28 min\n\n\n5,544 words\n\n\n\n\n\n\n\n\n\n\n\n\nWhat I learned in my first year as a graduate program director\n\n\n\n\n\n\n\n\nJun 6, 2022\n\n\nMichael E. Flynn\n\n\n14 min\n\n\n2,734 words\n\n\n\n\n\n\n\n\n\n\n\n\nMilitary Officers Pursuing PhDs: Are Three Years Enough?\n\n\n\n\n\n\n\n\nMay 24, 2022\n\n\nMichael E. Flynn\n\n\n17 min\n\n\n3,344 words\n\n\n\n\n\n\n\n\n\n\n\n\nWhat‚Äôs the Difference?\n\n\n\n\n\n\n\n\nOct 12, 2021\n\n\nMichael E. Flynn\n\n\n10 min\n\n\n1,985 words\n\n\n\n\n\n\n\n\n\n\n\n\nHurdle lognormal densities, take II\n\n\n\n\n\n\n\n\nJun 11, 2021\n\n\nMichael Flynn\n\n\n5 min\n\n\n998 words\n\n\n\n\n\n\n\n\n\n\n\n\nOn software choice\n\n\n\n\n\n\n\n\nApr 22, 2021\n\n\nMichael Flynn\n\n\n13 min\n\n\n2,494 words\n\n\n\n\n\n\n\n\n\n\n\n\nHurdle lognormal distribution densities?\n\n\n\n\n\n\n\n\nMar 3, 2021\n\n\nMichael Flynn\n\n\n5 min\n\n\n805 words\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "This page contains information on R software packages that I‚Äôve developed.\n\n\n\n\n\ntroopdata\n\n\nTools for analyzing US military deployment and basing data\n\n\nCRANGitHub\n\n\n\n\n\n\ngreenbook\n\n\nTools for analyzing US foreign policy spending data\n\n\nGitHub"
  },
  {
    "objectID": "book.html#beyond-the-wire-us-military-deployments-and-host-country-public-opinion",
    "href": "book.html#beyond-the-wire-us-military-deployments-and-host-country-public-opinion",
    "title": "Book",
    "section": "Beyond the Wire: US Military Deployments and Host Country Public Opinion",
    "text": "Beyond the Wire: US Military Deployments and Host Country Public Opinion\n\nOxford University Press Bridging the Gap Series\n\n  \nClick the image to purchase!\nFrom the publisher:\nIn a time where US deployments are uncertain, this book shows how US service members can either build the necessary support to sustain their presence or create added animosity towards the military presence.\nThe United States stands at a crossroads in international security. The backbone of its international position for the last 70 years has been the massive network of overseas military deployments. However, the US now faces pressures to limit its overseas presence and spending. In Beyond the Wire, Michael Allen, Michael Flynn, Carla Martinez Machain, and Andrew Stravers argue that the US has entered into a ‚ÄúDomain of Competitive Consent‚Äù where the longevity of overseas deployments relies upon the buy-in from host-state populations and what other major powers offer in security guarantees. Drawing from three years of surveys and interviews across fourteen countries, they demonstrate that a key component of building support for the US mission is the service members themselves as they interact with local community members. Highlighting both the positive contact and economic benefits that flow from military deployments and the negative interactions like crime and anti-base protests, this book shows in the most rigorous and concrete way possible how US policy on the ground shapes its ability to advance its foreign policy goals.\n\n‚ÄúThis contemporary research rigorously details the many positive, and potentially negative, impacts of US military overseas deployments. It is rich in analysis and insight and an absolute must read for our US national security policy makers, so they more deeply understand how to best shape future military deployments. There is no doubt that Beyond the Wire will have a profound and lasting impact on our national security and foreign policy.‚Äù\n‚Äì Richard B. Myers, General, USAF, Ret., 15th Chairman of the Joint Chiefs of Staff, and President Emeritus, Kansas State University\n\n‚ÄúBeyond the Wire is the most ambitious study to date examining the politics of overseas U.S. military deployments. Drawing on an impressive fourteen country survey and in-depth interviews across three continents, the authors unpack when and how host nations give consent or resist U.S. military presence. Their findings carry deep implications for global hierarchy and the liberal international order.‚Äù\n‚Äì Andrew Yeo, SK-Korea Foundation Chair and Senior Fellow, Brookings Institution, and Professor of Politics, The Catholic University of America\n\n‚ÄúBeyond the Wire offers an illuminating and innovative take on the topic of societal-military relations ‚Äìin the authors‚Äô case, on those between individual soldiers based overseas and the communities that host them. The book shows how the character of interactions between foreign military personnel and local citizens can have far reaching implications for international politics. In so doing, it moves civil-military relations research in new and exciting directions.‚Äù\n‚Äì Risa Brooks, Allis Chalmers Associate Professor of Political Science at Marquette University"
  },
  {
    "objectID": "posts/military-phds/military-phds.html",
    "href": "posts/military-phds/military-phds.html",
    "title": "Military Officers Pursuing PhDs: Are Three Years Enough?",
    "section": "",
    "text": "files/header-code/header-code.html\ntl;dr: My goal here is to catalog what ended up being a fairly wide ranging (and I think informative) series of tweets from lots of different people on timelines for military students pursuing their PhD in political science or related fields. A lot of folks brought a lot of different perspectives to this question, but Twitter isn‚Äôt always the best format for organizing material in a clear and coherent way."
  },
  {
    "objectID": "posts/military-phds/military-phds.html#background",
    "href": "posts/military-phds/military-phds.html#background",
    "title": "Military Officers Pursuing PhDs: Are Three Years Enough?",
    "section": "Background",
    "text": "Background\n\n\nI don‚Äôt know who in the military is responsible for creating these programs that are sending officers out with the expectation that they‚Äôre going to readily get a PhD in three years, but stop it.\n\n‚Äî Michael Flynn (@flynnpolsci) December 10, 2021\n\n\nAre three years enough time for a military student to complete a PhD program in political science or related fields? The original post that sparked this exchange is below, but I‚Äôll include other key followup posts, too.\nMany active duty military officers who are interested in pursuing a PhD do so through some different avenues. I‚Äôm not going to claim expert knowledge as to all the pathways, but one that often pops up is the Advanced Strategic Planning and Policy Program (ASP3). The US Army‚Äôs School of Advanced Military Studies describes the ASP3 program as follows:\n\nThe Advanced Strategic Planning and Policy Program is a multi-year program that prepares field-grade officers for service as strategic planners through a combination of practical experience, professional military education, and a doctorate from a civilian university. Once selected for the program, officers apply to doctoral programs at respected American universities in a liberal arts field of study related to strategy. They spend up to two years in graduate school satisfying all course and exam requirements leading to acceptance as a doctoral candidate. During these years, officers will also attend professional military education at the School of Advanced Military Studies (SAMS) at Fort Leavenworth, Kansas studying history, strategic theory, and the practice of strategic planning. Officers will then serve a developmental assignment in a strategic planning position. Those officers selected for battalion or brigade command will be afforded the opportunity. After the developmental assignment, the officer will spend one year working full time on the dissertation at SAMS or another suitable location and then be available for utilization as a strategic planner.\n\nAs outlined in the description, the general idea is to allocate three years to the completion of a PhD‚Äîtwo years of coursework and one year of dissertation writing. Contrast this with civilian programs in the US that often follow a path like this:\n\nCoursework. Usually 2-3 years of coursework, with a combination of general field seminars, methods seminars, and more focused subfield seminars. Specific requirements will vary by department. For example, we were required to take a set of core seminars in all subfields, a sequence of five quantitative methods courses (3 in statistics/econometrics, and two in game theory). We didn‚Äôt offer a qualitative methods course, I now wish we did, but you can imagine that extends, or makes more variable, what a given methods sequence might look like.\nExams. We had Qualifying Exams in year 1 or 2, and everyone took comprehensive exams after five semesters of coursework,\nThe prospectus. Basically your plan for your dissertation that you write and defend. Some are shorter (15-25 pages) and others can be longer (mine was ~60 pages).\nThe dissertation. Again, there are different models to follow here. Some write the ‚Äúthree paper‚Äù model, others follow more of a book model. A lot of this depends on advisory, department, program, topic, etc.\n\nThere are a number of difficulties in assessing timelines here as we‚Äôre not exactly comparing apples to apples. Here are a few points to consider that might alter baseline expectations:\n\nCivilian students are often working as teaching or research assistants (TA/RA) while taking courses and working on the dissertation.\nMilitary students often have full funding, meaning they‚Äôre not required to work as an TA/RA.\nUltimate career trajectories aren‚Äôt the same. Military students are often going to go back to the force while civilian students are likely training for a career in academia, NGOs, the private sector, government, etc.\nThe academic job market can introduce distortions into the timeline as there are a lot of external factors that affect the date of completion, and at a certain point you‚Äôre often mostly done and waiting to get a job before you officially ‚Äúcomplete‚Äù the program.\n\nI‚Äôm going to try to block off the main arguments for and against this time frame below. Let me also declare a couple of caveats before diving into more detail. First, because I‚Äôm the one writing this post and assembling the material there‚Äôs inevitably going to be a bit more content pertaining to my own viewpoint here. I came out of a political science program with more of a quantitative/statistical focus in terms of research methodology. I‚Äôm also currently the Director of Security Studies at Kansas State University‚Äîa program that‚Äôs run by both political science and history departments. We deal overwhelmingly with Army officers. Last, I can‚Äôt include all of the responses, but if you‚Äôre interested you should comb through the thread to check out what others think (especially some of the respondents who are or were active duty and dealing with this very issue)."
  },
  {
    "objectID": "posts/military-phds/military-phds.html#whats-a-phd-for-anyway",
    "href": "posts/military-phds/military-phds.html#whats-a-phd-for-anyway",
    "title": "Military Officers Pursuing PhDs: Are Three Years Enough?",
    "section": "What‚Äôs a PhD for, Anyway?",
    "text": "What‚Äôs a PhD for, Anyway?\nFirst, let me explain what I think the value of the PhD is, and why (presumably) people would want to get one. There are a couple of dimensions to this question. First, a PhD is a research degree‚Äîit‚Äôs a degree focused on evaluating existing knowledge and producing new knowledge.\nDo you need one to do research? No, but getting one offers you an opportunity to set aside a prolonged period of time to train under established experts and to learn as a part of a broader cohort/community. In the social sciences the emphasis is often on learning to synthesize theoretical and empirical insights from various sources, and using these insights to develop your own research program. For military officers, specifically, it offers them an opportunity to acquire skills that are perhaps not readily available through their regular training and educational processes.\nSecond, for officers who are pursuing a PhD at a civilian university it offers them a chance to get out of the bubble of the military. Taking time to embed oneself in a civilian institution, to learn from civilian experts, and to work alongside civilians in their broader cohort. More succinctly, there‚Äôs a socialization aspect here that may not be as readily available in venues that are predominately military students.\nOverall, it‚Äôs about having the time to really immerse yourself in material and a community that is focused primarily on developing research skills and building knowledge. For people pursuing an academic path these skills and experiences will be beneficial for obvious reasons. And military students who may soon retire and/or transition into the private sector or government will find these skills useful for lots of the same reasons.\nBut what about military students who will likely remain in the military? Lots of the skills students develop while pursuing graduate education at civilian institutions will continue to be of use for students once they return to their day jobs in the military. Often the questions that motivate these students during their PhD studies arise from their time in uniform. Ideally they will return to their jobs better equipped to tackle some of these problems.\nI once had a student who had been deployed to Afghanistan and was tasked with assessing the efficacy of certain measures on insurgent activities. This experience worked out great for a measurement paper assignment in one of his seminars. Another student coming from a logistics background did some great work on the relationship between transportation infrastructure and the efficacy of UN peacekeeping operations.\nUltimately these students seem to have been able to effectively use their time to improve develop skills to allow them to better perform the tasks they were already being asked to do in the military.\nThe question then is what amount of time is ‚Äúenough‚Äù to distinguish the PhD from other opportunities, like an MA degree? Students will cover a lot of the same ground in a one or two-year MA program as compared to a three-year PhD program, so what is the value added that makes an accelerated PhD program ‚Äúworth it‚Äù?"
  },
  {
    "objectID": "posts/military-phds/military-phds.html#is-three-years-enough-time-to-earn-a-phd",
    "href": "posts/military-phds/military-phds.html#is-three-years-enough-time-to-earn-a-phd",
    "title": "Military Officers Pursuing PhDs: Are Three Years Enough?",
    "section": "Is Three Years Enough Time to Earn a PhD?",
    "text": "Is Three Years Enough Time to Earn a PhD?\nSo can it be done in three years? I‚Äôve given this some thought in the intervening months and the short answer is ‚ÄúYes, it can be done, but it‚Äôs not ideal‚Äù.\nTo be clear, I don‚Äôt mean ‚Äúideal‚Äù from an academic-training-other-academics perspective. I mean ideal from the perspective of maximizing student success, ensuring sufficient value added from the PhD program, and return on investment from a taxpayer dollar perspective. I‚Äôll return to address these issues below.\nUltimately I think this path can work really well for some people, but I worry that we‚Äôre leaving a lot on the table, and even a fourth year would go a long ways towards optimizing outcomes.\nI‚Äôll come back to these concerns below. For now, I‚Äôll divide this section into two sections to capture the general arguments in support of, or in opposition to, this time frame. Let‚Äôs tackle some of the supportive arguments first.\n\nYes, it‚Äôs Enough\nFirst, let‚Äôs go over some of the arguments in favor. Several respondents to the thread think three years as enough, or nearly enough. One particularly important feature that some folks noted is that military officers who choose to pursue these programs are often highly motivated and further along in their careers.\nPhilip Hultquist, a professor at the School of Advanced Military Studies at Ft. Leavenworth, discusses some of his experiences with military students pursuing PhDs in this sub-thread.\n\n\nMichael, I shared your skepticism when I first learned of it too, but since working @us_sams I‚Äôve seen it done. I‚Äôm on a diss committee and have given advice to several students. Though I teach in a different program I can provide some context. A quick üßµ\n\n‚Äî Philip Hultquist (@HultquistPhilip) December 10, 2021\n\n\nPhilip notes that for military officers pursuing a PhD, the PhD is their full time job. They don‚Äôt need to worry about funding, grants, RA/TA responsibilities, etc. The military‚Äôs financial backing and ability to focus strictly on the degree program is a huge benefit with respect to finishing an accelerated timeline.\nImportantly, in addition to selecting for high achievers, these students typically have an MA degree in hand already when they enter a PhD program. Our own Security Studies program allows students to transfer up to 30 hours of coursework from a previous MA degree towards PhD coursework. So these students are 1) already familiar with graduate-level work, 2) probably have some prior exposure to the relevant literature, and 3) possibly already have some sort of methods training.\nThis prior experience can be a determining factor. Sheena Greitens, a professor at the LBJ School at UT-Austin, makes a good point about the fact that faculty at civilian universities should not automatically discount the prior educational and research experience military officers may have.\nThis impulse seems to reflect a difference between US and European programs, where the former typically require PhD students with an MA to repeat coursework and the latter will accept MA experience towards the doctorate. Elsewhere in this conversation she also makes the point that dealing with these timelines may require programs and/or advisers to make adjustments to their approach and expectations.\n\n\nLBJ requires a masters first. Similarly, when I did the Oxford MPhil, DPhil was a 3-yr option after that. Doesn‚Äôt work for every project (didn‚Äôt for mine, so I came back to US!). But the idea that everyone needs 5+ yrs regardless of prep beforehand strikes me as too rigid.\n\n‚Äî Sheena Chestnut Greitens (@SheenaGreitens) December 11, 2021\n\n\nFurthermore, people in the military may have pressing considerations beyond the degree itself. Nick Frazier offers some thoughts on career considerations that make longer program duration less feasible/desirable for the Army and military officers. In particular, he highlights the lengthy time period that officers ‚Äúowe‚Äù to the military subsequent to finishing their PhD, and also difficulties assessing an officer‚Äôs time in graduate school when it comes to promotions.\n\n\nI don‚Äôt know who in the military is responsible for creating these programs that are sending officers out with the expectation that they‚Äôre going to readily get a PhD in three years, but stop it.\n\n‚Äî Michael Flynn (@flynnpolsci) December 10, 2021\n\n\nI don‚Äôt take this to mean that PhD timelines should be shortened to provide special accommodations to the US military. Rather, I think this is akin to the mantra ‚Äúa good dissertation is a finished dissertation.‚Äù If military officers can finish in three years, there are very good reasons to do so.\nIn short, military students who are pursuing PhDs are often selected because they have the relevant prior experience, are high achievers, have financial backing that alleviates pressures others face, and have strong personal and professional incentives to succeed. Furthermore, the expectation that longer timelines are necessary is partly a function of more rigid programs and faculty advisers.\n\n\nNo, It‚Äôs Not Enough\nThe section title is a slight misrepresentation of my views on this since I think it‚Äôs possible and fine for some folks. More accurately, I think a four year PhD track would be desirable for lots of reasons. As I said above, I think there are lots of benefits to military officers pursuing PhDs, and I think we need to make sure that there is sufficient value added to ensure the degree is meaningfully different from another MA degree.\nAt the most basic level three years is, well, only three years. That‚Äôs just not a lot of time for things to happen, no matter how driven a student may be.\nFirst, two years of coursework is probably fine. That‚Äôs a little bit less than what a lot of PhD programs require, it‚Äôs probably OK if we‚Äôre generally selecting students who have already received an MA degree in the chosen field prior to enrolling in a PhD. In such cases we‚Äôre totaling around 4 years of coursework, which is more than enough.\nMy concerns really kick in with the prospectus and dissertation-writing process (but a little before, too). One year is not a lot of time to write and defend a prospectus, and then write and defend a dissertation. Can it be done? Sure, but it assumes a lot of things go right. And I think this is where the pressure really falls on admissions committees and faculty to try to screen for those candidates who are most likely to succeed under the given framework.\nBut here‚Äôs the thing I worry about‚Äîthis doesn‚Äôt allow a lot of room for failure. And I don‚Äôt mean completely failing the program and being forced to leave. Research and the production of knowledge are inherently messy processes, and not everything goes right on the first go around. Lots of smart people fail at getting a PhD because it‚Äôs not just about reading and retaining knowledge, but about synthesizing insights from existing work and using those to create something new. Coding rules require revising, inclusion criteria may need a tweak, whatever. Anyone who has collected original data as a part of their dissertation (or any project) can probably attest to this fact. And this is coming from a guy who had to write two prospectuses because my first one was hot garbage.\nAgain, can these hurdles be overcome? Yes, but I worry that for those who don‚Äôt fit the neat and clean model, or those who have some unexpected hurdles arise, will be pushed over their time limits or never selected to begin with. Maybe they‚Äôll still finish, but I‚Äôm willing to bet there‚Äôs much more uncertainty once they‚Äôre back in their regular jobs.\nSome of this is echoed in tweets by Will Winecoff, an associate professor at the University of Indiana, and a followup tweet by Dan Drezner, a professor at Tufts University. Both note that while the students they‚Äôve worked with are strong, many (most?) don‚Äôt finish within three years.\n\n\nIn my experience it doesn‚Äôt get done in 3 years, but it does get done. These folks are highly-motivated and have long track records of diligence. I have a lot of respect for the ones who enter these programs.\n\n‚Äî W. K. Winecoff (@whinecough) December 10, 2021\n\n\nI also worry that this limits the variability in what, and how, people research during the PhD process. Relying on existing sources, whether it‚Äôs quantitative or qualitative data, or more historical work or case studies, is suitable for lots of dissertations. But there are also lots of civilian dissertations that require field work, interviews, and other research tools that require more time and likely draw out the pre-writing process. Even lots of more quantitative dissertations may require more substantial methods training.\nClose advising, using seminar papers as chapters, and a three-paper dissertation model may be great for getting people across the finish line in the allotted time, but it seems to me that they necessarily preclude certain educational/research pathways and impose a substantial degree of homogeneity on the type of research military officers are able to conduct. I worry we‚Äôre leaving a lot of potentially good work on the table as a result.\nThis timeline also imposes a barrier for military students who seek to deepen methodological expertise. For civilian students who are going into academia they also have plenty of time after getting the PhD to spend more time learning/developing methods, improving language skills, spending time in their chosen country or region, etc. Military officers may not have this flexibility, and so maximizing what they can learn while still pursuing the PhD is perhaps more important for them.\nI think some folks will call back to the previous section and note that the time spent working on the MA also also contributes to these goals. Maybe. Lots of students at the MA level don‚Äôt necessarily know that they‚Äôll be going on to pursue a PhD. Professional and research interests also change over time. As I note above, this presumes lots of things go right. So it‚Äôs not clear to me that the cumulative time is equivalent to allowing more time to pursue the PhD.\nThe point here is not to simply be training military officers as academic researchers. The point is that many of their projects are motivated by the substantive concerns of their jobs, and we should want to train them as effectively as possible so the PhD brings real value added when they return to those jobs. Learning about research design, measurement, interview techniques, etc., are all skills that map directly onto what a lot of our students have done on previous deployments.\nLast, beyond having additional time to develop their own skills, the time where military students are interfacing with civilian faculty and students is, itself, highly valuable. You learn a lot about intellectual collaboration outside of seminars while working alongside your colleagues. Also, I think there‚Äôs tremendous normative value in these programs because they break down the barriers between the civilian and military worlds."
  },
  {
    "objectID": "posts/military-phds/military-phds.html#takeaways",
    "href": "posts/military-phds/military-phds.html#takeaways",
    "title": "Military Officers Pursuing PhDs: Are Three Years Enough?",
    "section": "Takeaways",
    "text": "Takeaways\nSo who‚Äôs right? ¬Ø\\_(„ÉÑ)_/¬Ø\nI think there are lots of reasons why a three year track can make sense for some people, and they can indeed be very successful. But I think even an additional year would bring tremendous benefits to everyone involved for lots of reasons.\nBut saying a four year track is better should not be taken as a call to scrap these relationships. Ultimately I think having military students in our programs and giving them an opportunity work alongside civilians is much better than not having them around at all. We get lots of bright students coming through our program, and lots of other folks have shared similar assessments/experiences. The challenge under the current system, then, is to figure out how faculty and programs can work with military students to maximize their chances of success, and what they take away, while we have them."
  },
  {
    "objectID": "posts/whats-the-difference/whats-the-difference.html",
    "href": "posts/whats-the-difference/whats-the-difference.html",
    "title": "What‚Äôs the Difference?",
    "section": "",
    "text": "files/header-code/header-code.html\nIn the social sciences we talk a lot about groups being different from one another. We might also talk about things like spending on certain programs going up or down over time. While this can seem direct, comparing quantities of interest can often be made complicated by a variety of factors. Below I‚Äôll provide an overview of some of the issues I want you to be aware of as we begin to read journal articles and other readings that will invariably discuss inter-group and inter-temporal differences."
  },
  {
    "objectID": "posts/whats-the-difference/whats-the-difference.html#comparing-two-groups",
    "href": "posts/whats-the-difference/whats-the-difference.html#comparing-two-groups",
    "title": "What‚Äôs the Difference?",
    "section": "Comparing two groups",
    "text": "Comparing two groups\nComparing group behaviors can be complicated because in addition to differences between the groups, there is also variation in how the individual members of those groups tend to behave. Let‚Äôs imagine that we want to compare bipartisanship in US foreign policy. There are different metrics we could use, but we‚Äôll settle for Congressional voting patterns. We might want to know if Democrats are more or less bipartisan than Republicans. We could imagine that we‚Äôre going to sample votes taken by members of Congress where each member of Congress votes on a series of votes during each year‚Äîlet‚Äôs say 100 votes‚Äîand we could code those votes as bipartisan (1) or not (0). Rather than collecting data on every vote we decide to just sample from one year. At the end we might end up with a distribution of bipartisan votes that looks like this:\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe figure shows the distribution of the percentage of the 100 votes each legislator makes that are bipartisan‚ÄîA value of 60 means that 60 of the 100 votes, or 60% of the 100 votes, a given legislator makes are bipartisan and 40% are not. We‚Äôll focus on the raw count of votes rather than percent for now, but the 100 vote total simplifies this to make these voting totals more intuitive for present purposes. Dealing with proportional measures can get more complicated, but we‚Äôll set some of that aside for now."
  },
  {
    "objectID": "posts/whats-the-difference/whats-the-difference.html#statistical-methods-for-determining-differences-between-groups",
    "href": "posts/whats-the-difference/whats-the-difference.html#statistical-methods-for-determining-differences-between-groups",
    "title": "What‚Äôs the Difference?",
    "section": "Statistical methods for determining differences between groups",
    "text": "Statistical methods for determining differences between groups\nSo are Republicans and Democrats different? Are they meaningfully different? How can we tell? The idea here is that when we‚Äôre dealing with data of the kind shown in the above example, we want to understand if the differences between two groups are 1) systematic or are they the result of chance, and 2) how big or substantively meaningful are the differences we observe? Making these sorts of judgements by looking at this figure alone is difficult.\nFor example, a simple comparison of the mean values of the two parties suggests that Democrats have a higher rate of bipartisanship than do Republicans. But the data shown above are also very noisy, they represent just a sample from a broader set of votes, and there‚Äôs quite a bit of overlap between members of each group, so we can‚Äôt be sure from casual observation that the differences we think we observe are accurate representations of broader differences between the two groups. This is where we turn to some basic statistical methods that can help us to estimate if and how two or more groups might be different from one another with respect to some outcome of interest.\n\nDifference of Means Tests\nThe sorts of statistical methods used by researchers vary quite a bit depending on the question at hand, but for this basic example we can start with something really simple called a difference of means test, or a t test. This test allows us to look at the values of two continuous variables and compare their mean values. Another way to think of this is to compare the values of a given variable across two groups. In our example we‚Äôre interested in comparing the mean number of bipartisan votes for Republican and Democratic legislators. So political party represents our grouping variable and the outcome of interest is the count of the number of bipartisan votes.\n\n\n\n\nDifference of means test of bipartisan voting\n\n\nParameter1\nParameter2\nMean_1\nMean_2\nDiff\nCI\nCI_low\nCI_high\nt\ndf_error\np\nMethod\nAlternative\n\n\n\n\nRepublican\nDemocrat\n44.88\n50.51\n-5.63\n0.95\n-6.6\n-4.67\n-11.47\n406.57\n0\nWelch Two Sample t-test\ntwo.sided\n\n\n\n\n\n\n\nThe output in @ref(tab:t-test-example) resembles the output for a regression model that we‚Äôll discuss below, but is much simpler. It tells us the names of the two groups, the mean score for each of the two groups, the difference between those mean scores, and the confidence interval, confidence interval limit values, and the test statistic or t value. There is also more information contained in the table about the type of t test and the direction of the test. Sometimes you might want to look at whether or not a group has a score that is higher or lower than another group, meaning that you have some specific direction in mind. In our case we‚Äôre content to just look at a non-directional test to see if the means of the two groups are different without hypothesizing the direction of that difference in advance. For now our basic hypothesis is that the groups are different, and our null hypothesis is that there is no difference between the two groups.\nSo is there a difference in the bipartisan voting behavior of the two parties? The t test shows that the Republican score is indeed lower than the Democratic score, and that this difference is statistically significant at the 0.95 level. The Democratic score is 5.63 higher than the Republican score as can be seen in the ‚ÄúDiff‚Äù column, and the test yields a test statistic of -11.17. This translates to a very low p value, which indicates the probability of seeing a test statistic greater than or equal to the observed test statistic of the null hypothesis were true. This is a lot, I know, but the important point is that researchers generally want low p values.\nSo this result is statistically significant, but is it substantively significant? That‚Äôs up to the researcher and requires some knowledge of the specific topic and/or broader domain expertise. In this example we see that, on average, Republican legislators case a bipartisan vote about 45% of the time and Democratic legislators case a bipartisan vote about 50% of the time. This this case we‚Äôre looking at a difference of approximately 5 percentage points. Again, whether or not this is substantively meaningful requires a lot of additional context, and my purpose here is less to tell you the answer to that specific question for this hypothetical and more to help you to learn to be thoughtful and cautious when consuming statistical analyses.\n\n\nRegression Models\nThe t test represents a very simple approach to analyzing the differences between two groups. While this can be useful, sometimes our data and the relationships between variables of interest are too complicated for a simple t test. In some cases we might want to know how groups compare when taking into account a number of other factors. Regression models are a broad class of statistical tools that allow us to model more complex relationships among various predictor and outcome variables. A fuller treatment of regression models is more than I want to get into for the purposes of this class, but there are a couple of points I want to highlight.\n\n\n\nLinear Regression Predicting Bipartisan Votes\n\n\n\nModel 1\n\n\n\n\nRepublicans\n‚àí5.634***\n\n\n\n(0.487)\n\n\nIntercept\n50.509***\n\n\n\n(0.331)\n\n\nNum.Obs.\n434\n\n\nR2\n0.236\n\n\nR2 Adj.\n0.234\n\n\nAIC\n2643.2\n\n\nBIC\n2655.4\n\n\nLog.Lik.\n‚àí1318.600\n\n\nF\n133.598\n\n\nRMSE\n5.05\n\n\n\n + p\n\n\n\n\n\n\n\n\n\nFirst, Table @ref(tab:linear-regression-example) shows the results of a regression model where we model bipartisan votes as a function of party identification. There‚Äôs a lot here but we‚Äôre only going to focus on a couple of points.\nFirst, the higlighted row with the red text is really what we‚Äôre interested in when comparing the two parties as we discuss above. The left column shows us the name of the variable‚Äîin this case it‚Äôs the group we‚Äôre interested in, which is the political party of the observed legislator. The right column shows us the correlation coefficient. Basically this tells us how big the difference is between the Democratic legislators (the reference group here) and the Republican legislators. Note that the number listed here, -5.634, is the same as the number listed in the t test above!\nThe asterisks next to the coefficient represent the level of statistical significance associated with that coefficient. This is often used as a marker of importance, but for reasons we‚Äôve discussed briefly above, this isn‚Äôt always a great basis on which to judge the substantive significance of a result.\nWhere do these stars come from? Well, the number in the parentheses below the coefficient is the standard error of the coefficient. Basically, you divide the coefficient value by the standard error (i.e.¬†\\(-5.64/0.48\\)) and (according to some arbitrarily defined conventions) a coefficient earns another asterisk every time that resulting ratio crosses a particular threshold. Don‚Äôt worry about that too much right now.\nThe Intercept represents the expected value of the outcome when the other predictor variables are set to 0. In this case the intercept represented the mean value of the bipartisan voting indicator (our outcome) when the Republican variable is set to 0. In other words, this is the mean score for the Democrats! See above in the t test table to check it out (there‚Äôs some minor rounding)!\nThere are other details here that can be useful, like the ‚ÄúN‚Äù or the number of observations. This tells you how many times we‚Äôre observing the phenomenon of interest. We often want more information, but a larger N doesn‚Äôt solve all our problems if there are other serious flaws with our measurement strategy or research design.\nAt the bottom of the table you can often find a listing of the significance thresholds. Each symbol corresponds to a particular p value. Remember that we derive these p values from the coefficient and the standard error. You can have very very small p values for coefficients with very very small substantive effects, so this is not equivalent to the importance of a coefficient. I can‚Äôt say that enough!"
  },
  {
    "objectID": "posts/software-choice/software-choice.html",
    "href": "posts/software-choice/software-choice.html",
    "title": "On software choice",
    "section": "",
    "text": "files/header-code/header-code.html\ntldr; If Stata or SPSS works well for what you do, that‚Äôs great‚ÄîYou should keep using the tool that makes you better at your job! However, as university faculty we‚Äôre not just making choices for ourselves, but for the students we teach. And those choices have a lot of knock-on effects. In general I think we should tend to err on the side of lowering barriers to entry for students and providing them with the most flexible tools for the job they want to have, but also the jobs they might have. While R has a lot of problems and quirks, I think it‚Äôs the best relative option considering these various factors.\nThis post is all about the glories of R. Not really. But maybe kind of? This recent tweet gained some traction and generated a bit of discussion on the subject of proprietary software vs open source alternatives like R:\n\n\nToo many young students are having their time and money wasted by being forced to use out of date tools like SPSS because their professors are scared of the new stuff. There. I said it. #rstats #python #datascience.\n\n‚Äî Keith McNulty (@dr_keithmcnulty) April 16, 2021\n\n\nI‚Äôm not going to delve much into the motivations for various faculty avoiding R (or even other alternatives like Python). I don‚Äôt necessarily think it‚Äôs fear driving people, but more likely they don‚Äôt see the utility or think the return on investment is low. But there are two bits that caught my attention that speak to these issues. First, the idea of wasting money. Second, the idea that tools are out of date.\nThis post is really primarily about my thoughts on switching from someone who uses R to an R user. I think this is a distinction with a meaningful difference, and it definitely pertains to some of these issues. I went from someone who was trained on and primarily used Stata for all of my work-related modeling (as well as other stuff, like tracking my cat‚Äôs weight when he was sick) to someone who uses R for just about everything, from modeling, to writing letters, to building my website. Suffice it to say, I‚Äôm currently a big fan.\nWhat this post is not intended to be is a screed about why R is objectively the best programming language/platform for social scientists. That said, I do think it‚Äôs relatively the best. I learned R about a decade ago while I was still using Stata for most of my work, but remained a very casual user and it took me another several years to make the switch. Part of this was because it felt like the learning curve was pretty long and arduous. Even now, there are a lot of things about R that are a massive pain in the ass. For example, the tedious process of updating a particular package when you have five projects up and running, or a package with a given function name masking the identically-named function you really want to call. I‚Äôm looking at you {plyr}. Even still, I lose a lot of time dealing with some of these issues, so I completely understand and even share a lot of the frustrations that people have with R. Hence the disclaimer about it not being objectively best. But, relatively I think it has a lot of qualities that are desirable, and even better than, many of the other major software packages used by social scientists. The rest of this post is intended to highlight some of these benefits.\n\nCost\nFirst, the most basic issue. Stata was always fairly expensive, but in the last year or two they announced that they were switching to an annual subscription model as opposed to the one-time charge for a perpetual license for whatever version you happened to purchase. Even when I started buying Stata as a graduate student in 2007, it was pretty expensive considering that I was making all of 17,000 per year before taxes. For students who are currently looking to purchase Stata Stata SE is running 179 per year for student pricing. The multicore variant is upwards of 375 per year. That‚Äôs a lot.\nThe upside of R here is clear‚Äîit‚Äôs free. There can be costs associated with using R and some of the packages you can run across. Querying Google Maps, for example, can incur costs if you run over a certain threshold. But if your goal is primarily to read in data, clean it, and run a linear regression you‚Äôre on solid ground.\nResources for students in many PhD programs have been getting slim for a while now. Particularly at state schools. And while faculty often have limited control over the baseline rate of pay for graduate students at these institutions, one thing we can control is the cost associated with things like books, software, etc. Is $180 going to break the bank for a graduate student? Maybe not, but it‚Äôs also lumped in with a lot of other costs that they incurring while in graduate school.\nIt‚Äôs also not unreasonable to expect that graduate students will need more powerful tools to do the kind of work they‚Äôre pursuing. Maybe work that they, or their advisers, are conducting requires more powerful software capabilities. With Stata MP running from about 300-400 per year, that‚Äôs a pretty huge cost. While faculty may have funds to pay for this kind of thing, graduate students might not. This creates a kind of capabilities gap between graduate students and faculty in a way that‚Äôs nowhere near as evident with alternatives like R. Maybe some departments have licenses to run better versions on multiple lab computers, but this is also a massive expense that many departments may not be in a position to keep up either.\n\n\nOut of date software?\nThis one‚Äôs a little trickier. I think it‚Äôs kind of true. Running a linear regression is going to be the same, regardless of what you use. If that‚Äôs all you‚Äôre doing then it doesn‚Äôt really matter, and it‚Äôs hard to be out of date on this point. But I remember a major motivation for learning R was that Stata‚Äôs graphical capabilities were extremely poor compared with the things I was seeing coming out of R. They‚Äôve gotten better for sure. I remember being super excited in 2014 or 2015 when Stata announced that they were finally implementing transparency capabilities. I‚Äôve always enjoyed the data visualization part of the job, so Stata‚Äôs lag in implementing capabilities that were long-since standard in R was frustrating. But that‚Äôs the thing, while running a regression is the same whether you do it in R, Stata, or SPSS, the ability to effectively communicate that information is not divorced from running the models themselves. The ability to effectively communicate information is a huge part of our jobs, so the inferior quality of data visualization capabilities in some software packages is a big problem.\nThe problem with capability lags isn‚Äôt limited to data visualization. We see it across the board in modeling capabilities as well. There‚Äôs a pretty strong assumption on the part of faculty who get by just fine with a given proprietary software platform that their students will also get by just fine with those capabilities. But this just doesn‚Äôt seem realistic. Stata finally implemented Bayesian modeling capabilities with version 15, but these capabilities had been present on other software platforms for a long time. And while Stata did finally enable users to implement Bayesian modeling, these capabilities were seriously limited compared to other packages available through R (e.g.¬†not running multiple chains).\nThe point here isn‚Äôt that everyone needs to get into Bayesian modeling‚Äîthat‚Äôs just an example‚Äîbut that if students want to learn about state of the art methods, proprietary software packages often aren‚Äôt going to be the place for it. And state of the art here shouldn‚Äôt be taken as synonymous with ‚Äúhighly advanced‚Äù or ‚Äúfuturistic‚Äù. But if and when they‚Äôre interests take them into methodological territory that doesn‚Äôt overlap perfectly with yours as an adviser the standard of what‚Äôs expected may diverge considerably from what‚Äôs capable with the tools you assume are fine. When I learned social network analysis I was taught on Pajek. But looking around at what was being published in Political Science and Sociology, it seemed like everyone was using R, not Pajek. While I learned a lot of substantive knowledge, the implementational/computational skills I learned were useless and I had to go back and relearn a lot of material. Not just because of tastes, but because the software just wasn‚Äôt capable of estimating the sorts of models that were relevant to my interests. More to the point, the field in practice seemed to have advanced beyond that that particular package was even capable of.\nHaving to learn a new software package to implement what I wanted wasn‚Äôt the end of the world, but it did create a lot of costs in terms of time and effort to re-learn basic implementation issues that could have been avoided had the person who taught the course been more in line with the state of the art, so to speak. But I get it‚ÄîPajek probably worked fine for them for what they were doing. The problem is that wasn‚Äôt effective or useful for what was expected of the students they were teaching. The advantage of R in this context is that if you only need to use linear regression for your work, great. But in teaching that material to your students you‚Äôre also teaching them the basic language and ecosystem they‚Äôll need to simply install another package that can handle material that is more relevant to their interests. That is, when you‚Äôre teaching someone in R they‚Äôll use that environment and language for their basic stats and regression skills, but if their interests take them in directions that diverge from their instructor‚Äôs they don‚Äôt have to reinvent the wheel by learning a new platform and language. Even if you only ever use frequentist models, it‚Äôs a short jump for a student whose research interests take into into multilevel Bayesian modeling with packages like {brms}. I‚Äôd even venture to guess that it will be easier for an adviser to keep up with what the student is doing if they‚Äôre both working from a shared language as the synatx is largely the same.\n\n\nExtended benefits\nAnother advantage of R seems to be that learning it seems to have much greater utility for students who are seeking non-academic jobs. Don‚Äôt get me wrong, there are some private firms that definitely use other proprietary software packages, like Stata or SPSS. In our work we‚Äôve dealt with survey firms who send us our data in SPSS file formats. I think that‚Äôs bad practice, but the point is the probability of being able to use some of these software packages outside of academia is not 0. That said, use of R and other languages like Python seem to be much more widespread in industry. Again, proceeding from the premise of ‚ÄúI can afford this software and it‚Äôs fine for my work‚Äù seems faulty when, as instructors and advisers, our role extends beyond what works for us. Not all students are going to end up in academia. Even many of those on the PhD track who aspire to get tenure track jobs will be forced out of the academic job market and into private sector jobs. Not all of them will choose to go into analytics or data science jobs (substantive skills are enormously beneficial, too), but a lot of them will. Combining substantive expertise with methodological skills that are more readily transferable across the academic/industry divide seems like another way to better prepare students for whatever is to come.\nI‚Äôd also venture to guess that R makes interdisciplinary collaboration easier. Obviously this isn‚Äôt ironclad, but my experience has generally been that different disciplines seem to cluster around particular software packages. While not insurmountable, this certainly raises the cost of conducting collaborative work with people outside of your discipline. Not just learning a new language, but figuring out how to unify workflows based around totally separate software languages and the processes those incentivize can be tricky.\n\n\nCulture\nThis last one is admittedly more idiosyncratic, but I just find the culture of the broader R community to be far more welcoming and helpful than Stata. To some extent this isn‚Äôt surprising. The people contributing to open source software development genuinely like working on what they‚Äôre doing. They have to, otherwise they wouldn‚Äôt be doing it. It‚Äôs not hard to find enthusiastic R users, but I‚Äôm not sure the same can be said for alternative platforms. This makes an enormous difference insofar as there is a massive reserve of people who are willing and happy to help when you run into problems.\nBut you don‚Äôt have to be a superfan of the software you‚Äôre using‚Äîit‚Äôs a tool, a means to an end. But there are other cultural bits that I do think make a difference. I guess I‚Äôd label this broadly as ‚Äúworkflow‚Äù culture. Again, this sort of thing is present with other software packages. Scott Long‚Äôs Workflow of Data Analysis Using Stata was a great resource for managing your workflow and projects. That said, I think the broader culture that has grown up around and along with R is one that is much more cognizant of tying the initially disparate elements of your projects together into a single, unified, workflow. You can see this with the marriage of statistical modeling and writing in RMarkdown, or in the development of different packages used to communicate information through web-based platforms, like {shiny}, {bookdown}, or {pkgdown}, or interfacing easily with GitHub.\nObviously a lot of this is still up to the individual user. Rather, using R by itself doesn‚Äôt automatically make you a better social scientist. It doesn‚Äôt automatically make your work more replicable. People can still be bad at their job regardless of what platform they use. What it does do is front-loads a lot of the capabilities and ancillary packages that make these things more visible and accessible. It‚Äôs more likely to make you aware that these tools even exist, whereas they tend to be more obscured on platforms like Stata.\n\n\nTakeaway?\nTo reiterate, if Stata or SPSS works well enough for what you do, that‚Äôs great‚ÄîYou should keep using the tool that makes you better at your job! However, as university faculty we‚Äôre not just making choices for ourselves, but for the students we teach. And those choices have a lot of knock-on effects. In general I think we should tend to err on the side of lowering barriers to entry for students and providing them with the most flexible tools for the job they want to have, but also the jobs they might have. While R has a lot of problems and quirks, I think it‚Äôs the best relative option considering these various factors."
  },
  {
    "objectID": "posts/grad-program-director/grad-program-director.html",
    "href": "posts/grad-program-director/grad-program-director.html",
    "title": "What I learned in my first year as a graduate program director",
    "section": "",
    "text": "files/header-code/header-code.html\ntl;dr: This post provides a brief overview of some of my biggest takeaways after my first year directing the Security Studies program at Kansas State University. Forging positive working relationships with other rank and file administrators is key, as is institutionalizing lots of the procedures that may have formerly been relegated to post-it notes on a computer monitor."
  },
  {
    "objectID": "posts/grad-program-director/grad-program-director.html#rank-and-file-administration",
    "href": "posts/grad-program-director/grad-program-director.html#rank-and-file-administration",
    "title": "What I learned in my first year as a graduate program director",
    "section": "Rank and file administration",
    "text": "Rank and file administration\nRelationships with other rank and file administrators are key. As much as academics complain about the administration side of universities, it‚Äôs important to recognize that there‚Äôs a real and meaningful distinction between the kind of upper-level administrative bloat that we often think of when we use ‚Äúadministration‚Äù as a pejorative term, and the rank and file administrators who help make the university run.\nProgram directors come and go, but these folks have often been in their jobs for a long time and know the relevant processes and options better than I ever will. In my experience they are also typically very willing to help accommodate student needs and can help you as a new program director to navigate the quirks of any given case.\nIn our case, we also have a representative agent from the Graduate School who works out at Ft. Leavenworth. She‚Äôs fantastic, and is often our first point of contact with students who might be interested in our programs. But for her to do her job well she needs the active involvement of program staff. For example, it‚Äôs not always immediately obvious what we cover in our courses. A representative trying to represent multiple graduate programs will probably need to sit down and discuss the substance of those programs with faculty to better understand what‚Äôs on offer. That way she‚Äôs better positioned to sell those programs and provide better guidance to interested students to steer them towards programs that would be a better fit.\nThis specific position is probably unique to our program, but the basic idea is that this kind of communication often falls on program directors."
  },
  {
    "objectID": "posts/grad-program-director/grad-program-director.html#upper-level-administration",
    "href": "posts/grad-program-director/grad-program-director.html#upper-level-administration",
    "title": "What I learned in my first year as a graduate program director",
    "section": "Upper-level administration",
    "text": "Upper-level administration\nThis part is far more fragmented and case-specific. There are lots of senior-level positions that are going to affect you in your time as a program director. These different offices often seem to operated more or less independently of one another and this can frequently cause some headaches.\nI came into this position just as we got a new dean of the graduate school/vice provost for graduate education. She‚Äôs been great and seems to be genuinely interested in helping departments and programs to maintain, and in many cases rebuild, relationships with Ft. Leavenworth. In our case this kind of senior level support has been essential.\nIt‚Äôs also been extremely important to maintain communication to make sure various administrators are getting reliable information on our program. It became clear over the course of this year that the statistics produced by university institutional research was often poorly suited for questions administrators were asking about our program. So making sure your department is keeping reliable enrollment and graduation data can be a good idea.\nBut even as one senior administrator might work to support your program, the policies of other offices might work against you. This can be intensely frustrating and create a tremendous amount of uncertainty. In our case, the Security Studies program no longer has its own independent operating funds (it used to). Our former dean didn‚Äôt seem to have a strategy guiding our college except to collect cuts through attrition. If there was a deeper plan, it was never communicated with us. But this meant that the departments of history and political science lost something like 14 faculty over the last 6-7 years, and many of these people taught courses that were relevant to the program. We‚Äôve also lost the majority of our funding lines to support graduate teaching/research assistants. These cuts come through our home departments and directly impact the program‚Äôs ability to recruit and retain students.\nAs this was happening, some individuals in the dean‚Äôs office and in the university‚Äôs research office have remained strong supporters of the program. They‚Äôve tried to help our home departments and faculty find sources of funding to help sustain operations.\nUltimately the point there is to never assume that the left hand knows what the right is doing. Different administrators have very different priorities, and your ability to sustain your program is not something that registers for everyone equally. Certain decisions or policies might be bad for your program, for your department, but good for someone else, and there‚Äôs no guarantee that those decisions or policies will ever be explained to you.\nLots of this might be driven by structural factors beyond any one person‚Äôs control. Kansas has been coping with cuts to higher ed long before Covid hit. Maybe some strategies or policies would be better/worse at blunting the impacts of these big events, but it might not be any one person‚Äôs ‚Äúfault‚Äù if resources are scarce. This all depends on where you are. As frustrated as I get I also definitely would not want to be a dean in this environment."
  },
  {
    "objectID": "posts/remote-computing/remote-computing.html",
    "href": "posts/remote-computing/remote-computing.html",
    "title": "Bayesian modeling, speed boosts, and how to run R remotely",
    "section": "",
    "text": "files/header-code/header-code.html"
  },
  {
    "objectID": "posts/remote-computing/remote-computing.html#use-a-cluster",
    "href": "posts/remote-computing/remote-computing.html#use-a-cluster",
    "title": "Bayesian modeling, speed boosts, and how to run R remotely",
    "section": "Use a Cluster",
    "text": "Use a Cluster\nIf you work at a larger university (or maybe even a smaller one) the chances are pretty good that the computer science/engineering department folks have a high performance computing cluster. The nice thing about these is that they are often available for use by anyone on campus.\nThe basic idea here is that you submit specific jobs that you want the cluster to process. These might be tasks that require significant processing speed or maybe more memory than your local machine can provide.\nAs an example, Kansas State University‚Äôs ‚ÄúBeocat‚Äù computing cluster is open to K-State faculty. You are first required to create an account. Once that‚Äôs done you have a personal profile that you can access and from which you can submit jobs for the cluster. Usually there‚Äôs some sort of manager that sorts jobs on the basis of estimated computing needs, duration, etc., so you might have to wait a little while for your models to start, finish, etc. You can also use a desktop client like Cyberduck that provides a GUI through which you can manage files and move them back and forth between your local machine and the cluster.\nI‚Äôll note that I had mixed success with this. R already runs on Beocat, but getting Stan and {brms} properly configured was a chore, and it never quite got here. I reached out for help a couple of times and the Beocat people (while very responsive) were never able to resolve things, so I had to find an alternative."
  },
  {
    "objectID": "posts/remote-computing/remote-computing.html#amazon-web-services",
    "href": "posts/remote-computing/remote-computing.html#amazon-web-services",
    "title": "Bayesian modeling, speed boosts, and how to run R remotely",
    "section": "Amazon Web Services",
    "text": "Amazon Web Services\nAmazon Web Services is another option. Someone recommended this to me, and I started to look into it but never actually used it. My impression is that is functions much like accessing a university-based cluster. Amazon has a bunch of powerful computers that you can submit jobs to. Unlike your university‚Äôs cluster, though, there may be some small fee attached to it. I‚Äôm always a bit wary of this sort of arrangement as I don‚Äôt trust my up-front estimates of time and resource needs to be accurate, but maybe others with more experience will have more to say."
  },
  {
    "objectID": "posts/remote-computing/remote-computing.html#r-studio-server",
    "href": "posts/remote-computing/remote-computing.html#r-studio-server",
    "title": "Bayesian modeling, speed boosts, and how to run R remotely",
    "section": "R Studio Server",
    "text": "R Studio Server\nThe first two options rely on accessing fairly substantial computing resources set up by third (second?) parties. But what if your jobs don‚Äôt require that much power or time? Can you find more local solutions? Yes.\nLet‚Äôs imagine that we have a slower laptop computer and a faster desktop computer located in our office. In this situation we might want to connect the laptop that we‚Äôre operating at home or at a coffee shop to the more powerful desktop so we can make the desktop do the more labor-intensive task of running our models.\nThis is the exact situation I now find myself in. I was fortunate enough to apply for a small internal grant for a new desktop and purchased a Mac Studio with the funds. It currently sits in my office on campus. I also often use my Macbook Pro, which as I mentioned previously doesn‚Äôt have a ton of memory. It‚Äôs fine for writing the models and code, but I don‚Äôt want to start a model that is going to suck up all of my memory and processing power, leaving me unable to complete other basic tasks.\nThere are a couple of ways that you can go about this. This first option was very graciously provided by Matti Vuorre in response to a question I posed on Mastodon. It requires that you download and install Tailscale, Docker, and possibly RStudio Server.\nI‚Äôll let people who are interested go ahead and check out Matti‚Äôs blog post on the subject because it‚Äôs more detailed than I can get into here. Suffice it to say his directions are very clear and I was able to get this approach up and running fairy easily.\nOnly a couple of major notes that I came across. First, make sure your remote (e.g.¬†desktop) computer can‚Äôt go to sleep as this will break the connection and you won‚Äôt be able to establish/re-establish the connection from your remote location.\nSecond, you‚Äôll also need something like Git/GitHub set up to help you move projects and files to the remote session."
  },
  {
    "objectID": "posts/remote-computing/remote-computing.html#mac-screen-share",
    "href": "posts/remote-computing/remote-computing.html#mac-screen-share",
    "title": "Bayesian modeling, speed boosts, and how to run R remotely",
    "section": "Mac Screen Share",
    "text": "Mac Screen Share\nAfter playing with Matti‚Äôs recommendation for a while I came across another even more direct solution‚ÄîMac OS‚Äô native Screen Sharing app. The only major limitation here that I‚Äôm currently aware of is that it requires both the local and remote computers to be Macs.\nFirst, download Tailscale (linked above) on the local and remote devices. This will establish a VPN that you can use to connect the two computers securely. The nice thing about Tailscale is that it‚Äôs super easy to use. Specifically, it can create a menu bar icon where you can easily access the IP addresses of the relevant machines connected to the network. There‚Äôs also a web-based dashboard that you can use to monitor the devices on the network.\n\n\n\n\n\n\nFigure¬†1: Screenshot showing the location of the sharing menu in System Settings.\n\n\n\nSecond, this also requires you to edit some of the security and connection settings on your remote machine. To do this you first open up the System Settings menu, either through the icon on your task bar at the bottom of your screen or by clicking on the Apple icon in the upper left. Then you click on the General menu, and then on sharing over on the right hand side. Figure¬†1 shows the locations of these buttons in Mac OS Ventura.\n\n\n\n\n\n\nFigure¬†2: Screenshot showing the security and permissions options that you can change\n\n\n\nOnce you‚Äôve navigated to this menu you then need to alter the security settings and permissions. Figure¬†2 shows the menu options here that you can change. A couple of these are linked by default, and I can‚Äôt remember which exactly, but you basically want to enable File Sharing, Remote Login, and Remote Management. You can also limit which profiles can log in to the computer, and you should definitely make sure that the device is still password protected with a solid password. Also note that you do not have to make these adjustments on your local machine. In my case I‚Äôve only made them on the more powerful machine I want to use remotely.\n\n\n\n\n\n\nFigure¬†3: Figure showing the screen sharing IP address menu\n\n\n\nFinally, if you type ‚ÄúScreen Sharing‚Äù into the finder on your local Mac (e.g.¬†your weaker laptop) and hit Enter it will open a small window where you can enter your remote device‚Äôs IP address from Tailscale. You can see this in Figure¬†3. After you hit ‚ÄúConnect‚Äù it will prompt you to log in to the remote device using your user credentials.\nThe great thing about this approach is it opens a window that you can use to directly control your remote device from your local screen. This means you can open RStudio and manipulate objects, files, apps, etc., as you normally would. I‚Äôm a lazy man, and being able to actually see the content and ‚Äúuse‚Äù the more powerful computer directly and without having to makes my life easier.\nSo far this option has worked great, and provided nobody points out some very obvious and critical security issues (I mean it‚Äôs definitely possible), I think this will become a staple of my workflow moving forward. That said, my understanding is that this should be pretty secure through a couple of mechanisms. First, more recent versions of the Mac OS secure connections when you‚Äôre using your login information, and keystrokes and mouse movements are all encrypted. Second, Tailscale uses end-to-end encryption and nothing is passed through a cloud-based server of any kind. There are other options that I‚Äôve come across, like using an ssh tunnel, but this is not a procedure I‚Äôm familiar with. Ultimately I‚Äôd defer to more computer-savvy people for more suggestions on how to use this method in the most secure way possible."
  },
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "Consulting",
    "section": "",
    "text": "I also offer freelance statistical and data analysis consulting services. I can work with you and your organization on a range of topics, including data collection, research study designs, performance metrics, statistical analyses, and more. For more information please visit my consulting page:\nPrior Analytics, LLC"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Student Resources",
    "section": "",
    "text": "This page contains general information for students who are currently enrolled in, or who are interested in taking, my classes.\n\nGeneral Advice\nStudents often ask for advice on how to succeed in class. The following is a list of general tips and guidelines that may be useful to anyone taking any of my undergraduate or graduate courses.\n\nAttend class regularly. The most basic thing you can do to improve your performance is to attend class regularly, take good notes, and pay attention. Please note that I post all lecture slides online, so you do not need to copy each individual slide. Instead, try to focus on the overarching concepts or additional details that supplement the core points appearing on the slides. Also, please note that missing class for family events or other social activities does not exempt you from completing assigned material on time. While I realize that everyone values different aspects of their college experience in different ways, please understand that it is not my responsibility to accommodate the social schedules of every student. My job is to provide you with an opportunity to learn‚Äîwhether or not you take advantage of that opportunity is up to you.\nDo the readings. As with attendance, another simple step you can take to improve performance is to keep up with the readings. Spend even a short amount of time each day with the material covered in class and in the assigned texts‚Äîspending 15 minutes per day reviewing material will be considerably more useful to you can 4 hours of cramming just before a test.\nAsk Questions. Don‚Äôt be afraid to ask questions. The whole reason you‚Äôre here is because you lack knowledge, not because you possess it. Whether it‚Äôs in class, after class, or during office hours, asking questions to clarify difficult material is a critical step.\nDocument special needs, disabilities, or illness early. I‚Äôm always happy to accommodate students with special needs, but please do not wait until the last week in the semester to inform me of something that has been systematically affecting your grade all semester. Please speak with the Student Access Center for more information.\n\n\n\nStudent Resources\nStudents deal with a lot. Aside from demanding class schedules, many students are also dealing with complicated personal lives, family issues, or working multiple jobs to make ends meet. This can take a toll on students, and many may be dealing with depression, anxiety, food insecurity, or other issues that can make it difficult to keep up in the classroom. The sheet below provides a variety of on-campus and off-campus resources for students.\n\nStudent Resource Sheet\n\n\n\nWriting Resources\n\nGoogle Scholar Instructions"
  },
  {
    "objectID": "posts/regex-data-table/regex-data-table.html",
    "href": "posts/regex-data-table/regex-data-table.html",
    "title": "Using regular expressions (regex) to turn raw text into a data table",
    "section": "",
    "text": "files/header-code/header-code.html"
  },
  {
    "objectID": "posts/regex-data-table/regex-data-table.html#lets-load-the-data",
    "href": "posts/regex-data-table/regex-data-table.html#lets-load-the-data",
    "title": "Using regular expressions (regex) to turn raw text into a data table",
    "section": "Let‚Äôs load the data",
    "text": "Let‚Äôs load the data\nOK, so let‚Äôs go ahead and load the raw data. Because we‚Äôre dealing with a text file that‚Äôs not in a nice format, we‚Äôll use the readr package to read the text file into R. Again, note that we can‚Äôt use the read_csv function because the data isn‚Äôt in a CSV format. Instead, we‚Äôll use the read_file function from the readr package to read the text file into R as one big character string.\nTo make it easy, we‚Äôll just call it ‚Äústring‚Äù. We‚Äôll also load the other packages that we‚Äôll be using.\nNext, take a quick look at the string object to get an idea of what the data look like in this format. I won‚Äôt run that here because it‚Äôs a massive string, but you can run string in the console to see what it looks like. Again, taking a look reveals some useful information.\nFirst, if you‚Äôre not familiar with reading in text data into R, you‚Äôll probably notice the weird characters like \\r and \\n that keep popping up. These are special characters that denote some feature of the underlying document that you read in. In this case, they mark line breaks.\nNow I‚Äôm not an expert on the genesis of this particular pattern‚Äîsome files only use \\n while files like this use both. This page does a deeper dive on the subject, but the important thing to know is that they‚Äôre explicit markers for features of the text that we looked at in Figure¬†1 above. To put it differently, even raw text files use different encoding schemes. Just because you see a line break or a blank line in a text file doesn‚Äôt mean there‚Äôs nothing there. Even blank spaces have to have some sort of encoding to be read by a computer to tell it that there‚Äôs a blank space! Weird!\nAnyway, these markers are a good first place to start. All we‚Äôve done so far is read the file into one massive character string. That means we have 9,000+ agreements all recorded in a single cell, and that just won‚Äôt do."
  },
  {
    "objectID": "posts/regex-data-table/regex-data-table.html#splitting-the-data-into-rows",
    "href": "posts/regex-data-table/regex-data-table.html#splitting-the-data-into-rows",
    "title": "Using regular expressions (regex) to turn raw text into a data table",
    "section": "Splitting the data into rows",
    "text": "Splitting the data into rows\nThe next step is going to be splitting the data into separate rows so that each individual agreement is in its own row. We can do this by splitting the string based on the line breaks.\n\ndata &lt;- string |&gt;\n  stringr::str_split(\"\\\\r\\\\n\\\\r\\\\n\") \n\nWhat I‚Äôm doing here is creating a new object named data, since we‚Äôre moving from the raw text and this will ultimately be the data that we want to work with.\nThe str_split function is from the stringr package, which is part of the tidyverse. The first argument to str_split is the string that we want to split. In this case, we‚Äôre splitting the string based on the regular expression \\\\r\\\\n\\\\r\\\\n. This regular expression tells R to split the string based on the pattern of two line breaks. The \\\\r and \\\\n are the special characters that denote line breaks, and the \\\\r\\\\n\\\\r\\\\n pattern tells R to split the string based on two line breaks in a row.\nWhy two in a row? Well, if we look back at Figure¬†1, we can see that each agreement is separated by a blank line, meaning two line breaks. This is a consistent pattern that we can use to split the string into separate agreements.\nNow let‚Äôs take a look to make sure we‚Äôre getting separate lines for each agreement. But note that it‚Äôs still going to be a character list, so we can‚Äôt use the head() function to look at the first few rows.\n\ndata[[1]][1:3]\n\n[1] \"\\r\\nA\\r\\nAFGHANISTAN\\r\\nCULTURAL EXCHANGES, PROPERTY & COOPERATION\\r\\nAgreement relating to the exchange of official publications.\\r\\nExchange of notes at Kabul February 29, 1944. Entered into force February 29, 1944.\\r\\n58 Stat. 1393; EAS 418; 5 Bevans 3; 106 UNTS 247\"\n[2] \"Agreement concerning cultural relations. Exchange of notes at Washington June 26, 1958. Entered into force June 26, 1958.\\r\\n9 UST 997; TIAS 4069; 321 UNTS 67\"                                                                                                               \n[3] \"Agreement relating to the establishment of a Peace Corps program in Afghanistan.\\r\\nExchange of notes at Kabul September 6 and 11, 1962. Entered into force September 11, 1962.\\r\\n13 UST 2100; TIAS 5169; 461 UNTS 169\"                                                      \n\n\nGreat! We can see that we have three agreements in the first three rows. I‚Äôll give a spoiler here and say that in reality we have 9,000+ agreements and a couple of the individual rows don‚Äôt correspond to agreements, but we‚Äôre on the right track and this is stuff we can clean up later.\nSo let‚Äôs go ahead and convert this into a data frame.\n\ndata &lt;- data |&gt; \n  as.data.frame(col.names = \"agreement_text\")\n\nAll we‚Äôre doing here is converting the list to a data frame and naming the resulting column ‚Äúagreement_text‚Äù."
  },
  {
    "objectID": "posts/regex-data-table/regex-data-table.html#extracting-relevant-data",
    "href": "posts/regex-data-table/regex-data-table.html#extracting-relevant-data",
    "title": "Using regular expressions (regex) to turn raw text into a data table",
    "section": "Extracting relevant data",
    "text": "Extracting relevant data\nNow that we have the data in a data frame, we can start extracting the relevant information from each agreement. This is where regular expressions come in handy.\nI‚Äôm going to walk through the code that I used to extract the relevant information from each agreement. I‚Äôll break it down into smaller steps so that it‚Äôs easier to follow. That said, there‚Äôs lots of information to go over here, so I‚Äôll just give a brief overview of what I‚Äôm doing in each step. I‚Äôve included notes for each line, so you can refer to those, too.\n\nCountry codes and names\nThe data are structured so that agreements are listed by individual country and agreement type. I want to first break out the individual country information for each agreement, and for that I‚Äôm going to use the countrycode package. This is one of the most useful packages around if you work with international relations or comparative politics data.\nThe first step will be to generate the correlates of war (COW) country code for each country that appears in the data. The countrycode package is super useful as it will scan the agreement text and match country names it finds there.\nThe second step is to identify the country names based on the COW codes we just pulled from the agreement text. We also want to remove any that read ‚ÄúUnited States‚Äù because we know that these are all US agreements with other countries, and so any of those matches are just incidental and we can replace them with missing values (i.e.¬†NA).\n\ndata &lt;- data |&gt; \n  mutate(ccode = countrycode::countrycode(agreement_text, origin = \"country.name\",\n                                      destination = \"cown\"), # Extract the country code from the agreement text\n        ccode = case_when( # Remove the United States. countrycode picked it up from agreement text.\n          ccode == 2 ~ NA,\n          TRUE ~ ccode\n        ),\n        countryname = countrycode::countrycode(ccode, origin = \"cown\", # create standardized country name variable\n                                      destination = \"country.name\"),\n        countryname = case_when( # Remove the United States. countrycode picked it up from agreement text.\n          countryname == \"United States\" ~ NA,\n          TRUE ~ countryname\n        )\n        )\n\n\n\nAgreement types\nAll of the agreements belong to different classes of agreements, depending on the specific issue areas they address. For example, some pertain to defense, others to cultural issues, and some to economics and finance. As with the country names, agreements are organized by type, so we can pull the relevant text from the agreement_text column.\nThis part is trickier, and it took me a lot of trial and error. But, from looking at the raw data we can see that the agreement type is always written out in all capital letters. Again, a super useful detail!\nThe str_extract function is super useful here. I‚Äôm using it to extract the parts of the agreement_text column that match the provided regular expression. In this case, we want to tell it to look for all of the capital letters at the beginning of the string (i.e.¬†the agreement_text cell). More specifically, we‚Äôll use it within the mutate() function to create a new column called type that contains the extracted agreement type.\n\ndata &lt;- data |&gt; \n  mutate(type = str_extract(agreement_text, \"^[A-ZA-Z\\\\sA-Z\\\\s\\\\W]*\\\\b\"), # Extract the type of agreement\n)\n\nThe regular expression I‚Äôm using here is ^[A-ZA-Z\\\\sA-Z\\\\s\\\\W]*\\\\b. This is a bit complicated, but it‚Äôs basically doing the following:\n\n^ - This tells R to start looking at the beginning of the string.\n[A-ZA-Z\\\\sA-Z\\\\s\\\\W]* - This tells R to look for any number of capital letters (i.e.¬†A-Z), any number of capital letters followed by spaces (i.e.¬†A-Z\\\\s), and any number of capital letters followed by a space and special characters like punctuation marks (i.e.¬†A-Z\\\\s\\\\W). The * means that it can match zero or more of these characters.\n\\\\b - This tells R to stop looking when it reaches a word boundary. This is important because we don‚Äôt want to match any additional characters that might come after the agreement type. If we omit this part then we also end up picking up the first capital letter of the next chunk of text.\n\nIt‚Äôs important to know that you can write this out in different ways that will all yield the same result. I found this to be a more efficient way to do it than my first pass, but there are other ways to write the regular expression that will also work. For example, you could also write it as ^[A-Z]*(?:[A-Z ]*[A-Z \\\\W]*[A-Z])?\\\\b. This is a little more complicated, but it should produce the same results.\n\n\nDates\nThe next bit of information that we want to extract is the dates associated with each agreement. This includes the date that the agreement was signed or where there was an exchange of diplomatic notes, the date that it entered into force, and any dates where the agreement might have been amended.\n\ndata &lt;- data |&gt; \n  mutate(exchange = str_extract(agreement_text, \"Exchange of.*\\\\.\"), # Extract the exchange of notes date\n         exchange_year = str_extract(exchange, \"\\\\d{4}\"), # Extract the year of the exchange of notes\n         signed = str_extract(agreement_text, \"Signed.*\\\\.\"), # Extract the signed date\n         signed_year = str_extract(signed, \"\\\\d{4}\"), # Extract the year of the signed date\n         entered_force = str_extract(agreement_text, \"Entered.*\\\\.|Entered*\\\\;\"), # Notice some entries have effective date, which is different from entered into force.\n         entered_force_year = str_extract(entered_force, \"\\\\d{4}\"), # Extract the year of the entered into force date\n         amended = str_extract(agreement_text, \"(.*Amendment.*)(?&lt;=Amendment)(?s)(.*$)\"), # Extract the amendment text\n         amended_year = str_extract_all(amended, \"(?&lt;=\\\\,\\\\s)\\\\d{4}\") # Extract the version number using amended dates\n  )\n\nThis is a bit clunky, and I‚Äôm sure that someone who is better at regular expressions could do this more efficiently. But the basic idea is that we‚Äôre using the str_extract function to extract the relevant dates from the agreement_text column.\nFor each date type I‚Äôm essentially performing the following steps. We can look at the exchange of notes year as an example:\n\nExchange of.*\\\\. - This regular expression tells R to look for the text ‚ÄúExchange of‚Äù followed by any number of characters and ending with a period. This will match the entire sentence that contains the exchange of notes date.\n\\\\d{4} - This regular expression tells R to look for a four-digit number. This will match the year of the exchange of notes date.\n\nThe other date types are extracted in a similar way. The only difference is that the regular expressions are tailored to match the specific text patterns that correspond to each date type.\nWhat‚Äôs clunky about this method is that I first extract the full string that corresponds to each date that we‚Äôre interested in. For example, I first extract the full sentence that contains ‚ÄúExchange of notes‚Ä¶‚Äù and then extract the year from that individual sentence. I‚Äôve tried other methods, like using the lookaround operators and non-capture groupings, to try to pull the years from sentences containing certain phrases, but I haven‚Äôt had much luck getting it to work. I‚Äôm sure there‚Äôs a more efficient way to do this, but this is what I came up with.\nAnother tricky bit concerns the amendment years. some agreements are never amended. Some only once. Others multiple times. So I‚Äôm using the str_extract_all function to extract all of the years that appear in the amended column. This will return a list of years for each agreement in the amended cell, which we can then unnest to create a unique row for each year. But this also requires us to create a value for the starting year of every agreement if we‚Äôre interested in tracking the version or iteration of any given agreement.\n\n\nCleaning up the data\nThe last step is to clean up the data. The biggest step here is to finish organizing the agreements according to amendment iteration and filling in the country names, agreement types, etc. This also involves removing any leading white spaces or punctuation marks from the date columns and converting the year columns to numeric encoding. It also involves removing any rows that have empty agreement text or missing version numbers.\nThe first two lines use the fill function to fill in the country code and country name columns. This is necessary because the country code and country name are only listed once for each country, but we want them to be listed for each agreement. The fill function fills in the missing values based on the previous non-missing value.\nNext, I want to ensure that every agreement has a ‚Äúbaseline‚Äù amendment year so we can expand and reshape the data. To do this, I group the data by individual agreement using the rowwise() function. Then I create the amended_year variable using the entered into force year and the amended years. We‚Äôll come back to this in a second.\n\ndata &lt;- data |&gt; \n  tidyr::fill(ccode, .direction = \"down\") |&gt; # Fill in the country code\n  tidyr::fill(countryname, .direction = \"down\") |&gt; # Fill in the country name\n  rowwise() |&gt; # Do this so the mutate works! Runs following functions on one row at a time.\n  mutate(amended_year = list(c(print(entered_force_year), print(amended_year))), # Print the entered into force year and version number\n         type = str_remove(type, paste0(\"A\\\\r\\\\n\", toupper(countryname), \"\\\\r\\\\n\")), # Remove the country name from the type\n         type = case_when( # Make missing agreement types NA\n           type == \"\" ~ NA,\n           TRUE ~ type\n         )) |&gt;\n  ungroup() |&gt;  # Do this so fill works! Removes grouping created by rowwise() above.\n  tidyr::fill(type, .direction = \"down\") |&gt; # Fill in the agreement type\n  mutate(type = factor(type)) |&gt; # Make the agreement type a factor\n  group_by(ccode) |&gt; # Group by country code\n  mutate(agreement_num = glue::glue(\"{ccode}-{as.numeric(type)*1000}-{row_number()+1000}\")) |&gt; # Create an agreement number\n  unnest(amended_year) |&gt; # Unnest by version number. This creates a unique row/observation for each value of the amendment dates/version number stored in amended_year.\n  dplyr::filter(agreement_text != \"\") |&gt; # Remove rows with empty agreement text\n  dplyr::filter(!is.na(amended_year)) |&gt; # Remove rows with NA version numbers\n  mutate(across(c(exchange, signed, entered_force), # Remove leading white spaces and punctuation marks.\n                ~ str_remove_all(., \"^\\\\s|^[:punct:]\")),\n         across(c(exchange_year, signed_year, entered_force_year, amended_year), # Convert to numeric encoding.\n                ~ as.numeric(.))) |&gt;\n  dplyr::select(agreement_text, ccode, countryname, type, exchange, exchange_year, signed, signed_year, entered_force, entered_force_year, agreement_num, amended_year) |&gt;\n  arrange(ccode, type, agreement_num, amended_year) |&gt;\n  group_by(agreement_num) |&gt;\n  mutate(version_num = row_number())\n\nThe next two lines in the mutate() function just clean up the agreement type variable by removing the country name from the type. This is necessary because the country name is included in the agreement type for some agreements. The str_remove function is used to remove the country name from the agreement type and then we also recode empty rows NA if there‚Äôs no agreement type. This is an artifact of some lines having no agreement listed, but they‚Äôre given their own row because of a white space or line break. Next we ungroup the data and we fill the agreement type down.\nThe only real important line left in this code is the unnest(amended_year) function. This function is used to unnest the amended_year column. Remember the cells that contained the list values for every year that an agreement was amended? Well we want a separate line for each of those amended versions of the agreement. If you‚Äôre familiar with IR data, you can think of this in a similar way to thinking about country-year data. We might be interested in when or why an agreement changes, so we want to break that out into separate rows. This is what the unnest function does. It takes a list column and creates a new row for each value in the list.\nThis is also why we wanted to make sure we had at least one year listed in this column, even if an agreement was never actually amended‚Äîit ensures that once we unnest the data on the amended_year column we have a unique row for each agreement (even those that were never amended).\nIf you look at the data in your viewer now, you should see a nice country-agreement-year data set that you can use to analyze the agreements. Cool beans."
  },
  {
    "objectID": "posts/hurdle-lognormal-densities/hurdle-lognormal-densities.html",
    "href": "posts/hurdle-lognormal-densities/hurdle-lognormal-densities.html",
    "title": "Hurdle lognormal distribution densities?",
    "section": "",
    "text": "files/header-code/header-code.html\nAs a part of a larger project I‚Äôve been working with BRMS and some models and family functions that are new to me. In particular, my work on troop deployments has made me think more about hurdle models. I‚Äôve had some experience with zero-inflated models in the past, but haven‚Äôt spent a lot of time with hurdle models, specifically. Anyway, without going down the rabbit hole, I‚Äôve been thinking about how to create probability density functions for some of these models. The {countreg} package contains some functions for hurdle negative binomial models, which got me thinking about building something similar for hurdle lognormal distributions. I‚Äôm really operating one the frontiers of my own experience/abilities, so I may be way off here, but let‚Äôs give this a shot and see if it works.\n\nBackground\nI work a lot with military deployment data. Typically these are country-year observations of the number of US military personnel stationed in various overseas locations (think Germany, Japan, etc.). So far most of our work has treated deployments as a predictor variable, but more recently we‚Äôve started thinking more about modeling deployment levels themselves. In general, there tends to be a ton of skew in these data. For example, from 1990 forward the troop deployment data we have are more or less distributed like this:\n\n\n# Simulation . Values reflect what we see in our data.\nsims &lt;- 1e4\nmuval = 2.8\nsdval = 2.54\npival = 0.2\n\n\nsimvals &lt;- rep(NA, sims)\nsimvals[c(1:2000)] &lt;- rep(0, sims*0.2)\nsimvals[c(2001:10000)] &lt;- rlnorm(sims*0.8, meanlog = muval, sdlog = sdval)\n\n\nhist(log1p(simvals), breaks = 200, main = \"Distribution of Simulated Troop Data\")\n\n\n\n\n\n\n\n\nWe‚Äôve got about 20% zero values, and the non-zero values have a median of 16 and a mean of about 1,700. Conceivably every country could receive deployments, but some are highly unlikely to (e.g.¬†North Korea). But even countries that do host US personnel tend to host very small deployments, as you can see by the relatively small median value. The mean is dragged upwards by large, long-standing legacy deployments in places like Germany, Japan, and South Korea.\n\n\nProblem\nI‚Äôm glossing over a lot of the details here, but working on this has prompted me to think more about using hurdle models (like I already said). Yada yada yada, this has led me to think about what a probability density function for a hurdle model looks like. The {stats} package in R comes with a set of functions for handling lognormal distributions, but doesn‚Äôt appear to have anything to handle hurdle variants. Riffing off of the aforementioned {countreg} package, this is my attempt to create something comparable for hurdle lognormal distributions (I couldn‚Äôt find one with Elaine).\n\nSeinfeld Jason Alexander GIF from Seinfeld GIFs\n\n\n\n\nMaybe a working probability density function?\nThis is my shot at creating a probability density function‚Ä¶umm‚Ä¶function. I started with the builtin dlnorm() function, but the problem is that it only accepts positive values. If we have a lot of data that are failing to cross that hurdle (i.e.¬†0 values) then this doesn‚Äôt really work. I think the solution is to insert an argument that details the propotion/probability of 0 values in the data (i.e.¬†the pval argument). For non-zero values we have to first get the probability using dlnorm(). Once we generate the probability value for the x value using dlnorm() we then have to weight that probability value by the proportion of non-zero values in the data. In this case we enter a pval argument of 0.2 since 20% of the data are 0s, but when we calculate the probability for some values &gt; 0 we want to make sure that‚Äôs weighted by 0.8 since 80% of observations are &gt; 0. So really this is \\(1 - pval\\). If x equals zero then the probability should (I think?) just default to the pval argument.\n\n\ndhlnorm &lt;- function(x, meanlog, sdlog, pval) {\n  \n    if (x &gt; 0) {\n    \n    value &lt;- dlnorm(x, meanlog = meanlog, sdlog = sdlog, log = FALSE) * (1-pval)\n    \n    return(value)\n    \n  } else {\n    \n    value &lt;- pval\n    \n    return(value)\n  }\n  }\n\n\nprob &lt;- dhlnorm(3, meanlog = muval, sdlog = sdval, pval = 0.2)\n\nprint(prob)\n## [1] 0.03346686\n\nAgain, this is very much on the frontiers of my experience, so if anyone has any feedback I‚Äôd appreciate it. I have some potential applications in mind, so this is more than just running down a rabbit hole. I promise."
  },
  {
    "objectID": "posts/tables-modelsummary-brms/tables-modelsummary-brms.html",
    "href": "posts/tables-modelsummary-brms/tables-modelsummary-brms.html",
    "title": "Making tables for multinomial models with {modelsummary} and {brms}",
    "section": "",
    "text": "files/header-code/header-code.html\nNOTE Since moving the site to Quarto I‚Äôve been having some trouble getting Quarto to render this particular post. Something to do witht he particular model objects that I read in in the second code chunk. I think the post is still useful, but I‚Äôm going to omit the output from the code chunks since it breaks the post. I‚Äôll try to follow up with someone smarter than I to see how I can go about fixing things.\ntl;dr: Learn how to make some cool and customizable tables for multinomial logit models using {brms} and {modelsummary}.\nUPDATE: Vincent informed me that the most recent version of {modelsummary} relies entirely on the {parameters} package. Apparently the {broom} package will no longer be actively developing. Keep this in mind if you‚Äôre trying this approach and get stuck.\n\nBackground\nI‚Äôm coming off a couple of long projects where we were using a lot of multinomial logit models, and making publication quality tables was a major challenge. Actually, started using {brms} several years ago partly because I had data where we had 1) lots of individuals making choices, 2) those individuals were all grouped in some pretty clear ways, and 3) we were also interested in modeling group-level characteristics that might relate to individuals‚Äô choices. This more or less marked my full transition from Stata to R and from frequentist stats to Bayesian stats.\nEarly on the biggest problem I ran into was finding a way to generate tables for multinomial models run using {brms}. Initially most packages didn‚Äôt support {brms} and/or developing tables for multilevel/hierarchical models required a lot of extra legwork. Building tables to accommodate choice models was another issue. Taken together, these problems meant that I had to write a lot of extensive code by hand to generate clean Latex tables for the models I was using. The process has, thankfully, become much simpler over the last couple of years.\nFirst, for those who aren‚Äôt familiar {brms} is an amazing package created by Paul B√ºrkner. It stands for Bayesian Regression Modeling using Stan, and, as the name suggests, provides users with a convenient front-end for building regression models using Stan as a back end. This is particularly useful for building multilevel models, but also lots of other stuff.\nSecond, {modelsummary} is another amazing and ever-expanding package created by Vincent Arel-Bundock. This packages does a few different things, but most importantly and prominently it helps users to create excellent tables for summarizing data and building tables for regression output. The package is incredibly flexible, supports dozens of different model types, and Vincent is constantly adding new features.\nBoth packages are fantastic. If you‚Äôre interested in Bayesian modeling, or just looking for a new and easy way to build tables for whatever modeling package you already use, you should check out one or both of these packages.\n\n\nMultilevel Multinomial Logit Models\nLots of regression models are going to be fairly simple to present in a table format, and there are some fairly easy ways to go about generating those tables. Typically you‚Äôll have a single column per model, and each row of your table will be for a single variable. You might also have some summary statistics for the model at the bottom (think $ N $, $ R^2 $, etc.).\nMultinomial models get a little more complicated because you‚Äôll typically have multiple outcomes. Specifically, you‚Äôll have $k-1 $ columns to present in the table, where $k $ is the number of choices respondents have. In our case we had lots of models where survey respondents offered their assessments of various actors and we condensed those assessments down into four general categories: Positive, Neutral, Negative, and Don‚Äôt Know. This means we ultimately ended up with three columns per model, with the ‚ÄúNeutral‚Äù response serving as the baseline category against which the others were compared.\nMultilevel models complicate things slightly because you may also have summary statistics for the groups in your data in addition to the general summary statistics for the model.\nLast, Bayesian models and {brms} specifically provide users with a ton of additional information they might want to present beyond the traditional stuff you‚Äôd find in frequentist models. Some of this information can take a long time to compile. There‚Äôs often going to be an efficiency and transparency tradeoff here, and so you may want to customize what you present in your table. Even if you end up presenting lots of information in the end, having the ability to control what‚Äôs in your table at the outset can be really useful as you run the code to make sure the basic output looks right.\nAnyway, the goal here is rather niche, but it‚Äôs to talk through the process of building nice and readable tables when we‚Äôre using these models and have lots of information to present. {modelsummary} lets us do this, but also requires a bit of additional effort to fully customize our output.\n\n\nGetting Started\nOK, first we‚Äôre going to load our libraries. The relevance of some of these is immediately obvious given what I‚Äôve already said, but I‚Äôll talk more about some of the additional packages we need below.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(here)\nlibrary(modelsummary)\nlibrary(brms)\nlibrary(parameters)\nlibrary(kableExtra)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(data.table)\n\nIn addition to loading the libraries we want to load our model objects. In this case I have three models, each with three outcome categories. Each of these ends with a ‚Äúp1‚Äù or a ‚Äút1‚Äù to denote the reference group for the model‚Äôs outcome variable (e.g.¬†if the outcome variable is asking about troops, people, or government).\nThen I‚Äôm creating a list object to store the three model objects. Note here that I‚Äôm leaving the label for the individual models blank by including the empty quotation marks in the list function. Depending on your situation you can go ahead and name these if you want. In my case it makes more sense to keep them empty because I plan to add a grouping header/title later in the final table and based on how {modelsummary} works including the titles here would create some redundancies in the final output and take up extra horizontal space. If you‚Äôre working with HTML output and you have scrollable tables maybe this doesn‚Äôt matter, but it can matter a lot for print versions..\n\nm.c.t1 &lt;- readRDS(here::here(\"files/data-files/m.c.t1.rds\"))\nm.c.p1 &lt;- readRDS(here::here(\"files/data-files/m.c.p1.rds\"))\nm.c.g1 &lt;- readRDS(here::here(\"files/data-files/m.c.g1.rds\"))\n\n# Create a list object to store the three separate model objects.\nmod.list &lt;- list(\" \" = m.c.t1,\n                 \" \" = m.c.p1,\n                 \" \" = m.c.g1)\n\nBefore we move on, let‚Äôs take a quick look at the models and what they look like.\n\nsummary(mod.list[[1]])\n\nThere‚Äôs a lot going on here, but you can see from the summary output that organizing this could be a bit of a bear. We have three outcome categories, lots of categorical variables (some conceptually related), model summary statistics, etc.\n\n\nCustomization\nThe first two chunks are pretty boiler plate, and for lots of types of models you can probably just go on to use {modelsummary} directly and get some nice tables. But this section is going to dive into some of the extra steps required to make the tables look nice and clean, but also to save us a ton of time.\nThe big issue that we have to address is that {modelsummary} is going to automatically try to generate lots of different types of goodness-of-fit or model summary statistics for the models you‚Äôre including. If this were OLS it would be super quick. But since we‚Äôre using {brms} models the defaults can run for a very long time and, depending on your workflow needs, cause some major slowdowns.\n{modelsummary} is using the packages like {tidy}, {parameters}, and {broom} to extract information from the models in your list and to generate a basic data frame with that output that serves as the basis for the final table. Things like WAIC and LOOIC can take a long time to calculate, and depending on your needs you might not want them right way.\nBeyond that, you might just want to customize how the footer of your table looks, what information is included in which places, etc. This is a good way to do that, but it takes a little extra work.\nFirst, we need to assign a ‚Äúcustom‚Äù class to each of the three model objects stored in the mod.list object. Assigning the custom class is going allow us to write some custom {tidy} and {glance} functions that will let us select the specific summary stats and other model info that we want to include.\n\nfor (i in seq_along(mod.list)){\n  class(mod.list[[i]]) &lt;- c(\"custom\", class(mod.list[[i]]))\n}\n\nNext we can write out custom tidy function, appropriately labeled here tidy.custom. Actually, I think you have to name it this so {modelsummary} recognizes that you want to use a custom function.\nThere‚Äôs a lot going on here, and some of it is going to be specific to the models I‚Äôm using as an example.\nFirst, you create the function, where x is the object placeholder, and conf.level=.95 is specifying the default confidence/credible interval threshold. The ... just leaves it open for other arguments.\nNext we create an object named out using the {parameters} package. First we start off creating a data frame using this function, then we standardize all of the variable/column names (i.e.¬†make them lower case). So far pretty standard.\nThe mutate chunk is where things get more complicated, and where you‚Äôll need to substitute your own model-specific factors. {modelsummary} also has a formula-based argument for helping you to arrange your models in the resulting table, but we need to do a little extra work here to properly identify the outcome variable levels. This is partly a function of the fact that {brms} does some weird things like attaching outcome choices (i.e.¬†the binary outcome variable for the individual equations) as prefixes to each variable name, meaning there are no outcome levels for {parameters} to automatically detect. So we need to create them.\nAll I‚Äôm going here is using case_when(...) to identify the outcome variable levels/choices and creating a categorical y.level variable containing the full name for each of the choices for the outcome variable. As I mentioned above, those are ‚ÄúDon‚Äôt know‚Äù, ‚ÄúNegative‚Äù, and ‚ÄúPositive‚Äù (with ‚ÄúNeutral‚Äù as the omitted reference category).\nNext, I take the term column that {parameters} generates, which contains all of the predictor variables, and I remove the outcome level prefixes that {brms} attaches. This way I have two columns with variable names (term) and the outcome variable choice/model equation (i.e.¬†y.level).\n\n# tidy method extracts level names into new column\ntidy.custom &lt;- function(x, conf.level=.95, ...) {                                   # Create function\n  out = parameters::parameters(x, ci=conf.level, verbose=FALSE) |&gt;                 # Call {parameters} to pull model parameter info with specified credible interval\n        parameters::standardize_names(style=\"broom\") |&gt;                            # make names lower case\n        mutate(y.level = case_when(grepl(\"mudk\", term) ~ \"Don't Know\",              # Change outcome level values to plain meaning for output table\n                                 grepl(\"muneg\", term) ~ \"Negative\",\n                                 grepl(\"mupos\", term) ~ \"Positive\"),\n               term = gsub(\"dk|neg|pos\", \"\", term))                                 # remove outcome prefix {brms} attaches to variable names\n  return(out)\n}\n\nNext we can move on to writing a custom function to pull the relevant model and summary statistic information. First, we need to tell glance() to quiet down since it‚Äôs going to do lots of stuff we don‚Äôt necessarily want it to do right now. I can‚Äôt remember if the gof.check lines are necessary at this point (I seem to recall it had no effect one way or the other when I was initially working on this), but I‚Äôll turn those off just to be safe, too.\n\n# Write custom glance function to extract summary information.\nglance.custom &lt;- function(x, ...) {\n  ret &lt;- tibble::tibble(N = summary(x)$nobs)\n  ret\n}\n\n# Turn off GOF stuff\ngof.check &lt;- modelsummary::gof_map\ngof.check$omit &lt;- TRUE\n\nNext we can move on to using the {parameters} package to full information on the ‚Äúrandom‚Äù part of our varying intercepts model. We‚Äôll also use this step to include some basic summary information on the models as mentioned above.\nUltimately the goal here is to generate a clean data frame containing summary information that we want. I‚Äôve included more specific comments in the code chunk below for readers who want to scrutinize each step, but much of this mirrors what we just did with the tidy.custom() function above, it‚Äôs just doing it to the ‚Äúrandom‚Äù or ‚Äúvarying‚Äù part of the model rather than the population-level coefficients.\n\n# Write function to loop over list of models\nrows &lt;- lapply(mod.list, function(x){\n\n  temp.sd &lt;- parameters::parameters(x, effect = \"random\")  |&gt;                   # start with parameters and pull \"random\" component of model\n    filter(grepl(\".*sd.*\", Parameter)) |&gt;                                       # filter out parameters containing standard deviation info\n    filter(grepl(\".*persYes.*|.*nonpersYes.*|.*Intercept.*\", Parameter)) |&gt;     # further filter parameters containing SD info for relevant variables\n    dplyr::select(Parameter, Median) |&gt;                                         # Keep only the Parameter (name) column and the Median column\n    dplyr::mutate(Median = as.character(round(Median, 2)),\n      y.level = case_when(                                                       # Like before, create a y.level column for outcome variable level/equation\n      grepl(\".*mudk.*\", Parameter) ~ \"dk\",\n      grepl(\".*mupos.*\", Parameter) ~ \"pos\",\n      grepl(\".*muneg.*\", Parameter) ~ \"neg\",\n    ),\n    Parameter = gsub(\"_mudk_|_mupos_|_muneg_\", \"_\", Parameter)) |&gt;              # Remove the {brms} prefix from the Parameter column\n    pivot_wider(id_cols = Parameter,                                             # Rearrange  columns from long to wide\n                values_from = Median,\n                names_from = y.level) |&gt; \n    mutate(Parameter = case_when(                                                # Rename relevant parameters to appear how you want in text\n      grepl(\".*Intercept.*\", Parameter) ~ \"sd(Intercept)\"\n    ))\n  \n  temp.obs &lt;- tibble::tribble(~Parameter, ~dk, ~neg, ~pos,                       # Create another data frame containing observation count and grouping info\n                              \"N\", as.character(nobs(x)), \"\", \"\",\n                              \"Group\", \"Country\", \"\", \"\",\n                              \"\\\\# Groups\", as.character(length(unique(x$data$country))), \"\", \"\")\n  \n\n  temp.com &lt;- bind_rows(temp.obs, temp.sd)                                       # Bind two data frames together for consolidated footer data frame\n\n  return(temp.com)\n  \n  }\n)\n\n# Group everything from the three models together\n# Also select relevant columns containing information\nrows.com &lt;- bind_cols(rows[[1]], rows[[2]], rows[[3]]) |&gt; \n  dplyr::select(1, 2, 3, 4, 6, 7, 8, 10, 11, 12) \n\n# Rename those columns so they'll match eventual output data frame names\nnames(rows.com) &lt;- c(\"term\", \"col1\", \"col2\", \"col3\", \"col4\", \"col5\", \"col6\", \"col7\", \"col8\", \"col9\")\n\nGreat! Next we can start cleaning up the variable names for presenting them in the table, and we can even go a step further to add grouping labels to help readers move between broad categories of predictor variables (e.g.¬†income, age, etc.).\nHere‚Äôs we‚Äôre going to generate a tribble with the ‚Äúraw‚Äù variable names and the ‚Äúclean‚Äù label for presentation. Note that we have to arrange them in the order in which we want them to appear.\n\ncoef.list &lt;- tibble::tribble(~raw, ~clean,\n                            \"b_mu_contact_persYes\", \"Personal Contact: Yes\",\n                            \"b_mu_contact_persDontknowDdeclinetoanswer\", \"Personal Contact: DK/Decline\",\n                            \"b_mu_contact_nonpersYes\", \"Network Contact: Yes\",\n                            \"b_mu_contact_nonpersDontknowDdeclinetoanswer\", \"Network Contact: DK/Decline\",\n                            \"b_mu_benefit_persYes\", \"Personal Benefit: Yes\",\n                            \"b_mu_benefit_persDontknowDdeclinetoanswer\", \"Personal Benefit: DK/Decline\",\n                            \"b_mu_benefit_nonpersYes\", \"Network Benefit: Yes\",\n                            \"b_mu_benefit_nonpersDontknowDdeclinetoanswer\", \"Network Benefit: DK/Decline\",\n                            \"b_mu_age25to34years\", \"25-34\",\n                            \"b_mu_age35to44years\", \"35-44\",\n                            \"b_mu_age45to54years\", \"45-54\",\n                            \"b_mu_age55to64years\", \"55-65\",\n                            \"b_mu_ageAge65orolder\", \"&gt;65\",\n                            \"b_mu_income.5.cat21M40%\", \"21-40\",\n                            \"b_mu_income.5.cat41M60%\", \"41-60\",\n                            \"b_mu_income.5.cat61M80%\", \"61-80\",\n                            \"b_mu_income.5.cat81M100%\", \"81-100\",\n                            \"b_mu_genderFemale\", \"Female\",\n                            \"b_mu_genderNonMbinary\", \"Non-binary\",\n                            \"b_mu_genderNoneoftheabove\", \"None of the above\",\n                            \"b_mu_minorityYes\", \"Minority: Yes\",\n                            \"b_mu_minorityDeclinetoanswer\", \"Minoriy: Decline to answer\",\n                            \"b_mu_religCatholicism\", \"Catholic\",\n                            \"b_mu_religChristianityprotestant\", \"Protestant\",\n                            \"b_mu_religBuddhism\", \"Buddhism\",\n                            \"b_mu_religHinduism\", \"Hindu\",\n                            \"b_mu_religIslam\", \"Islam\",\n                            \"b_mu_religJudaism\", \"Judaism\",\n                            \"b_mu_religShinto\", \"Shinto\",\n                            \"b_mu_religMormonism\", \"Mormonism\",\n                            \"b_mu_religLocal\", \"Local Religion\",\n                            \"b_mu_religOther\", \"Other\",\n                            \"b_mu_religDeclinetoanswer\", \"Religion: Decline to answer\",\n                            \"b_mu_ed_z\", \"Education\",\n                            \"b_mu_ideology_z\", \"Ideology\",\n                            \"b_mu_troops_crime_persYes\", \"Personal Crime Experience: Yes\",\n                            \"b_mu_american_inf_1DontknowDdeclinetoanswer\", \"Influence 1: DK/Decline\",\n                            \"b_mu_american_inf_1Alittle\", \"Influence 1: A little\",\n                            \"b_mu_american_inf_1Some\", \"Influence 1: Some\",\n                            \"b_mu_american_inf_1Alot\", \"Influence 1: A lot\",\n                            \"b_mu_american_inf_2DontknowDdeclinetoanswer\", \"Influence 2: DK/Decline\",\n                            \"b_mu_american_inf_2Veryative\", \"Influence 2: Very negative\",\n                            \"b_mu_american_inf_2Negative\", \"Influence 2: Negative\",\n                            \"b_mu_american_inf_2Positive\", \"Influence 2: Positive\",\n                            \"b_mu_american_inf_2Veryitive\", \"Influence 2: Very positive\",\n                            \"b_mu_basecount_z\", \"Base count\",\n                            \"b_mu_gdp_z\", \"GDP\",\n                            \"b_mu_pop_z\", \"Population\",\n                            \"b_mu_troops_z\", \"Troop deployment size\",\n                            \"b_mu_Intercept\", \"Intercept\")\n\nNext we‚Äôre going to add a new column that contains the grouping list. Since we‚Äôre using lots of categorical predictor variables we want to make sure they‚Äôre grouped in a sensible way.\n\ncoef.list &lt;- coef.list |&gt; \n  mutate(group = case_when(\n           grepl(\".*ontact.*\", raw) ~ \"Contact Status\",\n           grepl(\".*enefit.*\", raw) ~ \"Economic Benefits\",\n           grepl(\".*age.*\", raw) ~ \"Age\",\n           grepl(\".*ncome.*\", raw) ~ \"Income Quintile\",\n           grepl(\".*gender.*\", raw) ~ \"Gender Identification\",\n           grepl(\".*minority.*\", raw) ~ \"Minority Self-Identification\",\n           grepl(\".*relig.*\", raw) ~ \"Religious Identification\",\n           grepl(\".*ed_z.*\", raw) ~ \"Education\",\n           grepl(\".*ideology_z.*\", raw) ~ \"Ideology\",\n           grepl(\".*crime.*\", raw) ~ \"Crime Experience\",\n           grepl(\".*inf_1.*\", raw) ~ \"American Influence (Amount)\",\n           grepl(\".*inf_2.*\", raw) ~ \"American Influence (Quality)\",\n           TRUE ~ \"Group-Level Variables\"\n         )) \n\nNext, because we‚Äôre dealing with a really wide table we‚Äôll have coefficients with standard errors underneath. This means that each variable name is actually going to take up two lines. So we need to do a little extra here to properly format this bit. I have to admit I think Vincent actually came up with this particular solution as I was puttering with it for a while. So more props to him!\n\n# Find how long the coefficient list is for the final table hline\nlast.line &lt;- length(coef.list[[1]]) * 2\n\ncoef_map &lt;- setNames(coef.list$clean, coef.list$raw)\nidx &lt;- rle(coef.list$group)\nidx &lt;- setNames(idx$lengths  * 2, idx$values)\n\n\n\nPutting it all together\nFinally, we‚Äôve done all of the prep work. Now we can generate the actual table itself! I‚Äôm going to change just a couple of things here. Since the output is going to appear on a webpage, I‚Äôll change the ‚Äúoutput‚Äù argument to HTML. I‚Äôll also delete the save_kable() bit that tells it to save to a latex file, but you can add that in at the end if you want output.\nYou can see that we use the last.line values from the previous chunk in that last row_spec() line to tell it where to put an horizontal line.\n\nmodelsummary::modelsummary(mod.list,\n                  estimate = \"{estimate}\",\n                  statistic = \"conf.int\",\n                  fmt = 2,\n                  group = term ~ model + y.level,\n                  gof_map = gof.check,\n                  coef_map = coef_map,\n                  add_rows = rows.com,\n                  stars = FALSE,\n                  output = \"kableExtra\",\n                  caption = \"Bayesian multilevel multinomial logistic regressions. Population level effects. \\\\label{tab:contactfull}\") |&gt; \n  kable_styling(bootstrap_options = c(\"striped\"), font_size = 10, position = \"center\", full_width = FALSE) |&gt;\n  add_header_above(c(\" \", \"US Presence\" = 3, \"US People\" = 3, \"US Government\" = 3)) |&gt; \n  group_rows(index = idx, bold = TRUE, background = \"gray\", color = \"white\", hline_after = TRUE)  |&gt;  \n  row_spec(last.line, hline_after = TRUE) |&gt; \n  column_spec(1, width = \"5cm\") |&gt;\n  column_spec(2:10, width = \"3cm\") |&gt; \n  kableExtra::scroll_box(width = \"100%\", height = \"600px\") \n\n\n\nWhat‚Äôs left?\nThere are a few things I‚Äôd like to tweak. For example, {modelsummary} adds little slashes before the outcome variable name and inserts the name from the model list before that. I leave the list entries blank to avoid this since horizontal space is a premium.\nBlogdown/hugo is also being a bit frustrating with the final table. I‚Äôve tried making the font larger, but for whatever reason the horizontal scrollbar isn‚Äôt working with the kableExtra table in this environment. Not great, but it looks ok for now. Sizing and scale were a littl tough to tweak in Tex files, but I got it more or less where it needed to be by the end. I‚Äôm sure someone with a little more skill than me can figure out some of the hiccups.\nThere‚Äôs way more you can do to cusomtize the output of these tables, particularly in the footer. You might want to add particular model fit or summary statistics, and the glance.custom() function is a great place to specify what you want. Much is possible, but I‚Äôm going to stop here for now since it took me something like a month to get this posted."
  },
  {
    "objectID": "posts/20240807-ggplot-spacing/20240807-ggplot-spacing.html",
    "href": "posts/20240807-ggplot-spacing/20240807-ggplot-spacing.html",
    "title": "Better Organizing Plots with ggplot and patchwork",
    "section": "",
    "text": "files/header-code/header-code.html\n#|echo: false\n#|message: false\n#|include: false\n#|warning: false\n#|error: false\n\n# Load libraries and data\n# Do some basic cleaning to get the variables ready for plotting\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(patchwork)\n\ndata &lt;- readr::read_csv(here::here(\"posts/20240807-ggplot-spacing/poland-survey.csv\"))\n\nRows: 2254 Columns: 106\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (79): Status, ResponseId, DistributionChannel, UserLanguage, Q56, gender...\ndbl (11): StartDate, EndDate, Progress, Duration (in seconds), RecordedDate,...\nlgl (16): Finished, rnid, study, PID, K2, RISN, rid, Q_BallotBoxStuffing, Q_...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata.contact.type &lt;- data |&gt;\n      dplyr::select(contact_pers_type) |&gt;\n      tidyr::separate_longer_delim(contact_pers_type,\n                                   delim = stringr::regex(\"\\\\.\\\\,|\\\\,(?=[A-Z])\")) |&gt;  # This regex identifies a comma followed by a period, or a comma immediately followed by a capital letter, but uses the lookahead question mark thing to ignore the capital letter so it doesn't include it as a delimiter.\n      dplyr::mutate(contact_pers_type = trimws(contact_pers_type),\n                    contact_pers_type = gsub(\"\\\\b$\", \"\\\\.\", contact_pers_type),\n                    contact_pers_type = gsub(\"\\\\.\\\\.\", \"\\\\.\", contact_pers_type)) |&gt;\n      dplyr::group_by(contact_pers_type) |&gt;\n      dplyr::summarise(total = n()) |&gt;\n      filter(!is.na(contact_pers_type))\nThis is a short post inspired by some recent work I did on a project. The primary goal is to show ggplot users a couple of tricks that they can use to make better use of the visual space in their plots when they have a lot of information to display. This is especially useful when you have multiple plots that you want to display together, and you want to make sure that they are visually appealing and informative. It can also be helpful when you‚Äôre trying to display lots of information in axis labels. I‚Äôll be using the ggplot2 package in R for this post, but the principles should be applicable to other plotting libraries as well.\nThis is a pretty short example but I‚Äôll try to walk through my process. Readers who want to take a deeper dive into this topic might want to check our Kieran Healy‚Äôs excellent book on data visualization, Data Visualization: A Practical Introduction."
  },
  {
    "objectID": "posts/20240807-ggplot-spacing/20240807-ggplot-spacing.html#the-problem",
    "href": "posts/20240807-ggplot-spacing/20240807-ggplot-spacing.html#the-problem",
    "title": "Better Organizing Plots with ggplot and patchwork",
    "section": "The Problem",
    "text": "The Problem\nWhen you‚Äôre working with ggplot, you often have a lot of information that you want to display in your plots. This can include things like titles, axis labels, legends, and other annotations. However, if you‚Äôre not careful, all of this information can quickly clutter up your plot and make it difficult to read and interpret. This is especially true when you‚Äôre trying to display multiple plots together, as you might in a grid or faceted plot.\nOne thing to note is that I have a personal bias towards wanting to display information with figures where possible. Tables can be great, but I think it‚Äôs often easier to process lots of information‚Äîparticularly when we‚Äôre comparing across categories‚Äîwhen we‚Äôre dealing with visual representations of that information. So I‚Äôm always looking for ways to make my plots as informative as possible without sacrificing readability."
  },
  {
    "objectID": "posts/20240807-ggplot-spacing/20240807-ggplot-spacing.html#example",
    "href": "posts/20240807-ggplot-spacing/20240807-ggplot-spacing.html#example",
    "title": "Better Organizing Plots with ggplot and patchwork",
    "section": "Example",
    "text": "Example\nOn a recent project we were attempting to show a couple of different related points of information. Without going into too much unnecessary detail, we wanted to show information from a survey that we fielded on individual attitudes towards US military personnel stationed in their country. Specifically, we wanted to show 1) the number of people who reported having personal contact with US service personnel, and 2) the nature of the contact for those individuals who reported it. Ultimately we wanted to show the categories and the count of individuals who responded for each category.\n\nExample 1\nHere‚Äôs a bare bones example of what we‚Äôre looking for.\n\np1 &lt;- ggplot(data, aes(y = contact_pers, x = after_stat(count/sum(count)))) +\n  geom_bar() +\n  labs(x = \"Percent of Respondents\",\n       y = \"Contact with US Service Personnel\")\n\np2 &lt;- ggplot(data = data.contact.type, aes(y = reorder(contact_pers_type, total), x = total)) +\n  geom_bar(stat = \"identity\")  +\n  labs(x = \"Number of Respondents\",\n       y = \"Type of Contact\")\n\npatchwork::wrap_plots(p1, p2, ncol = 2)\n\n\n\n\n\n\n\nFigure¬†1\n\n\n\n\n\nSome obvious problems with Figure¬†1. The proportions are way off, largely due to the fact that the reported contact types are long character strings. This forces the plot on the right to take up waaaaaaay more space then it should, crowding out all the other information. We can‚Äôt even see the bars or numbers, which are kind of important.\nThis is a bit of a straw man as it uses a completely bare bones ggplot without any modifications, but it helps to highlight some of the problems that we‚Äôre dealing with. There are a few basic things we can do to clean this up and make it more informative.\n\n\nExample 2\n\np1 &lt;- ggplot(data, aes(y = contact_pers, x = after_stat(count/sum(count)))) +\n  geom_bar() +\n  labs(x = \"Percent of Respondents\",\n       y = \"Contact with US Service Personnel\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\np2 &lt;- ggplot(data = data.contact.type, aes(y = reorder(contact_pers_type, total), x = total)) +\n  geom_bar(stat = \"identity\")  +\n  labs(x = \"Number of Respondents\",\n       y = \"Type of Contact\") +\n  theme_minimal() +\n  scale_y_discrete(labels = function(x) str_wrap(x, width = 55))\n\npatchwork::wrap_plots(p1, p2, ncol = 2)\n\n\n\n\n\n\n\nFigure¬†2\n\n\n\n\n\nFigure¬†2 is a little bit better. We can see the actual axis values and bars, which are carrying almost all of the information that we care about. The big improvement here comes from the scale_y_discrete(labels = function(x) str_wrap(x, width = 55)) line of code in the right plot. The str_wrap() function allows ggplot to wrap the character strings across multiple lines, which makes them much easier to read and produces far less distortion in the plot. You can manipulate the width argument to get the desired number of characters per line and tweak the spacing.\nBut we can still do better. For example, in the left panel we can see the bars are disproportionately sized compared to the bars in the right panel. This isn‚Äôt surprising as we have very few categories in the left plot and several in the right, but we‚Äôre stretching them out across the same vertical space. It‚Äôs not terrible, but the distortion is notable and distracting. The ordering of these values also isn‚Äôt great. We could order them so their in descending frequency, like we do in the right panel. Last, presenting the x axis values as percentage values rather than proportions will better match the count values in the right panel.\nWe can also see that the category character strings in the right panel are extremely difficult to read. They‚Äôre not right justified, and they run across multiple lines. Finding specific starting points can be tough, and sometimes one category kind of runs into the next, making it hard to match specific categories with their corresponding bars and counts.\n\n\nExample 3\nLet‚Äôs take another pass and try to iron out some of these issues.\n\n# set custom color for bars\nbarcolor &lt;- viridis::turbo(1, begin = 0.10)\n\np1 &lt;- ggplot(data = data,\n         aes(y = contact_pers,\n             x = after_stat(count/sum(count)))) +\n  geom_bar(fill = barcolor) +\n  theme_minimal() +\n  scale_x_continuous(labels = scales::percent_format(),\n                     limits = c(0, 1),\n                     expand = c(0, 0.01)) +\n  labs(x = \"Percent\",\n       y = \"Reported Contact\") \n\np2 &lt;- ggplot(data = data.contact.type,\n           aes(y = reorder(contact_pers_type, total),\n               x = total)) +\n  geom_bar(fill = barcolor,\n               stat = \"identity\") +\n  scale_x_continuous(expand = c(0, 1)) +\n      scale_y_discrete(labels = function(x) str_wrap(x, width = 55)) +\n  labs(x = \"Count\",\n           y = \"Contact Type\") +\n  theme_minimal() +\n  theme(axis.text.y.left = element_text(size = 10.0,\n                                        hjust = 0,\n                                        vjust = 0,\n                                        margin = margin(r = -5.0, b = 0.5, unit = \"cm\")),\n          axis.ticks.length.y.left = unit(5.5, \"cm\"),\n          axis.ticks.y.left = element_line(linetype = \"dotted\",\n                                           linewidth = 0.3))\n\npatchwork::wrap_plots(p1, p2, ncol = 1)\n\n\n\n\n\n\n\nFigure¬†3\n\n\n\n\n\nFigure¬†3 fixes some big problems with the previous plots, but also creates some new ones. Stacking the figures on top of one another fixes the problems with the distorted column sizes and the difficulty in reading the category labels. It looks a little better now.\nFirst, we moved from two columns to a single column, placing the plots on top of one another. This produces less compression in the bars helps us to better compare the relative size/magnitude of the x axis values within each plot.\nWe can also see in the bottom panel that the respondent categories are left justified, which makes it easier to read the categories and match them to bars. To make it even easier to match categories to particular bars/counts we‚Äôve added a dotted line that runs the length of the y-axis. We do this using the theme() function and the axis.ticks.y.left and axis.ticks.length.y.left arguments. This takes some tweaking to get right, and depends on the settings in the str_wrap() arguments in the scale_y_discrete() function. It will also depend on the overall dimensions of the figure.\nBut we can see the bottom panel is now even more crowded. The dotted lines are great, but they don‚Äôt help when the text is spilling over from one category to the next.\n\n\nExample 4\nOK, let‚Äôs take one more pass and iron out some of the details. This last step also requires that we revisit some of the data cleaning processes that produce the data we use in each plot.\n\ndata.contact.type &lt;- data |&gt;\n      dplyr::select(contact_pers_type) |&gt;\n      tidyr::separate_longer_delim(contact_pers_type,\n                                   delim = stringr::regex(\"\\\\.\\\\,|\\\\,(?=[A-Z])\")) |&gt;  # This regex identifies a comma followed by a period, or a comma immediately followed by a capital letter, but uses the lookahead question mark thing to ignore the capital letter so it doesn't include it as a delimiter.\n      dplyr::mutate(contact_pers_type = trimws(contact_pers_type),\n                    contact_pers_type = gsub(\"\\\\b$\", \"\\\\.\", contact_pers_type),\n                    contact_pers_type = gsub(\"\\\\.\\\\.\", \"\\\\.\", contact_pers_type),\n                    contact_pers_type = gsub(\"^\", \"\\u2022 \", contact_pers_type)) |&gt;\n      dplyr::group_by(contact_pers_type) |&gt;\n      dplyr::summarise(total = n()) |&gt;\n      filter(!is.na(contact_pers_type))\n\n\n# set custom color for bars\nbarcolor &lt;- viridis::turbo(1, begin = 0.10)\n\np1 &lt;- ggplot(data = data,\n         aes(y = contact_pers,\n             x = after_stat(count/sum(count)))) +\n  geom_bar(fill = barcolor) +\n  theme_minimal() +\n  scale_x_continuous(labels = scales::percent_format(),\n                     limits = c(0, 1),\n                     expand = c(0, 0.01)) +\n  labs(x = \"Percent\",\n       y = \"Reported Contact\") \n\np2 &lt;- ggplot(data = data.contact.type,\n           aes(y = reorder(contact_pers_type, total),\n               x = total)) +\n  geom_bar(fill = barcolor,\n               stat = \"identity\") +\n  scale_x_continuous(expand = c(0, 1)) +\n      scale_y_discrete(labels = function(x) str_wrap(x, width = 55, exdent = 3)) +\n  labs(x = \"Count\",\n           y = \"Contact Type\") +\n  theme_minimal() +\n  theme(axis.text.y.left = element_text(size = 9.0,\n                                        hjust = 0,\n                                        vjust = 0,\n                                        margin = margin(r = -5.0, b = 0.5, unit = \"cm\")),\n          axis.ticks.length.y.left = unit(5.5, \"cm\"),\n          axis.ticks.y.left = element_line(linetype = \"dotted\",\n                                           linewidth = 0.3))\n\npatchwork::wrap_plots(p1, p2, ncol = 1) +\n  patchwork::plot_layout(ncol = 1,\n                         heights = c(1,3)) +\n  plot_annotation(tag_levels = 'A') &\n  theme(text = element_text(family = \"Oswald\", size = 10))\n\n\n\n\n\n\n\nFigure¬†4\n\n\n\n\n\nWay better! So what did we do here?\nFirst, we added a bullet point to the beginning of each category in the contact type data. We did this with the simple gsub() function here: contact_pers_type = gsub(\"^\", \"\\u2022 \", contact_pers_type). This just uses a mutate command to insert a bullet point into the beginning of each character string in the contact_pers_type category. This helps to visually separate the categories from one another and makes it easier to match the categories to the bars.\nNote that because the bullet point adds a new character and a blank space, we also want to make sure that the str_wrap() argument is aligning the new lines with the first line so the bullet point actually leads the rest of the text. That is, we don‚Äôt want the line break to start a new line flush with the bullet point because this would reduce the bullet point‚Äôs use in distinguishing new lines. We do this with the exdent argument in the str_wrap() function. Again, play with values here to find what works for your plot.\nWe also increased the overall vertical spacing of the plot. We did this here with Quarto‚Äôs fig-height and fig-width arguments, but you could also do this by adjusting the size of the plot in the ggsave() function when you save the plot to include in another document. But adding vertical space helps to eliminate the overlap in the text for the contact type categories.\nLast, we used the patchwork package to adjust the proportions of the individual plots. In this case we set the heights of the plots to 1 and 3, respectively. This gives the bottom plot more vertical space, which helps to eliminate the overlap in the text, and also brings the heights of the top panel‚Äôs bars even more into line with the bar heights in the bottom plot. We also added subplot labels to distinguish more easily between panels ‚ÄúA‚Äù and ‚ÄúB‚Äù."
  },
  {
    "objectID": "posts/20240807-ggplot-spacing/20240807-ggplot-spacing.html#other-options",
    "href": "posts/20240807-ggplot-spacing/20240807-ggplot-spacing.html#other-options",
    "title": "Better Organizing Plots with ggplot and patchwork",
    "section": "Other Options?",
    "text": "Other Options?\nThis assumes we want to do all of the work in ggplot, but there are other options for accomplishing similar goals. For example, we could use the gt and geExtras packages to do something similar with sparkline plots. This can be be great in certain circumstances, but I think it would be a bit more difficult to get the same level of control over the plot aesthetics since we‚Äôre dealing with two plots and two different axis scales.\nBut here‚Äôs an example of how you might do this with gt and gtExtras:\n\n#|warning: false\n#|message: false\n\nlibrary(gt)\nlibrary(gtExtras)\n\ndata.contact.type &lt;- data |&gt; \n  dplyr::select(contact_pers_type) |&gt; \n  tidyr::separate_longer_delim(contact_pers_type,\n                               delim = stringr::regex(\"\\\\.\\\\,|\\\\,(?=[A-Z])\")) |&gt; \n  dplyr::mutate(contact_pers_type = trimws(contact_pers_type),\n                contact_pers_type = gsub(\"\\\\b$\", \"\\\\.\", contact_pers_type),\n                contact_pers_type = gsub(\"\\\\.\\\\.\", \"\\\\.\", contact_pers_type),\n                contact_pers_type = gsub(\"^\", \"\\u2022 \", contact_pers_type)) |&gt; \n  dplyr::group_by(contact_pers_type) |&gt; \n  dplyr::summarise(total = list(n())) |&gt; \n  filter(!is.na(contact_pers_type))\n\ndata.contact.type |&gt; \n  dplyr::rename(\"Contact Type\" = contact_pers_type,\n                \"Count\" = total) |&gt;\n  gt() |&gt; \n  gt_plt_bar(\n    column = Count) \n\n\n\n\n\n\n\n\n\n\n\nContact Type\nCount\n\n\n\n\n‚Ä¢ Been close friends with a U.S. service member.\n\n\n\n   \n\n\n\n‚Ä¢ Dated/were romantically involved with a U.S. service member.\n\n\n\n   \n\n\n\n‚Ä¢ Did business with a U.S. service member, such as selling them something or buying something from them.\n\n\n\n   \n\n\n\n‚Ä¢ Don't know/decline to answer.\n\n\n\n   \n\n\n\n‚Ä¢ Had a U.S. service member as a family member (other than a spouse).\n\n\n\n   \n\n\n\n‚Ä¢ Had a U.S. service member as a spouse or domestic partner.\n\n\n\n   \n\n\n\n‚Ä¢ Had a brief everyday interaction with a U.S. service member, such as conversing on public transport, or shopping at the same market.\n\n\n\n   \n\n\n\n‚Ä¢ Had a co-worker who was a U.S. servicemember.\n\n\n\n   \n\n\n\n‚Ä¢ Had a different interaction that does not fit into these categories (please list here).\n\n\n\n   \n\n\n\n‚Ä¢ Had a neighbor who was a U.S. servicemember.\n\n\n\n   \n\n\n\n‚Ä¢ Interacted with a U.S. service member in a recurring social setting, such as a house of worship, sports club, school, etc.\n\n\n\n   \n\n\n\n‚Ä¢ Interacted with a U.S. service member in an individual social setting, such as a party, bar, festival, etc."
  },
  {
    "objectID": "posts/hurdle-lognormal-densities-ii/lognormal-densities-ii.html",
    "href": "posts/hurdle-lognormal-densities-ii/lognormal-densities-ii.html",
    "title": "Hurdle lognormal densities, take II",
    "section": "",
    "text": "files/header-code/header-code.htmlI previously wrote about a project in which I was attempting to figure out how to build a probability density function for a hurdle log-normal model. Ultimately I kind of left this topic hanging because I wasn‚Äôt really sure that the solution I had settled on was correct. The good news is, it was. The bad news is, I spent quite a bit of time trying to solve a problem that already had a solution. Still, it was probably good practice to just work through the problem on my own, even if it was somewhat moot. All that said, it‚Äôs probably also useful to run through the problem again and the answer.\nTo quickly rehash, we‚Äôre dealing with U.S. troop data reported on a country-year basis from 1950-2020. The data have a long right tail and are truncated at 0. There are also a lot of 0 values. The following figure shows some simulated data to give you a rough idea of what the distribution of the data look like.\n\n# Simulation . Values reflect what we see in our data.\nsims &lt;- 1e4\nmuval = 2.8\nsdval = 2.54\npival = 0.2\n\n\nsimvals &lt;- rep(NA, sims)\nsimvals[c(1:2000)] &lt;- rep(0, sims*0.2)\nsimvals[c(2001:10000)] &lt;- rlnorm(sims*0.8, meanlog = muval, sdlog = sdval)\n\nggplot(as.data.frame(simvals), aes(x = log1p(simvals))) +\n  geom_histogram(bins = 100, fill = \"dodgerblue1\") +\n  theme_minimal() +\n  labs(x = \"log(Troops)\",\n       y = \"Count\",\n       title = \"Simulated troop deployment values\")\n\n\n\n\n\n\n\n\nUltimately I want to be able to calculate probability values for specific values of the troops variable. The {stats} package has a built-in probability density function for lognormal distributions. The trick was to produce one that would work with a hurdle lognormal distribution like the one shown above. The function I settled on is below:\n\n\ndhlnorm &lt;- function(x, meanlog, sdlog, pval) {\n  \n    if (x &gt; 0) {\n    \n    value &lt;- dlnorm(x, meanlog = meanlog, sdlog = sdlog, log = TRUE) + log(1-pval)\n    \n    return(exp(value))\n    \n  } else {\n    \n    value &lt;- pval\n    \n    return(value)\n  }\n  }\n\n\nprob &lt;- dhlnorm(3, meanlog = muval, sdlog = sdval, pval = 0.2)\n\nprint(prob)\n## [1] 0.03346686\n\nThis just uses the dlnorm() function to calculate probability values for cases where troops \\(&gt;0\\). Note that I have it set to return the log of the probability value. This allows us to add the log of \\(1-pval\\), which is just one minus the hurdle probability value. This ultimately gives us the log of the probability of the chosen value, adjusting for the 0 values in the distribution. We just exponentiate that value to get the return the probability value we‚Äôre interested in. In this case that‚Äôs \\(\\approx\\) 0.0335. This is a little different than what I did in the previous post, but gets us the same result with a little more flexibility.\nSo is this right?\nTurns out the {brms} package already has a suite of functions that calcualte these quantities of interest. As is often the case, even though I‚Äôve been running some hurdle lognormal models, I missed the probability density functions corresponding to these distributions. Using the simulated data we can see if that function matches the homemade one above.\n\n\nprob.brms &lt;- brms::dhurdle_lognormal(3, mu = muval, sigma = sdval, hu = 0.2, log = FALSE)\n\nprint(prob)\n## [1] 0.03346686\nprint(prob.brms)\n## [1] 0.03346686\n\nNote that the arguments in the {brms} function have different names, but otherwise things look good! The probability values are identical. As another experiment, let‚Äôs look at the probability values for various values of the troops variable that we actually observe. Deployments usually run from about 0 up through a max of about 500,000 during the Vietnam War. This is relatively brief, though. Most of the higher values cluster around 150,000-200,000 in Germany during the Cold War. The following loop just compares the home-rolled function (the black line) with the {brms} function (the red line) as a further check to make sure I didn‚Äôt screw anything up.\n\n\nxval &lt;- seq(0, 14, length = 100)\nprob.test &lt;- rep(NA, 100)\nprob.brms &lt;- rep(NA, 100)\n\ntest.df &lt;- data.frame(xval = xval, \n                   prob.test = prob.test,\n                   prob.brms = prob.brms)\n\nfor(i in 1:100) {\n  xval &lt;- test.df[i, 1]\n  test.df[i, 2] &lt;- dhlnorm(x = xval, meanlog = muval, sdlog = sdval, pval = 0.2)\n  test.df[i, 3] &lt;- brms::dhurdle_lognormal(x = xval, mu = muval, sigma = sdval, hu = 0.2, log = FALSE)\n}\n\nggplot(test.df, aes(x = xval)) +\n  geom_line(aes(y = prob.test), size = 2.5) +\n  geom_line(aes(y = prob.brms), color = \"red\", size = 1) +\n  theme_minimal() +\n  labs(x = \"log(Troops)\",\n       y = \"Probability\")\n\n\n\n\n\n\n\n\nLooks good! Overall this was a nice learning experience, and even though I could have arrived at the same conclusion with a pre-assembled function, it felt good to dig into the process on my own since this isn‚Äôt something I‚Äôm typically thinking about.\nThe next step will be integrating this into some marginal structural models so we cal calculate some inverse probability of treatment weights. This is something I‚Äôve been working on for a while now, but have stuck with basic Gaussian distributions for the treatment weighting models. Given the distribution of the data this produces some ill-fitting models, and I think this is a way to generate better weights. I haven‚Äôt come across this method in the literature. There are papers on continuous treatment models, but most examples tend to utilize normally distributed treatments. As always, if this is something anyone has come across before send me a link! I‚Äôd love to check it out."
  },
  {
    "objectID": "minerva.html",
    "href": "minerva.html",
    "title": "Minerva Research Initiative Project",
    "section": "",
    "text": "This page is dedicated to our Minerva Research Initiative project: The Social, Political, and Economic Consequences of the US Military‚Äôs Overseas Deployments (Grant # W911NF-18-1-0087)."
  },
  {
    "objectID": "minerva.html#research-team",
    "href": "minerva.html#research-team",
    "title": "Minerva Research Initiative Project",
    "section": "Research Team",
    "text": "Research Team\n\nMichael E. Flynn\nMichael A. Allen\nCarla Martinez Machain\nAndrew Stravers"
  },
  {
    "objectID": "minerva.html#application-documents",
    "href": "minerva.html#application-documents",
    "title": "Minerva Research Initiative Project",
    "section": "Application Documents",
    "text": "Application Documents\nYou can find a copy of our white paper and fully length proposals here.\n\nWhite Paper\nFull Proposal"
  },
  {
    "objectID": "minerva.html#data-documentation",
    "href": "minerva.html#data-documentation",
    "title": "Minerva Research Initiative Project",
    "section": "Data Documentation",
    "text": "Data Documentation\n\nCodebook for Survey\nCodebook for Protest Data\nCodebook for Military Expenditures"
  },
  {
    "objectID": "minerva.html#datasets",
    "href": "minerva.html#datasets",
    "title": "Minerva Research Initiative Project",
    "section": "Datasets",
    "text": "Datasets\n\nSurvey Data\n\nYears 1-3 Data Combined (Current)\nYear 2 Data (Outdated)\nYear 1 Data (Outdated)\n\n\n\nMilitary Spending\n\nDeployment Cost Data\nMilitary Construction Data\n\nAlso see the {troopdata} package for most up-to-date military construction spending data."
  },
  {
    "objectID": "minerva.html#research-publications",
    "href": "minerva.html#research-publications",
    "title": "Minerva Research Initiative Project",
    "section": "Research Publications",
    "text": "Research Publications\n\nAllen, Michael A., Michael E. Flynn, Carla Martinez Machain, and Andrew Stravers. 2020. Outside the wire: US military deployments and public opinion in host states. American Political Science Review. 114(2): 326-341."
  }
]